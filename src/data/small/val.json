{"label_name":"process","label":2,"method_name":"process_body","method":"\n'Return (params, method) from request body.'\ntry:\n    return xmlrpc_loads(cherrypy.request.body.read())\nexcept Exception:\n    return (('ERROR PARAMS',), 'ERRORMETHOD')\n"}
{"label_name":"process","label":2,"method_name":"process","method":"\n' Tokenization\\n    '\nregex = settings['regex']\nfor doc in content:\n    text = ' '.join(tokenizer(doc['text'], regex))\n    try:\n        (yield set_text(doc, text))\n    except Exception:\n        logger.exception('Error in converting to Doc %r', text)\n        continue\n"}
{"label_name":"predict","label":4,"method_name":"check_predict_proba_raises","method":"\n\"Test that predict_proba doesn't work with hinge loss\"\npp = Clf(loss='squared_hinge', random_state=0).predict_proba\nassert_raises(ValueError, pp, X)\n"}
{"label_name":"save","label":1,"method_name":"save_obj_no_sort_w","method":"\nlink_list = []\nnew_queue = queue.Queue()\nnew_queue.queue = copy.deepcopy(queue1.queue)\nif os.path.isfile(filename):\n    os.remove(filename)\nwhile (new_queue.empty() == False):\n    link_list.append(new_queue.get())\nfilename = (filename + '.p')\npickle.dump(link_list, open(filename, 'wb'))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n(logits, loss, preds, accuracy, saver) = bilstm_reader(placeholders, len(vocab), emb_dim)\noptim = tf.train.AdamOptimizer(learning_rate=0.001)\nif (l2 != 0.0):\n    loss = (loss + (tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) * l2))\nif (clip is not None):\n    gradients = optim.compute_gradients(loss)\n    if (clip_op == tf.clip_by_value):\n        capped_gradients = [(tf.clip_by_value(grad, clip[0], clip[1]), var) for (grad, var) in gradients]\n    elif (clip_op == tf.clip_by_norm):\n        capped_gradients = [(tf.clip_by_norm(grad, clip), var) for (grad, var) in gradients]\n    min_op = optim.apply_gradients(capped_gradients)\nelse:\n    min_op = optim.minimize(loss)\ntf.global_variables_initializer().run(session=sess)\nif (not PRETRAINED):\n    prev_loss = 1000\n    steps_since_save = 0\n    breakout = False\n    for i in range(1, (max_epochs + 1)):\n        if breakout:\n            break\n        loss_all = []\n        avg_acc = 0\n        count = 0\n        for (j, batch) in enumerate(train_feed_dicts):\n            print('Training iteration: ', j, end='\\r')\n            sys.stdout.flush()\n            (_, current_loss, p, acc) = sess.run([min_op, loss, preds, accuracy], feed_dict=batch)\n            avg_acc += acc\n            count += 1\n            loss_all.append(np.mean(current_loss))\n            if ((j % DISPLAY_EVERY) == 0):\n                print()\n                avg_test_acc = 0\n                avg_test_loss = 0\n                count = 0\n                for (k, batch) in enumerate(test_feed_dicts):\n                    print('Testing iteration: ', k, end='\\r')\n                    sys.stdout.flush()\n                    (acc, l) = sess.run([accuracy, loss], feed_dict=batch)\n                    avg_test_acc += acc\n                    avg_test_loss += np.mean(l)\n                    count += 1\n                avg_test_loss \/= count\n                avg_test_acc \/= count\n                print('\\n\\t\\t**** EPOCH ', i, ' ****')\n                print('Test Accuracy on Iteration ', j, ' is: ', avg_test_acc)\n                print('Test Loss on Iteration ', j, ' is: ', avg_test_loss)\n                if (avg_test_loss < prev_loss):\n                    print('>> New Lowest Loss <<')\n                    saver.save(sess=sess, save_path=MODEL_SAVE_PATH)\n                    print('>> Model Saved <<')\n                    prev_loss = avg_test_loss\n                    steps_since_save = 0\n                else:\n                    steps_since_save += 1\n                if (steps_since_save > 10):\n                    breakout = True\n                    break\n        l = np.mean(loss_all)\nsaver.restore(sess, MODEL_SAVE_PATH)\nreturn (logits, loss, preds, accuracy, saver)\n"}
{"label_name":"save","label":1,"method_name":"np_save","method":"\nnp.save(file_path, results)\n"}
{"label_name":"train","label":0,"method_name":"_wait_until_training_can_be_updated","method":"\nready_for_updating = _check_secondary_status(sagemaker_client, job_name)\nwhile (not ready_for_updating):\n    time.sleep(poll)\n    ready_for_updating = _check_secondary_status(sagemaker_client, job_name)\n"}
{"label_name":"process","label":2,"method_name":"step_preprocess","method":"\n'Preprocess the input at the beginning of each step.\\n\\n  Args:\\n    x: input tensor\\n    step: step\\n    hparams: model hyper-parameters\\n\\n  Returns:\\n    preprocessed input.\\n\\n  '\noriginal_channel_size = common_layers.shape_list(x)[(- 1)]\nif hparams.add_position_timing_signal:\n    x = add_position_timing_signal(x, step, hparams)\nif hparams.add_step_timing_signal:\n    x = add_step_timing_signal(x, step, hparams)\nif ((hparams.add_position_timing_signal or hparams.add_position_timing_signal) and (hparams.add_or_concat_timing_signal == 'concat')):\n    x = common_layers.dense(x, original_channel_size, activation=None, use_bias=False)\nif hparams.add_sru:\n    x = common_layers.sru(x)\nreturn x\n"}
{"label_name":"process","label":2,"method_name":"preprocess_corpora_multilingual_many_to_one","method":"\nsource_dicts = build_vocab_multicorpus(args.multiling_source_lang, args.multiling_train_source_text_file, args.multiling_encoder_lang, args.multiling_source_vocab_file, args.source_max_vocab_size, dictionary_cls)\nsource_corpus_lang_ids = [args.multiling_encoder_lang.index(l) for l in args.multiling_source_lang]\nsource_corpus_dicts = [source_dicts[l] for l in args.multiling_source_lang]\nbinarize_text_file_multilingual(corpus_configs=make_multiling_corpus_configs(source_corpus_lang_ids, args.multiling_train_source_text_file, source_corpus_dicts, args.multiling_train_oversampling), output_path=args.train_source_binary_path, append_eos=args.append_eos_to_source, reverse_order=args.reverse_source, prepend_language_id=False)\nbinarize_text_file_multilingual(corpus_configs=make_multiling_corpus_configs(source_corpus_lang_ids, args.multiling_eval_source_text_file, source_corpus_dicts, args.multiling_train_oversampling), output_path=args.eval_source_binary_path, append_eos=args.append_eos_to_source, reverse_order=args.reverse_source, prepend_language_id=False)\ntarget_dicts = build_vocab_multicorpus(args.multiling_target_lang, args.multiling_train_target_text_file, args.multiling_decoder_lang, args.multiling_target_vocab_file, args.target_max_vocab_size, args.penalized_target_tokens_file, dictionary_cls)\ntarget_corpus_lang_ids = [args.multiling_decoder_lang.index(l) for l in args.multiling_target_lang]\ntarget_corpus_dicts = [target_dicts[l] for l in args.multiling_target_lang]\nbinarize_text_file_multilingual(corpus_configs=make_multiling_corpus_configs(target_corpus_lang_ids, args.multiling_train_target_text_file, target_corpus_dicts, args.multiling_train_oversampling), output_path=args.train_target_binary_path, append_eos=True, reverse_order=False, prepend_language_id=True)\nbinarize_text_file_multilingual(corpus_configs=make_multiling_corpus_configs(target_corpus_lang_ids, args.multiling_eval_target_text_file, target_corpus_dicts, args.multiling_train_oversampling), output_path=args.eval_target_binary_path, append_eos=True, reverse_order=False, prepend_language_id=True)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n' Train the bot '\n(test_buckets, data_buckets, train_buckets_scale) = _get_buckets()\nmodel = ChatBotModel(False, config.BATCH_SIZE)\nmodel.build_graph()\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    print('Running session')\n    sess.run(tf.global_variables_initializer())\n    _check_restore_parameters(sess, saver)\n    iteration = model.global_step.eval()\n    total_loss = 0\n    while True:\n        skip_step = _get_skip_step(iteration)\n        bucket_id = _get_random_bucket(train_buckets_scale)\n        (encoder_inputs, decoder_inputs, decoder_masks) = data.get_batch(data_buckets[bucket_id], bucket_id, batch_size=config.BATCH_SIZE)\n        start = time.time()\n        (_, step_loss, _) = run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, False)\n        total_loss += step_loss\n        iteration += 1\n        if ((iteration % skip_step) == 0):\n            print('Iter {}: loss {}, time {}'.format(iteration, (total_loss \/ skip_step), (time.time() - start)))\n            start = time.time()\n            total_loss = 0\n            saver.save(sess, os.path.join(config.CPT_PATH, 'chatbot'), global_step=model.global_step)\n            if ((iteration % (10 * skip_step)) == 0):\n                _eval_test_set(sess, model, test_buckets)\n                start = time.time()\n            sys.stdout.flush()\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nnetwork = load_model(model_path)\nresult = []\nfor data in predict_set:\n    person_id = data[0]\n    product_id = data[1]\n    user_self_vector = user_feature[person_id][:8]\n    user_desc_vector = user_feature[person_id][8]\n    product_self_vector = product_feature[product_id][:6]\n    product_desc_vector = product_feature[product_id][6]\n    prob = output(network, user_self_vector, user_desc_vector, product_self_vector, product_desc_vector)\n    prob = prob.squeeze(0)\n    prob = prob.data.numpy()\n    result.append(prob)\nreturn result\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\nlayer: Model[(Padded, Padded)] = model.layers[0]\nY: Union[(Padded, Ragged, List2d, PaddedData)]\nif isinstance(Xseq, Padded):\n    (Y, backprop) = layer(Xseq, is_train)\nelif isinstance(Xseq, Ragged):\n    (Y, backprop) = _ragged_forward(layer, cast(Ragged, Xseq), is_train)\nelif _is_padded_data(Xseq):\n    (Y, backprop) = _tuple_forward(layer, cast(PaddedData, Xseq), is_train)\nelif is_xp_array(Xseq):\n    (Y, backprop) = _array_forward(layer, cast(Floats3d, Xseq), is_train)\nelse:\n    (Y, backprop) = _list_forward(layer, cast(List2d, Xseq), is_train)\nreturn cast(Tuple[(SeqT, Callable)], (Y, backprop))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nuse_amp = (args.dtype == 'float16')\n(store, num_workers, rank, local_rank, is_master_node, ctx_l) = init_comm(args.comm_backend, args.gpus)\nsetup_logging(args, local_rank)\n(cfg, tokenizer, qa_net, use_segmentation) = get_network(args.model_name, ctx_l, args.classifier_dropout, args.param_checkpoint, args.backbone_path)\nlogging.info('Prepare training data')\ntrain_features = get_squad_features(args, tokenizer, segment='train')\ndataset_processor = SquadDatasetProcessor(tokenizer=tokenizer, doc_stride=args.doc_stride, max_seq_length=args.max_seq_length, max_query_length=args.max_query_length)\nlogging.info('Processing the Training data:')\n(train_dataset, num_answer_mismatch, num_unreliable) = dataset_processor.get_train(train_features, skip_unreliable=True)\nlogging.info('Done! #Unreliable Span={} \/ #Mismatched Answer={} \/ #Total={}'.format(num_unreliable, num_answer_mismatch, len(train_features)))\nnum_impossible = 0\nfor sample in train_dataset:\n    num_impossible += sample.is_impossible\nlogging.info('Before Chunking, #Train\/Is Impossible = {}\/{}'.format(len(train_features), sum([ele.is_impossible for ele in train_features])))\nlogging.info('After Chunking, #Train Sample\/Is Impossible = {}\/{}'.format(len(train_dataset), num_impossible))\nrs = np.random.RandomState(args.pre_shuffle_seed)\nrs.shuffle(train_dataset)\nsampler = SplitSampler(len(train_dataset), num_parts=num_workers, part_index=rank, even_size=True)\ntrain_dataloader = mx.gluon.data.DataLoader(train_dataset, batchify_fn=dataset_processor.BatchifyFunction, batch_size=args.batch_size, num_workers=0, sampler=sampler)\nif ('electra' in args.model_name):\n    if (args.untunable_depth > 0):\n        qa_net.backbone.frozen_params(args.untunable_depth)\n    if (args.layerwise_decay > 0):\n        qa_net.backbone.apply_layerwise_decay(args.layerwise_decay)\nlogging.info('Creating distributed trainer...')\nparam_dict = deduplicate_param_dict(qa_net.collect_params())\nfor (_, v) in qa_net.collect_params('.*beta|.*gamma|.*bias').items():\n    v.wd_mult = 0.0\nparams = [p for p in param_dict.values() if (p.grad_req != 'null')]\nnum_accumulated = args.num_accumulated\nif (num_accumulated > 1):\n    logging.info('Using gradient accumulation. Effective global batch size = {}'.format((((num_accumulated * args.batch_size) * len(ctx_l)) * num_workers)))\n    for p in params:\n        p.grad_req = 'add'\nif (args.comm_backend == 'horovod'):\n    hvd.broadcast_parameters(param_dict, root_rank=0)\nepoch_size = (((len(train_dataloader) + len(ctx_l)) - 1) \/\/ len(ctx_l))\nif (args.num_train_steps is not None):\n    num_train_steps = args.num_train_steps\nelse:\n    num_train_steps = int(((args.epochs * epoch_size) \/ args.num_accumulated))\nif (args.warmup_steps is not None):\n    warmup_steps = args.warmup_steps\nelse:\n    warmup_steps = int((num_train_steps * args.warmup_ratio))\nassert (warmup_steps is not None), 'Must specify either warmup_steps or warmup_ratio'\nlog_interval = args.log_interval\nsave_interval = (args.save_interval if (args.save_interval is not None) else (epoch_size \/\/ args.num_accumulated))\nlogging.info('#Total Training Steps={}, Warmup={}, Save Interval={}'.format(num_train_steps, warmup_steps, save_interval))\nlr_scheduler = PolyScheduler(max_update=num_train_steps, base_lr=args.lr, warmup_begin_lr=0, pwr=1, final_lr=0, warmup_steps=warmup_steps, warmup_mode='linear')\noptimizer_params = {'learning_rate': args.lr, 'wd': args.wd, 'lr_scheduler': lr_scheduler}\nadam_betas = ast.literal_eval(args.adam_betas)\nif (args.optimizer == 'adamw'):\n    optimizer_params.update({'beta1': adam_betas[0], 'beta2': adam_betas[1], 'epsilon': args.adam_epsilon, 'correct_bias': False})\nelif (args.optimizer == 'adam'):\n    optimizer_params.update({'beta1': adam_betas[0], 'beta2': adam_betas[1], 'epsilon': args.adam_epsilon})\nif use_amp:\n    optimizer_params.update({'multi_precision': True})\nif (args.comm_backend == 'horovod'):\n    trainer = hvd.DistributedTrainer(param_dict, args.optimizer, optimizer_params)\nelse:\n    trainer = mx.gluon.Trainer(param_dict, args.optimizer, optimizer_params, update_on_kvstore=False)\nif use_amp:\n    amp.init_trainer(trainer)\nlog_span_loss = 0\nlog_answerable_loss = 0\nlog_total_loss = 0\nlog_sample_num = 0\nglobal_tic = time.time()\ntic = time.time()\nfor (step_num, batch_data) in enumerate(grouper(repeat(train_dataloader), (len(ctx_l) * num_accumulated))):\n    for sample_l in grouper(batch_data, len(ctx_l)):\n        loss_l = []\n        span_loss_l = []\n        answerable_loss_l = []\n        for (sample, ctx) in zip(sample_l, ctx_l):\n            if (sample is None):\n                continue\n            tokens = sample.data.as_in_ctx(ctx)\n            log_sample_num += len(tokens)\n            segment_ids = (sample.segment_ids.as_in_ctx(ctx) if use_segmentation else None)\n            valid_length = sample.valid_length.as_in_ctx(ctx)\n            p_mask = sample.masks.as_in_ctx(ctx)\n            gt_start = sample.gt_start.as_in_ctx(ctx).astype(np.int32)\n            gt_end = sample.gt_end.as_in_ctx(ctx).astype(np.int32)\n            is_impossible = sample.is_impossible.as_in_ctx(ctx).astype(np.int32)\n            batch_idx = mx.np.arange(tokens.shape[0], dtype=np.int32, ctx=ctx)\n            p_mask = (1 - p_mask)\n            with mx.autograd.record():\n                (start_logits, end_logits, answerable_logits) = qa_net(tokens, segment_ids, valid_length, p_mask, gt_start)\n                sel_start_logits = start_logits[(batch_idx, gt_start)]\n                sel_end_logits = end_logits[(batch_idx, gt_end)]\n                sel_answerable_logits = answerable_logits[(batch_idx, is_impossible)]\n                span_loss = ((- 0.5) * (sel_start_logits + sel_end_logits).mean())\n                answerable_loss = ((- 0.5) * sel_answerable_logits.mean())\n                loss = ((span_loss + answerable_loss) \/ (len(ctx_l) * num_accumulated))\n                loss_l.append(loss)\n                span_loss_l.append(span_loss)\n                answerable_loss_l.append(answerable_loss)\n        if use_amp:\n            with mx.autograd.record():\n                with amp.scale_loss(loss_l, trainer) as amp_loss_l:\n                    for loss in amp_loss_l:\n                        loss.backward()\n            norm_clip_mult = (num_workers * trainer.amp_loss_scale)\n        else:\n            with mx.autograd.record():\n                for loss in loss_l:\n                    loss.backward()\n            norm_clip_mult = num_workers\n        log_span_loss += sum([ele.as_in_ctx(ctx_l[0]) for ele in span_loss_l]).asnumpy()\n        log_total_loss += sum([ele.as_in_ctx(ctx_l[0]) for ele in loss_l]).asnumpy()\n        log_answerable_loss += sum([ele.as_in_ctx(ctx_l[0]) for ele in answerable_loss_l]).asnumpy()\n    trainer.allreduce_grads()\n    if (args.max_grad_norm > 0):\n        (total_norm, ratio, is_finite) = clip_grad_global_norm(params, (args.max_grad_norm * norm_clip_mult))\n    else:\n        total_norm = grad_global_norm(params)\n    if (args.comm_backend == 'horovod'):\n        trainer.update(1, ignore_stale_grad=True)\n    else:\n        trainer.update(num_workers, ignore_stale_grad=True)\n    total_norm = (total_norm \/ norm_clip_mult)\n    if (args.num_accumulated > 1):\n        qa_net.zero_grad()\n    if (((local_rank == 0) and (((step_num + 1) % save_interval) == 0)) or ((step_num + 1) >= num_train_steps)):\n        version_prefix = ('squad' + args.version)\n        ckpt_name = '{}_{}_{}.params'.format(args.model_name, version_prefix, (step_num + 1))\n        params_saved = os.path.join(args.output_dir, ckpt_name)\n        qa_net.save_parameters(params_saved)\n        ckpt_candidates = [f for f in os.listdir(args.output_dir) if f.endswith('.params')]\n        if (len(ckpt_candidates) > args.max_saved_ckpt):\n            ckpt_candidates.sort(key=(lambda ele: (len(ele), ele)))\n            os.remove(os.path.join(args.output_dir, ckpt_candidates[0]))\n        logging.info('Params saved in: {}'.format(params_saved))\n    if (((step_num + 1) % log_interval) == 0):\n        log_span_loss \/= log_sample_num\n        log_answerable_loss \/= log_sample_num\n        log_total_loss \/= log_sample_num\n        toc = time.time()\n        logging.info('Step: {}\/{}, Loss span\/answer\/total={:.4f}\/{:.4f}\/{:.4f}, LR={:.8f}, grad_norm={:.4f}. Time cost={:.2f}, Throughput={:.2f} samples\/s ETA={:.2f}h'.format((step_num + 1), num_train_steps, log_span_loss, log_answerable_loss, log_total_loss, trainer.learning_rate, total_norm, (toc - tic), (log_sample_num \/ (toc - tic)), (((num_train_steps - (step_num + 1)) \/ ((step_num + 1) \/ (toc - global_tic))) \/ 3600)))\n        tic = time.time()\n        log_span_loss = 0\n        log_answerable_loss = 0\n        log_total_loss = 0\n        log_sample_num = 0\n    if ((step_num + 1) >= num_train_steps):\n        toc = time.time()\n        logging.info('Finish training step: {} within {} hours'.format((step_num + 1), ((toc - global_tic) \/ 3600)))\n        break\nreturn params_saved\n"}
{"label_name":"save","label":1,"method_name":"_save_model_with_dict_input_output","method":"\n'Writes SavedModel using dicts to compute x+y, x+2y and maybe x-y.'\n\n@tf.function\ndef call_fn(d, return_dict=False):\n    x = d['x']\n    y = d['y']\n    sigma = tf.concat([tf.add(x, y), tf.add(x, (2 * y))], axis=(- 1))\n    if return_dict:\n        return dict(sigma=sigma, delta=tf.subtract(x, y))\n    else:\n        return sigma\nd_spec = dict(x=tf.TensorSpec(shape=(None, 1), dtype=tf.float32), y=tf.TensorSpec(shape=(None, 1), dtype=tf.float32))\nfor return_dict in (False, True):\n    call_fn.get_concrete_function(d_spec, return_dict=return_dict)\nobj = tf.train.Checkpoint()\nobj.__call__ = call_fn\ntf.saved_model.save(obj, export_dir)\n"}
{"label_name":"process","label":2,"method_name":"get_preprocessing","method":"\n'Returns preprocessing_fn(image, height, width, **kwargs).\\n\\n  Args:\\n    name: The name of the preprocessing function.\\n    is_training: `True` if the model is being used for training and `False`\\n      otherwise.\\n\\n  Returns:\\n    preprocessing_fn: A function that preprocessing a single image (pre-batch).\\n      It has the following signature:\\n        image = preprocessing_fn(image, output_height, output_width, ...).\\n\\n  Raises:\\n    ValueError: If Preprocessing `name` is not recognized.\\n  '\npreprocessing_fn_map = {'cifarnet': cifarnet_preprocessing, 'inception': inception_preprocessing, 'inception_v1': inception_preprocessing, 'inception_v2': inception_preprocessing, 'inception_v3': inception_preprocessing, 'inception_v4': inception_preprocessing, 'inception_resnet_v2': inception_preprocessing, 'lenet': lenet_preprocessing, 'mobilenet_v1': inception_preprocessing, 'nasnet_mobile': inception_preprocessing, 'nasnet_large': inception_preprocessing, 'resnet_v1_50': vgg_preprocessing, 'resnet_v1_101': vgg_preprocessing, 'resnet_v1_152': vgg_preprocessing, 'resnet_v1_200': vgg_preprocessing, 'resnet_v2_50': vgg_preprocessing, 'resnet_v2_101': vgg_preprocessing, 'resnet_v2_152': vgg_preprocessing, 'resnet_v2_200': vgg_preprocessing, 'vgg': vgg_preprocessing, 'vgg_a': vgg_preprocessing, 'vgg_16': vgg_preprocessing, 'vgg_19': vgg_preprocessing}\nif (name not in preprocessing_fn_map):\n    raise ValueError(('Preprocessing name [%s] was not recognized' % name))\n\ndef preprocessing_fn(image, output_height, output_width, **kwargs):\n    return preprocessing_fn_map[name].preprocess_image(image, output_height, output_width, is_training=is_training, **kwargs)\nreturn preprocessing_fn\n"}
{"label_name":"train","label":0,"method_name":"load_train_data","method":"\next = filename.split('.')[(- 1)]\nif (ext == 'csv'):\n    return read_smiles_csv(filename)\nif (ext == 'smi'):\n    return read_smi(filename)\nelse:\n    raise ValueError('data is not smi or csv!')\nreturn\n"}
{"label_name":"predict","label":4,"method_name":"create_prediction_sample","method":"\nsample = Sample()\nsample.left_str = context\nsample.left_tokens = bpe_model.EncodeAsPieces(sample.left_str)\nnb_segm = sample.left_str.count('|')\nisegm = nb_segm\nsample.segments = []\nfor token in sample.left_tokens:\n    sample.segments.append(isegm)\n    if (token == '|'):\n        isegm -= 1\nsample.right_str = ''\nsample.right_tokens = []\nreturn sample\n"}
{"label_name":"save","label":1,"method_name":"save_variables_to_ckpt","method":"\ninit_all_op = [variables.global_variables_initializer()]\nwith tf_session.Session() as sess:\n    sess.run(init_all_op)\n    saver.Saver().save(sess, os.path.join(model_dir, 'model.ckpt'))\n"}
{"label_name":"train","label":0,"method_name":"trainGradAscent","method":"\n'\\n    Implement gradient ascent in targetFunction\\n    '\nlistFilterImages = []\nfloatLearningRate = 0.01\nfor i in range(intIterationSteps):\n    (floatLossValue, arrayGradientsValue) = targetFunction([arrayInputImageData, 0])\n    arrayInputImageData += (arrayGradientsValue * floatLearningRate)\n    if ((i % intRecordFrequent) == 0):\n        listFilterImages.append((arrayInputImageData, floatLossValue))\n        print('#{}, loss rate: {}'.format(i, floatLossValue))\nreturn listFilterImages\n"}
{"label_name":"train","label":0,"method_name":"train_CartPole","method":"\nmodel = SimpleNeuralNetwork([4, 24, 2])\nenv = CartPoleEnv()\nqnetwork = DeepQNetwork(model=model, env=env, learning_rate=0.0001, logdir='.\/tmp\/CartPole\/')\nqnetwork.train(4000)\n"}
{"label_name":"predict","label":4,"method_name":"create_predict_function","method":"\n'Creates a predict function and registers it to\\n    the Flask app using the route decorator.\\n\\n    :param str route:\\n      Path of the entry point.\\n\\n    :param palladium.interfaces.PredictService predict_service:\\n      The predict service to be registered to this entry point.\\n\\n    :param str decorator_list_name:\\n      The decorator list to be used for this predict service. It is\\n      OK if there is no such entry in the active Palladium config.\\n\\n    :return:\\n      A predict service function that will be used to process\\n      predict requests.\\n    '\nmodel_persister = config.get('model_persister')\n\n@app.route(route, methods=['GET', 'POST'], endpoint=route)\n@PluggableDecorator(decorator_list_name)\ndef predict_func():\n    return predict(model_persister, predict_service)\nreturn predict_func\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\npaddle.init(use_gpu=False, trainer_count=1)\nTIME_STEP = 10\nx = paddle.layer.data(name='x', type=paddle.data_type.dense_vector_sequence(TIME_STEP))\noutput = network(x)\nif (not is_predict):\n    label = paddle.layer.data(name='y', type=paddle.data_type.dense_vector(dim=1))\n    loss = paddle.layer.mse_cost(input=output, label=label)\n    parameters = paddle.parameters.create(loss)\n    optimizer = paddle.optimizer.Adam(learning_rate=0.001, regularization=paddle.optimizer.L2Regularization(rate=0.0008))\n    trainer = paddle.trainer.SGD(cost=loss, parameters=parameters, update_equation=optimizer)\n    feeding = {'x': 0, 'y': 1}\n\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if ((event.batch_id % 50) == 0):\n                print(('\\n pass %d, Batch: %d cost: %f' % (event.pass_id, event.batch_id, event.cost)))\n            else:\n                sys.stdout.write('.')\n                sys.stdout.flush()\n        if isinstance(event, paddle.event.EndPass):\n            feeding = {'x': 0, 'y': 1}\n            with gzip.open(('output\/params_pass_%d.tar.gz' % event.pass_id), 'w') as f:\n                parameters.to_tar(f)\n            filepath = 'data\/test.data'\n            test_reader = data_provider.data_reader(filepath)\n            result = trainer.test(reader=paddle.batch(test_reader, batch_size=16), feeding=feeding)\n            print(('\\nTest with Pass %d, cost: %s' % (event.pass_id, result.cost)))\n    train_file_path = 'data\/train.data'\n    reader = data_provider.data_reader(train_file_path)\n    trainer.train(paddle.batch(reader=reader, batch_size=128), num_passes=200, event_handler=event_handler, feeding=feeding)\nelse:\n    with gzip.open(model_path, 'r') as openFile:\n        parameters = paddle.parameters.Parameters.from_tar(openFile)\n    result = paddle.infer(input=x_, parameters=parameters, output_layer=output, feeding={'x': 0})\n    return result\n"}
{"label_name":"save","label":1,"method_name":"saveBatchResults","method":"\nsaveBatchResultByField(results, 'ce')\nsaveBatchResultByField(results, 'val_ce')\nsaveBatchResultByField(results, 'accs')\nsaveBatchResultByField(results, 'val_accs')\n"}
{"label_name":"save","label":1,"method_name":"save_params","method":"\n'\\n    Save parameters to file\\n    '\npickle.dump(params, open('params.p', 'wb'))\n"}
{"label_name":"predict","label":4,"method_name":"predict_seq2seq","method":"\n'Predict for sequence to sequence.'\nsrc_tokens = (src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']])\nenc_valid_len = np.array([len(src_tokens)], ctx=device)\nsrc_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\nenc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\nenc_outputs = net.encoder(enc_X, enc_valid_len)\ndec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\ndec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n(output_seq, attention_weight_seq) = ([], [])\nfor _ in range(num_steps):\n    (Y, dec_state) = net.decoder(dec_X, dec_state)\n    dec_X = Y.argmax(axis=2)\n    pred = dec_X.squeeze(axis=0).astype('int32').item()\n    if save_attention_weights:\n        attention_weight_seq.append(net.decoder.attention_weights)\n    if (pred == tgt_vocab['<eos>']):\n        break\n    output_seq.append(pred)\nreturn (' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq)\n"}
{"label_name":"save","label":1,"method_name":"save_balanced_sampled_class_count_hdf5","method":"\n' Resample keys in an HDF5 to generate a near balanced dataset\\n    and save into a new HDF5.\\n    Returns indicies from the original label that were sampled.\\n    Not suitable for very large datasets.\\n\\n    Classes with count < target_count will sub-sampled without replacement.\\n    Classes with count > target_count will get over-sampled.\\n    Classes with count equal to target_count will be copied.\\n\\n    fpath -- path to source HDF5 file\\n    keys -- keys to resample (e.g. features)\\n    fpath_dst -- path to destination HDF5 file\\n    Keyword arguments:\\n    key_label -- key for ground truth data in HDF5\\n    other_clname -- name for negative class (None if non-existent)\\n    chunks -- forward chunks parameter to use during hdf5 writing\\n    target_count -- per-class count to target when sampling\\n    '\nif (os.path.abspath(fpath) == os.path.abspath(fpath_dst)):\n    raise IOError(('Cannot read and write to the same file (%s) (%s)' % (fpath, fpath_dst)))\nwith h5py.File(fpath, 'r') as h_src:\n    labls = h_src[key_label][:]\n    bal = Balancer(np.squeeze(labls))\n    class_count = bal.get_class_count(other_clname=other_clname)\n    idxs = bal.get_idxs_to_balance_class_count(class_count.values(), target_count)\n    np.random.shuffle(idxs)\n    with h5py.File(fpath_dst, 'w') as h_dst:\n        h_dst[key_label] = labls[idxs]\n        for k in keys:\n            dataset_src = h_src[k]\n            shape_new = list(dataset_src.shape)\n            shape_new[0] = len(idxs)\n            dataset_dst = h_dst.create_dataset(k, tuple(shape_new), dataset_src.dtype, chunks=chunks)\n            for (idx_dst, idx_src) in enumerate(idxs):\n                dataset_dst[idx_dst] = dataset_src[idx_src]\nreturn idxs\n"}
{"label_name":"process","label":2,"method_name":"preprocess_corpora_multilingual","method":"\nsource_langs = multilingual_utils.get_source_langs(args.lang_pairs.split(','))\ntarget_langs = multilingual_utils.get_target_langs(args.lang_pairs.split(','))\n(dict_paths, dict_objects) = multilingual_utils.prepare_dicts(args, list(set((source_langs + target_langs))))\ntrain_binary_path_config = []\neval_binary_path_config = []\nfor lang_pair in args.lang_pairs.split(','):\n    (source_lang, target_lang) = lang_pair.split('-')\n    (source_corpus, target_corpus) = multilingual_utils.get_parallel_corpus_for_lang_pair(args.multilingual_train_text_file, lang_pair)\n    source_binary_path = maybe_generate_temp_file_path(multilingual_utils.default_binary_path(args.save_dir, lang_pair, source_lang, 'train'))\n    binarize_text_file(text_file=source_corpus, dictionary=dict_objects[source_lang], output_path=source_binary_path, append_eos=args.append_eos_to_source, reverse_order=args.reverse_source)\n    target_binary_path = maybe_generate_temp_file_path(multilingual_utils.default_binary_path(args.save_dir, lang_pair, target_lang, 'train'))\n    binarize_text_file(text_file=target_corpus, dictionary=dict_objects[target_lang], output_path=target_binary_path, append_eos=True, reverse_order=False)\n    train_binary_path_config.append(f'{lang_pair}:{source_binary_path},{target_binary_path}')\n    (source_corpus, target_corpus) = multilingual_utils.get_parallel_corpus_for_lang_pair(args.multilingual_eval_text_file, lang_pair)\n    source_binary_path = maybe_generate_temp_file_path(multilingual_utils.default_binary_path(args.save_dir, lang_pair, source_lang, 'eval'))\n    binarize_text_file(text_file=source_corpus, dictionary=dict_objects[source_lang], output_path=source_binary_path, append_eos=args.append_eos_to_source, reverse_order=args.reverse_source)\n    target_binary_path = maybe_generate_temp_file_path(multilingual_utils.default_binary_path(args.save_dir, lang_pair, target_lang, 'eval'))\n    binarize_text_file(text_file=target_corpus, dictionary=dict_objects[target_lang], output_path=target_binary_path, append_eos=True, reverse_order=False)\n    eval_binary_path_config.append(f'{lang_pair}:{source_binary_path},{target_binary_path}')\nargs.vocabulary = [f'{lang}:{dict_paths[lang]}' for lang in dict_paths]\nargs.multilingual_train_binary_path = train_binary_path_config\nargs.multilingual_eval_binary_path = eval_binary_path_config\n"}
{"label_name":"train","label":0,"method_name":"_create_training_files","method":"\nroot_path = Path(source_files_dir)\nfor dirname in DATASET.values():\n    dataset_path = root_path.joinpath(dirname)\n    dataset_path.mkdir()\n    dataset_path.joinpath('xxx.c').write_text(C_CODE)\n    dataset_path.joinpath('xxx.py').write_text(PYTHON_CODE)\n"}
{"label_name":"predict","label":4,"method_name":"_fit_and_predict","method":"\n\"Fit estimator and predict values for a given dataset split.\\n\\n    Read more in the :ref:`User Guide <cross_validation>`.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit' and 'predict'\\n        The object to use to fit the data.\\n\\n    X : array-like of shape at least 2D\\n        The data to fit.\\n\\n    y : array-like, optional, default: None\\n        The target variable to try to predict in the case of\\n        supervised learning.\\n\\n    train : array-like, shape (n_train_samples,)\\n        Indices of training samples.\\n\\n    test : array-like, shape (n_test_samples,)\\n        Indices of test samples.\\n\\n    verbose : integer\\n        The verbosity level.\\n\\n    fit_params : dict or None\\n        Parameters that will be passed to ``estimator.fit``.\\n\\n    method : string\\n        Invokes the passed method name of the passed estimator.\\n\\n    Returns\\n    -------\\n    predictions : sequence\\n        Result of calling 'estimator.method'\\n\\n    test : array-like\\n        This is the value of the test parameter\\n    \"\nfit_params = (fit_params if (fit_params is not None) else {})\nfit_params = dict([(k, _index_param_value(X, v, train)) for (k, v) in fit_params.items()])\n(X_train, y_train) = _safe_split(estimator, X, y, train)\n(X_test, _) = _safe_split(estimator, X, y, test, train)\nif (y_train is None):\n    estimator.fit(X_train, **fit_params)\nelse:\n    estimator.fit(X_train, y_train, **fit_params)\nfunc = getattr(estimator, method)\npredictions = func(X_test)\nif (method in ['decision_function', 'predict_proba', 'predict_log_proba']):\n    n_classes = len(set(y))\n    if (n_classes != len(estimator.classes_)):\n        recommendation = 'To fix this, use a cross-validation technique resulting in properly stratified folds'\n        warnings.warn('Number of classes in training fold ({}) does not match total number of classes ({}). Results may not be appropriate for your use case. {}'.format(len(estimator.classes_), n_classes, recommendation), RuntimeWarning)\n        if (method == 'decision_function'):\n            if ((predictions.ndim == 2) and (predictions.shape[1] != len(estimator.classes_))):\n                raise ValueError('Output shape {} of {} does not match number of classes ({}) in fold. Irregular decision_function outputs are not currently supported by cross_val_predict'.format(predictions.shape, method, len(estimator.classes_), recommendation))\n            if (len(estimator.classes_) <= 2):\n                raise ValueError('Only {} class\/es in training fold, this is not supported for decision_function with imbalanced folds. {}'.format(len(estimator.classes_), recommendation))\n        float_min = np.finfo(predictions.dtype).min\n        default_values = {'decision_function': float_min, 'predict_log_proba': float_min, 'predict_proba': 0}\n        predictions_for_all_classes = np.full((_num_samples(predictions), n_classes), default_values[method])\n        predictions_for_all_classes[:, estimator.classes_] = predictions\n        predictions = predictions_for_all_classes\nreturn (predictions, test)\n"}
{"label_name":"process","label":2,"method_name":"process_decoder_input","method":"\n'\\n    Preprocess target data for encoding\\n    :param target_data: Target Placehoder\\n    :param target_vocab_to_int: Dictionary to go from the target words to an id\\n    :param batch_size: Batch Size\\n    :return: Preprocessed target data\\n    '\nreturn None\n"}
{"label_name":"process","label":2,"method_name":"process_emp_length","method":"\nx = re_not_decimal.sub('', x)\nif (x == ''):\n    return np.nan\nelse:\n    return float(x)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n'\\n  This function contains the loop that actually trains the model.\\n  :param images: a numpy array with the input data\\n  :param labels: a numpy array with the output labels\\n  :param ckpt_path: a path (including name) where model checkpoints are saved\\n  :param dropout: Boolean, whether to use dropout or not\\n  :return: True if everything went well\\n  '\nassert (len(images) == len(labels))\nassert (images.dtype == np.float32)\nassert (labels.dtype == np.int32)\nwith tf.Graph().as_default():\n    global_step = tf.Variable(0, trainable=False)\n    train_data_node = _input_placeholder()\n    train_labels_shape = (FLAGS.batch_size,)\n    train_labels_node = tf.placeholder(tf.int32, shape=train_labels_shape)\n    print('Done Initializing Training Placeholders')\n    if FLAGS.deeper:\n        logits = inference_deeper(train_data_node, dropout=dropout)\n    else:\n        logits = inference(train_data_node, dropout=dropout)\n    loss = loss_fun(logits, train_labels_node)\n    train_op = train_op_fun(loss, global_step)\n    saver = tf.train.Saver(tf.global_variables())\n    print('Graph constructed and saver created')\n    init = tf.global_variables_initializer()\n    sess = tf.Session(config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n    print('Session ready, beginning training loop')\n    data_length = len(images)\n    nb_batches = math.ceil((data_length \/ FLAGS.batch_size))\n    for step in xrange(FLAGS.max_steps):\n        start_time = time.time()\n        batch_nb = (step % nb_batches)\n        (start, end) = utils.batch_indices(batch_nb, data_length, FLAGS.batch_size)\n        feed_dict = {train_data_node: images[start:end], train_labels_node: labels[start:end]}\n        (_, loss_value) = sess.run([train_op, loss], feed_dict=feed_dict)\n        duration = (time.time() - start_time)\n        assert (not np.isnan(loss_value)), 'Model diverged with loss = NaN'\n        if ((step % 100) == 0):\n            num_examples_per_step = FLAGS.batch_size\n            examples_per_sec = (num_examples_per_step \/ duration)\n            sec_per_batch = float(duration)\n            format_str = '%s: step %d, loss = %.2f (%.1f examples\/sec; %.3f sec\/batch)'\n            print((format_str % (datetime.now(), step, loss_value, examples_per_sec, sec_per_batch)))\n        if (((step % 1000) == 0) or ((step + 1) == FLAGS.max_steps)):\n            saver.save(sess, ckpt_path, global_step=step)\nreturn True\n"}
{"label_name":"predict","label":4,"method_name":"_predictive_sequential","method":"\ncollected = []\nsamples = [{k: v[i] for (k, v) in posterior_samples.items()} for i in range(num_samples)]\nfor i in range(num_samples):\n    trace = poutine.trace(poutine.condition(model, samples[i])).get_trace(*model_args, **model_kwargs)\n    if return_trace:\n        collected.append(trace)\n    else:\n        collected.append({site: trace.nodes[site]['value'] for site in return_site_shapes})\nif return_trace:\n    return collected\nelse:\n    return {site: torch.stack([s[site] for s in collected]).reshape(shape) for (site, shape) in return_site_shapes.items()}\n"}
{"label_name":"predict","label":4,"method_name":"analyse_predictions","method":"\nif (analyses_directory is None):\n    analyses_directory = defaults['analyses']['directory']\nprint('Saving predictions.')\npredictions_directory = os.path.join(analyses_directory, 'predictions')\ntable_name = 'predictions'\nif evaluation_set.prediction_specifications:\n    table_name += ('-' + evaluation_set.prediction_specifications.name)\nelse:\n    table_name += '-unknown_prediction_method'\nif evaluation_set.has_predicted_cluster_ids:\n    saving_time_start = time()\n    save_values(values=evaluation_set.predicted_cluster_ids, name='{}-predicted_cluster_ids'.format(table_name), row_names=evaluation_set.example_names, column_names=['Cluster ID'], directory=predictions_directory)\n    saving_duration = (time() - saving_time_start)\n    print('    Predicted cluster IDs saved ({}).'.format(format_duration(saving_duration)))\nif evaluation_set.has_predicted_labels:\n    saving_time_start = time()\n    save_values(values=evaluation_set.predicted_labels, name='{}-predicted_labels'.format(table_name), row_names=evaluation_set.example_names, column_names=[evaluation_set.terms['class'].capitalize()], directory=predictions_directory)\n    saving_duration = (time() - saving_time_start)\n    print('    Predicted labels saved ({}).'.format(format_duration(saving_duration)))\nif evaluation_set.has_predicted_superset_labels:\n    saving_time_start = time()\n    save_values(values=evaluation_set.predicted_superset_labels, name='{}-predicted_superset_labels'.format(table_name), row_names=evaluation_set.example_names, column_names=[evaluation_set.terms['class'].capitalize()], directory=predictions_directory)\n    saving_duration = (time() - saving_time_start)\n    print('    Predicted superset labels saved ({}).'.format(format_duration(saving_duration)))\nprint()\n"}
{"label_name":"predict","label":4,"method_name":"display_image_predictions","method":"\nn_classes = 10\nlabel_names = _load_label_names()\nlabel_binarizer = LabelBinarizer()\nlabel_binarizer.fit(range(n_classes))\nlabel_ids = label_binarizer.inverse_transform(np.array(labels))\n(fig, axies) = plt.subplots(nrows=4, ncols=2)\nfig.tight_layout()\nfig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\nn_predictions = 3\nmargin = 0.05\nind = np.arange(n_predictions)\nwidth = ((1.0 - (2.0 * margin)) \/ n_predictions)\nfor (image_i, (feature, label_id, pred_indicies, pred_values)) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n    pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n    correct_name = label_names[label_id]\n    axies[image_i][0].imshow((feature * 255))\n    axies[image_i][0].set_title(correct_name)\n    axies[image_i][0].set_axis_off()\n    axies[image_i][1].barh((ind + margin), pred_values[::(- 1)], width)\n    axies[image_i][1].set_yticks((ind + margin))\n    axies[image_i][1].set_yticklabels(pred_names[::(- 1)])\n    axies[image_i][1].set_xticks([0, 0.5, 1.0])\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nlosses = []\nfor e in range(epochs):\n    for (i, (data, label)) in enumerate(train_data):\n        data = data.as_in_context(ctx)\n        label = label.as_in_context(ctx).reshape(((- 1), 1))\n        with autograd.record():\n            output = net(data)\n            loss = loss_fn(output, label)\n        loss.backward()\n        trainer()\n        losses.append(nd.mean(loss).asscalar())\n        if (((i + 1) % 500) == 0):\n            print('Epoch {:3d}, batch {:5d}. Curr loss: {:2.10f}'.format(e, i, nd.mean(loss).asscalar()))\nreturn losses\n"}
{"label_name":"predict","label":4,"method_name":"_make_prediction_and_prepare_results","method":"\nresult = stub.Predict(request, 60.0)\nresult = numpy.array(result.outputs['price'].float_val)\nreturn {'val': str(_inv_transf(result))}\n"}
{"label_name":"save","label":1,"method_name":"save_pred","method":"\nbc = bcolz.carray(pred_arr, mode='w', rootdir=fpath, cparams=bcolz.cparams(clevel=9, cname='lz4'))\nif (meta_dict is not None):\n    bc.attrs['meta'] = meta_dict\nbc.flush()\nreturn bc\n"}
{"label_name":"predict","label":4,"method_name":"cv_predictiveness","method":"\n'\\n    Compute a cross-validated measure of predictiveness based on the data and the chosen measure\\n\\n    @param x: the features\\n    @param y: the outcome\\n    @param S: the covariates to fit\\n    @param measure: measure of predictiveness\\n    @param pred_func: function that fits to the data\\n    @param V: the number of CV folds\\n    @param stratified: should the folds be stratified?\\n    @param na_rm: should we do a complete-case analysis (True) or not (False)\\n    @param folds (dummy)\\n    @param ensemble is this an ensemble (True) or not (False)\\n\\n    @return cross-validated measure of predictiveness, along with preds and ics\\n    '\nimport numpy as np\nfrom .vimpy_utils import make_folds\nif na_rm:\n    xs = x[:, S]\n    cc = (np.sum(np.isnan(xs), axis=1) == 0)\n    newx = x[cc, :]\n    newy = y[cc]\nelse:\n    cc = np.repeat(True, x.shape[0])\n    newx = x\n    newy = y\nfolds = make_folds(newy, V, stratified=stratified)\npreds = np.empty((y.shape[0],))\npreds.fill(np.nan)\nics = np.empty((y.shape[0],))\nics.fill(np.nan)\nvs = np.empty((V,))\ncc_cond = np.flatnonzero(cc)\nif (V == 1):\n    (x_train, y_train) = (newx, newy)\n    pred_func.fit(x_train[:, S], np.ravel(y_train))\n    if ensemble:\n        preds_v = np.mean(pred_func.transform(x_train[:, S]))\n    else:\n        try:\n            preds_v = pred_func.predict_proba(x_train[:, S])[:, 1]\n        except AttributeError:\n            preds_v = pred_func.predict(x_train[:, S])\n    preds[cc_cond] = preds_v\n    vs[0] = measure(y_train, preds_v)\n    ics[cc_cond] = compute_ic(y_train, preds_v, measure.__name__)\nelse:\n    for v in range(V):\n        fold_cond = np.flatnonzero((folds == v))\n        (x_train, y_train) = (newx[(folds != v), :], newy[(folds != v)])\n        (x_test, y_test) = (newx[(folds == v), :], newy[(folds == v)])\n        pred_func.fit(x_train[:, S], np.ravel(y_train))\n        if ensemble:\n            preds_v = np.mean(pred_func.transform(x_test[:, S]))\n        else:\n            try:\n                preds_v = pred_func.predict_proba(x_test[:, S])[:, 1]\n            except AttributeError:\n                preds_v = pred_func.predict(x_test[:, S])\n        preds[cc_cond[fold_cond]] = preds_v\n        vs[v] = measure(y_test, preds_v)\n        ics[cc_cond[fold_cond]] = compute_ic(y_test, preds_v, measure.__name__)\nreturn (np.mean(vs), preds, ics, folds, cc)\n"}
{"label_name":"save","label":1,"method_name":"save_checkpoint_atomic","method":"\n'Wrapper around trainer.save_checkpoint to make save atomic.'\ntemp_filename = os.path.join((final_filename + '.tmp'))\ntrainer.save_checkpoint(temp_filename, extra_state)\nassert PathManager.copy(temp_filename, final_filename, overwrite=True), f'Failed to copy {temp_filename} to {final_filename}'\nPathManager.rm(temp_filename)\n"}
{"label_name":"predict","label":4,"method_name":"predict_classification","method":"\n'\\n    Given feature data and a trained estimator, return a classification prediction\\n\\n    Args:\\n        x_test: \\n        trained_estimator (sklearn.base.BaseEstimator): a trained scikit-learn estimator\\n\\n    Returns:\\n        a prediction\\n    '\nvalidate_estimator(trained_estimator)\nprediction = np.squeeze(trained_estimator.predict_proba(x_test)[:, 1])\nreturn prediction\n"}
{"label_name":"train","label":0,"method_name":"training_job","method":"\nproblem = Problem.query.get(problem_id)\nassert_rights_to_problem(problem)\ndata = db.session.query(TrainingJob.id, TrainingJob.accuracy, TrainingJob.created_at).filter((TrainingJob.problem_id == problem.id)).order_by(TrainingJob.created_at.desc()).all()\nplot_data = db.session.query(db.func.to_char(TrainingJob.created_at, db.text(\"'YYYY-MM-DD HH24:MI:SS'\")), TrainingJob.accuracy).filter((TrainingJob.problem_id == problem.id)).order_by(TrainingJob.created_at.asc()).all()\nreturn render_template('training_job.html', data=data, plot_data=plot_data, problem=problem)\n"}
{"label_name":"save","label":1,"method_name":"_savez","method":"\nimport zipfile\nimport tempfile\nif isinstance(file, basestring):\n    if (not file.endswith('.npz')):\n        file = (file + '.npz')\nelif is_pathlib_path(file):\n    if (not file.name.endswith('.npz')):\n        file = (file.parent \/ (file.name + '.npz'))\nnamedict = kwds\nfor (i, val) in enumerate(args):\n    key = ('arr_%d' % i)\n    if (key in namedict.keys()):\n        raise ValueError(('Cannot use un-named variables and keyword %s' % key))\n    namedict[key] = val\nif compress:\n    compression = zipfile.ZIP_DEFLATED\nelse:\n    compression = zipfile.ZIP_STORED\nzipf = zipfile_factory(file, mode='w', compression=compression)\n(file_dir, file_prefix) = (os.path.split(file) if _is_string_like(file) else (None, 'tmp'))\n(fd, tmpfile) = tempfile.mkstemp(prefix=file_prefix, dir=file_dir, suffix='-numpy.npy')\nos.close(fd)\ntry:\n    for (key, val) in namedict.items():\n        fname = (key + '.npy')\n        fid = open(tmpfile, 'wb')\n        try:\n            format.write_array(fid, np.asanyarray(val), allow_pickle=allow_pickle, pickle_kwargs=pickle_kwargs)\n            fid.close()\n            fid = None\n            zipf.write(tmpfile, arcname=fname)\n        except IOError as exc:\n            raise IOError(('Failed to write to %s: %s' % (tmpfile, exc)))\n        finally:\n            if fid:\n                fid.close()\nfinally:\n    os.remove(tmpfile)\nzipf.close()\n"}
{"label_name":"process","label":2,"method_name":"process_dataset","method":"\nnum_in_contact = 0\nnum_residues = 0\nall_cdrs = []\nall_lbls = []\nall_masks = []\nfor (ag_search, ab_h_chain, ab_l_chain, _, seqs, pdb) in load_chains(summary_file):\n    print('Processing PDB: ', pdb)\n    res = process_chains(ag_search, ab_h_chain, ab_l_chain, seqs, pdb, max_cdr_len=MAX_CDR_LEN)\n    if (res is None):\n        continue\n    (cdrs, lbls, cdr_mask, (nic, nr)) = res\n    num_in_contact += nic\n    num_residues += nr\n    all_cdrs.append(cdrs)\n    all_lbls.append(lbls)\n    all_masks.append(cdr_mask)\ncdrs = np.concatenate(all_cdrs, axis=0)\nlbls = np.concatenate(all_lbls, axis=0)\nmasks = np.concatenate(all_masks, axis=0)\nreturn (cdrs, lbls, masks, (num_residues \/ num_in_contact))\n"}
{"label_name":"process","label":2,"method_name":"_process_transform_argument","method":"\ntform = (tform if (tform is not None) else _pass_through)\nif is_tuple_or_list(tform):\n    if (len(tform) != num_inputs):\n        raise Exception('If transform is list, must provide one transform for each input')\n    tform = [(t if (t is not None) else _pass_through) for t in tform]\nelse:\n    tform = ([tform] * num_inputs)\nreturn tform\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n_changeable_range = 6\ny = alex_net(x, weights, biases)\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\nadam_optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\nprediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\nfor i in range(_epochs):\n    print('Starting training')\n    for k in range(_changeable_range):\n        print(('Start minibatch ' + str(k)))\n        (image_btch, label_btch) = gen_next_batch(k)\n        (_, btch_acc) = sess.run([adam_optimizer, accuracy], feed_dict={x: image_btch, y_: label_btch})\n        print(('End minibatch' + str(btch_acc)))\nprint('End training')\ntest_acc = sess.run(accuracy, feed_dict={x: images[(- 82):], y_: labels[(- 82):]})\nprint(test_acc)\n"}
{"label_name":"process","label":2,"method_name":"_process_image_path_list","method":"\nwrite_features = (data.image_set_features_dir is not None)\nif write_features:\n    etal.ExposesFeatures.ensure_exposes_features(classifier)\n    features_handler = etaf.ImageSetFeaturesHandler(data.image_set_features_dir)\nif data.input_image_set_labels_path:\n    logger.info(\"Reading existing labels from '%s'\", data.input_image_set_labels_path)\n    image_set_labels = etai.ImageSetLabels.from_json(data.input_image_set_labels_path)\nelse:\n    image_set_labels = etai.ImageSetLabels()\nfor inpath in inpaths:\n    logger.info(\"Processing image '%s'\", inpath)\n    filename = os.path.basename(inpath)\n    img = etai.read(inpath)\n    attrs = _classify_image(img, classifier, attr_filter, record_top_k_probs)\n    if write_features:\n        fvec = classifier.get_features()\n        features_handler.write_feature(fvec, filename)\n    image_set_labels[filename].add_attributes(attrs)\nlogger.info(\"Writing labels to '%s'\", data.output_image_set_labels_path)\nimage_set_labels.write_json(data.output_image_set_labels_path)\n"}
{"label_name":"train","label":0,"method_name":"_get_training_labels","method":"\nreturn [x[1] for x in os.walk(training_dir_path)][0]\n"}
{"label_name":"process","label":2,"method_name":"process_file_init","method":"\nprocess_file.queue = queue\nprocess_file.options = options\n"}
{"label_name":"train","label":0,"method_name":"get_train_set","method":"\ndf = pd.read_csv(PATH_SPAM_TRAIN, names=COLUMNS)\nX = np.array(df.drop(['data_id', 'label'], axis=1))\ny = np.hstack((np.array(df[['label']]), (1 - np.array(df[['label']]))))\nnum_data = X.shape[0]\nnum_train = int((VALID_RATIO * num_data))\nX_train = X[0:num_train, :]\ny_train = y[0:num_train, :]\nX_valid = X[num_train:, :]\ny_valid = y[num_train:, :]\nreturn (X_train, y_train, X_valid, y_valid)\n"}
{"label_name":"forward","label":3,"method_name":"lstm_forward","method":"\noutdim = (W.shape[(- 1)] \/\/ 4)\n(time, batch, indim) = X.shape\nZ = np.zeros((time, batch, (indim + outdim)))\nO = np.zeros((time, batch, outdim))\nT = np.zeros((time, 6, batch, outdim))\nfor t in range(time):\n    Z[t] = np.concatenate((X[t], O[(t - 1)]), axis=(- 1))\n    p = (np.dot(Z[t], W) + b)\n    p[:, :outdim] = activation(p[:, :outdim])\n    p[:, outdim:] = sigmoid(p[:, outdim:])\n    T[(t, 2)] = p[:, :outdim]\n    T[(t, 3)] = p[:, outdim:(2 * outdim)]\n    T[(t, 4)] = p[:, (2 * outdim):(3 * outdim)]\n    T[(t, 5)] = p[:, (3 * outdim):]\n    T[(t, 0)] = ((T[((t - 1), 0)] * T[(t, 3)]) + (T[(t, 2)] * T[(t, 4)]))\n    T[(t, 1)] = activation(T[(t, 0)])\n    O[t] = (T[(t, 1)] * T[(t, 5)])\nreturn np.concatenate((O.ravel(), Z.ravel(), T.ravel()))\n"}
{"label_name":"process","label":2,"method_name":"get_preprocessed_data","method":"\n'Get preprocessed data for a machine learning pipeline.\\n\\n    Arguments:\\n        data_dir: string\\n            directory where data files are located\\n        data_fn: string\\n            filename of data file\\n        prop_missing: float\\n            proportion of feature observations which should be randomly masked;\\n            values in [0, 1)\\n\\n    Returns:\\n        x_unvec: [[int]]\\n            feature indices that have not been vectorized; each inner list\\n            collects the indices of features that are present (binary on)\\n            for a sample\\n        y: [int]\\n            list of class labels as integer indices\\n        idx_feat_dict: {int: string}\\n            dictionary mapping feature indices to features\\n        idx_class_dict: {int: string}\\n            dictionary mapping class indices to classes\\n        icd9_descript_dict: {string: string}\\n            dictionary mapping ICD9 codes to description text\\n        perm_indices: np.ndarray, int\\n            array of indices representing a permutation of the samples with\\n            shape (num_sample, )\\n    '\nprint('Loading data...')\nstart = time.time()\nicd9_descript_path = get_icd9_descript_path(data_dir)\ndata_path = get_data_path(data_dir, data_fn)\nperm_indices_path = get_perm_indices_path(data_dir, data_fn)\nicd9_descript_dict = emr.get_icd9_descript_dict(icd9_descript_path)\n(x_unvec, y, idx_feat_dict, idx_class_dict) = emr.get_data(path=data_path, icd9_descript_dict=icd9_descript_dict)\nnum_sample = len(x_unvec)\nperm_indices = np.random.permutation(num_sample)\nif os.path.isfile(perm_indices_path):\n    with open(perm_indices_path, 'rb') as f:\n        expected_perm_indices = pickle.load(f)\n    assert np.all((perm_indices == expected_perm_indices))\nelse:\n    with open(perm_indices_path, 'wb') as f:\n        pickle.dump(perm_indices, f)\nif (prop_missing != 0.0):\n    x_unvec = _simulate_missing_data(x_unvec, prop_missing=prop_missing)\nprint('Data loaded in {:.5f} s'.format((time.time() - start)))\nprint()\nreturn (x_unvec, y, idx_feat_dict, idx_class_dict, icd9_descript_dict, perm_indices)\n"}
{"label_name":"save","label":1,"method_name":"validate_save_restore","method":"\n\"Helper method to check if your Trainable class will resume correctly.\\n\\n    Args:\\n        trainable_cls: Trainable class for evaluation.\\n        config (dict): Config to pass to Trainable when testing.\\n        num_gpus (int): GPU resources to allocate when testing.\\n        use_object_store (bool): Whether to save and restore to Ray's object\\n            store. Recommended to set this to True if planning to use\\n            algorithms that pause training (i.e., PBT, HyperBand).\\n    \"\nassert ray.is_initialized(), 'Need Ray to be initialized.'\nremote_cls = ray.remote(num_gpus=num_gpus)(trainable_cls)\ntrainable_1 = remote_cls.remote(config=config)\ntrainable_2 = remote_cls.remote(config=config)\nfrom ray.tune.result import TRAINING_ITERATION\nfor _ in range(3):\n    res = ray.get(trainable_1.train.remote())\nassert res.get(TRAINING_ITERATION), 'Validation will not pass because it requires `training_iteration` to be returned.'\nif use_object_store:\n    restore_check = trainable_2.restore_from_object.remote(trainable_1.save_to_object.remote())\n    ray.get(restore_check)\nelse:\n    restore_check = ray.get(trainable_2.restore.remote(trainable_1.save.remote()))\nres = ray.get(trainable_2.train.remote())\nassert (res[TRAINING_ITERATION] == 4)\nres = ray.get(trainable_2.train.remote())\nassert (res[TRAINING_ITERATION] == 5)\nreturn True\n"}
{"label_name":"train","label":0,"method_name":"training","method":"\n'Sets up the training Ops.\\n\\n  Creates a summarizer to track the loss over time in TensorBoard.\\n\\n  Creates an optimizer and applies the gradients to all trainable variables.\\n\\n  The Op returned by this function is what must be passed to the\\n  `sess.run()` call to cause the model to train.\\n\\n  Args:\\n    loss: Loss tensor, from loss().\\n    learning_rate: The learning rate to use for gradient descent.\\n\\n  Returns:\\n    train_op: The Op for training.\\n  '\ntf.summary.scalar('loss', loss)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\ntrain_op = optimizer.minimize(loss, global_step=global_step)\nreturn train_op\n"}
{"label_name":"predict","label":4,"method_name":"predict_video","method":"\nif save_path:\n    save_path = (os.path.splitext(save_path)[0] + '.mp4')\n    try:\n        writer = skvideo.io.FFmpegWriter(save_path)\n    except AssertionError as e:\n        tf.logging.error(e)\n        tf.logging.error('Please install ffmpeg before making video predictions.')\n        exit()\nelse:\n    click.echo('Video not being saved. Note that for the time being, no JSON output is being generated. Did you mean to specify `--save-path`?')\nnum_of_frames = int(skvideo.io.ffprobe(path)['video']['@nb_frames'])\nvideo_progress_bar = click.progressbar(skvideo.io.vreader(path), length=num_of_frames, label='Predicting {}'.format(path))\ncolormap = build_colormap()\nobjects_per_frame = []\nwith video_progress_bar as bar:\n    try:\n        start_time = time.time()\n        for (idx, frame) in enumerate(bar):\n            objects = network.predict_image(frame)\n            objects = filter_classes(objects, only_classes=only_classes, ignore_classes=ignore_classes)\n            objects_per_frame.append({'frame': idx, 'objects': objects})\n            if save_path:\n                image = vis_objects(frame, objects, colormap=colormap)\n                writer.writeFrame(np.array(image))\n        stop_time = time.time()\n        click.echo('fps: {0:.1f}'.format((num_of_frames \/ (stop_time - start_time))))\n    except RuntimeError as e:\n        click.echo()\n        click.echo('Error while processing {}: {}'.format(path, e))\n        if save_path:\n            click.echo('Partially processed video file saved in {}'.format(save_path))\nif save_path:\n    writer.close()\nreturn objects_per_frame\n"}
{"label_name":"train","label":0,"method_name":"training","method":"\nrank = idist.get_rank()\nmanual_seed((config['seed'] + rank))\ndevice = idist.device()\nlogger = setup_logger(name='CIFAR10-QAT-Training', distributed_rank=local_rank)\nlog_basic_info(logger, config)\noutput_path = config['output_path']\nif (rank == 0):\n    now = datetime.now().strftime('%Y%m%d-%H%M%S')\n    folder_name = f\"{config['model']}_backend-{idist.backend()}-{idist.get_world_size()}_{now}\"\n    output_path = (Path(output_path) \/ folder_name)\n    if (not output_path.exists()):\n        output_path.mkdir(parents=True)\n    config['output_path'] = output_path.as_posix()\n    logger.info(f\"Output path: {config['output_path']}\")\n    if ('cuda' in device.type):\n        config['cuda device name'] = torch.cuda.get_device_name(local_rank)\n    if config['with_clearml']:\n        try:\n            from clearml import Task\n        except ImportError:\n            from trains import Task\n        task = Task.init('CIFAR10-Training', task_name=output_path.stem)\n        task.connect_configuration(config)\n        hyper_params = ['model', 'batch_size', 'momentum', 'weight_decay', 'num_epochs', 'learning_rate', 'num_warmup_epochs']\n        task.connect({k: config[k] for k in hyper_params})\n(train_loader, test_loader) = get_dataflow(config)\nconfig['num_iters_per_epoch'] = len(train_loader)\n(model, optimizer, criterion, lr_scheduler) = initialize(config)\ntrainer = create_trainer(model, optimizer, criterion, lr_scheduler, train_loader.sampler, config, logger)\nmetrics = {'Accuracy': Accuracy(), 'Loss': Loss(criterion)}\nevaluator = create_supervised_evaluator(model, metrics=metrics, device=device, non_blocking=True)\ntrain_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device, non_blocking=True)\n\ndef run_validation(engine):\n    epoch = trainer.state.epoch\n    state = train_evaluator.run(train_loader)\n    log_metrics(logger, epoch, state.times['COMPLETED'], 'Train', state.metrics)\n    state = evaluator.run(test_loader)\n    log_metrics(logger, epoch, state.times['COMPLETED'], 'Test', state.metrics)\ntrainer.add_event_handler((Events.EPOCH_COMPLETED(every=config['validate_every']) | Events.COMPLETED), run_validation)\nif (rank == 0):\n    evaluators = {'training': train_evaluator, 'test': evaluator}\n    tb_logger = common.setup_tb_logging(output_path, trainer, optimizer, evaluators=evaluators)\nbest_model_handler = Checkpoint({'model': model}, get_save_handler(config), filename_prefix='best', n_saved=2, global_step_transform=global_step_from_engine(trainer), score_name='test_accuracy', score_function=Checkpoint.get_default_score_fn('Accuracy'))\nevaluator.add_event_handler(Events.COMPLETED((lambda *_: (trainer.state.epoch > (config['num_epochs'] \/\/ 2)))), best_model_handler)\ntry:\n    trainer.run(train_loader, max_epochs=config['num_epochs'])\nexcept Exception as e:\n    logger.exception('')\n    raise e\nif (rank == 0):\n    tb_logger.close()\n"}
{"label_name":"process","label":2,"method_name":"process_texts","method":"\nconfig = create_pretrained_transformers_config(model_name, model_weights)\nwhitespace_tokenizer = WhitespaceTokenizer()\nlm_featurizer = LanguageModelFeaturizer(config)\nmessages = []\nfor text in texts:\n    message = Message.build(text=text)\n    whitespace_tokenizer.process(message)\n    lm_featurizer.process(message)\n    messages.append(message)\nreturn messages\n"}
{"label_name":"save","label":1,"method_name":"_save","method":"\nif ((_handler is None) or (not hasattr('_handler', 'save'))):\n    raise IOError('FITS save handler not installed')\n_handler.save(im, fp, filename)\n"}
{"label_name":"train","label":0,"method_name":"traindata","method":"\n(_, nimages, nrows, ncols) = imageheader(TRAINIMAGES)\nfeatures = np.empty(((nrows * ncols), nimages))\nlabels = np.empty(nimages)\nfor index in xrange(nimages):\n    features[:, index] = trainfeatures(index)\n    labels[index] = trainlabel(index)\nreturn (features, labels)\n"}
{"label_name":"predict","label":4,"method_name":"ens_prediction_files","method":"\npreds = [predictions.load_pred(f) for f in pred_fpaths]\nn_inputs = preds[0].shape[0]\nif os.path.exists(ens_fpath):\n    print('Ens file exists. Overwriting')\n    time.sleep(2)\n    shutil.rmtree(ens_fpath)\ni = 0\nstart = time.time()\nwhile (i < n_inputs):\n    pred_block = np.array([p[i:(i + block_size)] for p in preds])\n    ens_block = predictions.ensemble_with_method(pred_block, method)\n    if (i == 0):\n        ens_pred = predictions.save_pred(ens_fpath, ens_block, meta)\n    else:\n        ens_pred = predictions.append_to_pred(ens_pred, ens_block)\n    i += block_size\nprint(utils.logger.get_time_msg(start))\nreturn ens_fpath\n"}
{"label_name":"save","label":1,"method_name":"image_save","method":"\nstd = 1\nx_z = np.linspace(((- 3) * std), (3 * std), 20)\ny_z = np.linspace(((- 3) * std), (3 * std), 20)\nout = np.empty(((28 * 20), (28 * 20)))\nfor (x_idx, x) in enumerate(x_z):\n    for (y_idx, y) in enumerate(y_z):\n        z_mu = np.random.uniform((- 1), 1, [16, 10])\n        img = model.generate_samples(z_mu)\n        out[(x_idx * 28):((x_idx + 1) * 28), (y_idx * 28):((y_idx + 1) * 28)] = img[0].reshape(28, 28)\nplt.imsave(path, out, cmap='gray')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\ng_input_size = 1\ng_hidden_size = 5\ng_output_size = 1\nd_input_size = 500\nd_hidden_size = 10\nd_output_size = 1\nminibatch_size = d_input_size\nd_learning_rate = 0.001\ng_learning_rate = 0.001\nsgd_momentum = 0.9\nnum_epochs = 5000\nprint_interval = 100\nd_steps = 20\ng_steps = 20\n(dfe, dre, ge) = (0, 0, 0)\n(d_real_data, d_fake_data, g_fake_data) = (None, None, None)\ndiscriminator_activation_function = torch.sigmoid\ngenerator_activation_function = torch.tanh\nd_sampler = get_distribution_sampler(data_mean, data_stddev)\ngi_sampler = get_generator_input_sampler()\nG = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size, f=generator_activation_function)\nD = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size, f=discriminator_activation_function)\ncriterion = nn.BCELoss()\nd_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\ng_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\nfor epoch in range(num_epochs):\n    for d_index in range(d_steps):\n        D.zero_grad()\n        d_real_data = Variable(d_sampler(d_input_size))\n        d_real_decision = D(preprocess(d_real_data))\n        d_real_error = criterion(d_real_decision, Variable(torch.ones([1, 1])))\n        d_real_error.backward()\n        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n        d_fake_data = G(d_gen_input).detach()\n        d_fake_decision = D(preprocess(d_fake_data.t()))\n        d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1, 1])))\n        d_fake_error.backward()\n        d_optimizer.step()\n        (dre, dfe) = (extract(d_real_error)[0], extract(d_fake_error)[0])\n    for g_index in range(g_steps):\n        G.zero_grad()\n        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n        g_fake_data = G(gen_input)\n        dg_fake_decision = D(preprocess(g_fake_data.t()))\n        g_error = criterion(dg_fake_decision, Variable(torch.ones([1, 1])))\n        g_error.backward()\n        g_optimizer.step()\n        ge = extract(g_error)[0]\n    if ((epoch % print_interval) == 0):\n        print(('Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) ' % (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data)))))\nif matplotlib_is_available:\n    print('Plotting the generated distribution...')\n    values = extract(g_fake_data)\n    print((' Values: %s' % str(values)))\n    plt.hist(values, bins=50)\n    plt.xlabel('Value')\n    plt.ylabel('Count')\n    plt.title('Histogram of Generated Distribution')\n    plt.grid(True)\n    plt.show()\n"}
{"label_name":"save","label":1,"method_name":"show_and_save_image","method":"\n'Shows an image using matplotlib and saves it.'\ntry:\n    import matplotlib.pyplot as plt\nexcept ImportError as e:\n    tf.logging.warning('Showing and saving an image requires matplotlib to be installed: %s', e)\n    raise NotImplementedError('Image display and save not implemented.')\nplt.imshow(img)\nwith tf.gfile.Open(save_path, 'wb') as sp:\n    plt.savefig(sp)\n"}
{"label_name":"predict","label":4,"method_name":"tree_predictions_v4","method":"\n\"Outputs the predictions for the given input data.\\n\\n  Args:\\n    tree_handle: A `Tensor` of type `resource`. The handle to the tree.\\n    input_data: A `Tensor` of type `float32`.\\n      The training batch's features as a 2-d tensor; `input_data[i][j]`\\n      gives the j-th feature of the i-th input.\\n    sparse_input_indices: A `Tensor` of type `int64`.\\n      The indices tensor from the SparseTensor input.\\n    sparse_input_values: A `Tensor` of type `float32`.\\n      The values tensor from the SparseTensor input.\\n    sparse_input_shape: A `Tensor` of type `int64`.\\n      The shape tensor from the SparseTensor input.\\n    input_spec: A `string`.\\n    params: A `string`. A serialized TensorForestParams proto.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A tuple of `Tensor` objects (predictions, tree_paths).\\n\\n    predictions: A `Tensor` of type `float32`. `predictions[i][j]` is the probability that input i is class j.\\n    tree_paths: A `Tensor` of type `string`. `tree_paths[i]` is a serialized TreePath proto for example i.\\n  \"\ninput_spec = _execute.make_str(input_spec, 'input_spec')\nparams = _execute.make_str(params, 'params')\n_ctx = _context.context()\nif _ctx.in_graph_mode():\n    (_, _, _op) = _op_def_lib._apply_op_helper('TreePredictionsV4', tree_handle=tree_handle, input_data=input_data, sparse_input_indices=sparse_input_indices, sparse_input_values=sparse_input_values, sparse_input_shape=sparse_input_shape, input_spec=input_spec, params=params, name=name)\n    _result = _op.outputs[:]\n    _inputs_flat = _op.inputs\n    _attrs = ('input_spec', _op.get_attr('input_spec'), 'params', _op.get_attr('params'))\nelse:\n    tree_handle = _ops.convert_to_tensor(tree_handle, _dtypes.resource)\n    input_data = _ops.convert_to_tensor(input_data, _dtypes.float32)\n    sparse_input_indices = _ops.convert_to_tensor(sparse_input_indices, _dtypes.int64)\n    sparse_input_values = _ops.convert_to_tensor(sparse_input_values, _dtypes.float32)\n    sparse_input_shape = _ops.convert_to_tensor(sparse_input_shape, _dtypes.int64)\n    _inputs_flat = [tree_handle, input_data, sparse_input_indices, sparse_input_values, sparse_input_shape]\n    _attrs = ('input_spec', input_spec, 'params', params)\n    _result = _execute.execute(b'TreePredictionsV4', 2, inputs=_inputs_flat, attrs=_attrs, ctx=_ctx, name=name)\n_execute.record_gradient('TreePredictionsV4', _inputs_flat, _attrs, _result, name)\n_result = _TreePredictionsV4Output._make(_result)\nreturn _result\n"}
{"label_name":"save","label":1,"method_name":"save_converted_labels","method":"\nf = open(filepath, 'wb')\npickle.dump(converted_labels, f)\nf.close()\n"}
{"label_name":"train","label":0,"method_name":"train_evaluate","method":"\npipeline_manager.start_experiment()\npipeline_manager.train(pipeline_name, dev_mode)\npipeline_manager.evaluate(pipeline_name, dev_mode, chunk_size)\npipeline_manager.finish_experiment()\n"}
{"label_name":"train","label":0,"method_name":"train_step","method":"\nwith tf.GradientTape() as tape:\n    (y_pred, logits) = model(x)\n    loss = cross_entropy_loss(logits, y)\ngradients = tape.gradient(loss, vars(model).values())\noptimizer.apply_gradients(zip(gradients, vars(model).values()))\n"}
{"label_name":"train","label":0,"method_name":"sv2_train_output","method":"\n(biallelic_dels, biallelic_dups, male_sex_chrom_dels, male_sex_chrom_dups) = partition_svs(feats, Ped, gen)\nif (len(biallelic_dels) > 0):\n    training_features(init_dataframe(biallelic_dels), 0, opre)\nif (len(biallelic_dups) > 0):\n    training_features(init_dataframe(biallelic_dups), 1, opre)\nif (len(male_sex_chrom_dels) > 0):\n    training_features(init_dataframe(male_sex_chrom_dels), 2, opre)\nif (len(male_sex_chrom_dups) > 0):\n    training_features(init_dataframe(male_sex_chrom_dups), 3, opre)\n"}
{"label_name":"predict","label":4,"method_name":"filter_patient_nodules_predictions","method":"\nsrc_dir = (settings.LUNA_16_TRAIN_DIR2D2 if luna16 else settings.NDSB3_EXTRACTED_IMAGE_DIR)\npatient_mask = helpers.load_patient_images(patient_id, src_dir, '*_m.png')\ndelete_indices = []\nfor (index, row) in df_nodule_predictions.iterrows():\n    z_perc = row['coord_z']\n    y_perc = row['coord_y']\n    center_x = int(round((row['coord_x'] * patient_mask.shape[2])))\n    center_y = int(round((y_perc * patient_mask.shape[1])))\n    center_z = int(round((z_perc * patient_mask.shape[0])))\n    mal_score = row['diameter_mm']\n    start_y = (center_y - (view_size \/ 2))\n    start_x = (center_x - (view_size \/ 2))\n    nodule_in_mask = False\n    for z_index in [(- 1), 0, 1]:\n        img = patient_mask[(z_index + center_z)]\n        start_x = int(start_x)\n        start_y = int(start_y)\n        view_size = int(view_size)\n        img_roi = img[start_y:(start_y + view_size), start_x:(start_x + view_size)]\n        if (img_roi.sum() > 255):\n            nodule_in_mask = True\n    if (not nodule_in_mask):\n        print('Nodule not in mask: ', (center_x, center_y, center_z))\n        if (mal_score > 0):\n            mal_score *= (- 1)\n        df_nodule_predictions.loc[(index, 'diameter_mm')] = mal_score\n    else:\n        if (center_z < 30):\n            print('Z < 30: ', patient_id, ' center z:', center_z, ' y_perc: ', y_perc)\n            if (mal_score > 0):\n                mal_score *= (- 1)\n            df_nodule_predictions.loc[(index, 'diameter_mm')] = mal_score\n        if (((z_perc > 0.75) or (z_perc < 0.25)) and (y_perc > 0.85)):\n            print('SUSPICIOUS FALSEPOSITIVE: ', patient_id, ' center z:', center_z, ' y_perc: ', y_perc)\n        if ((center_z < 50) and (y_perc < 0.3)):\n            print('SUSPICIOUS FALSEPOSITIVE OUT OF RANGE: ', patient_id, ' center z:', center_z, ' y_perc: ', y_perc)\ndf_nodule_predictions.drop(df_nodule_predictions.index[delete_indices], inplace=True)\nreturn df_nodule_predictions\n"}
{"label_name":"train","label":0,"method_name":"raidus_train","method":"\n'\\n    please note that the side of the patch is equal to 2*r_input+1\\n    :return:\\n    '\nr_input = 50\nr_output = 10\nreturn (r_input, r_output)\n"}
{"label_name":"predict","label":4,"method_name":"predict_future","method":"\n'\\n\\tpredict_future(model, frequency)\\n\\n\\tparameters:\\n\\t- model: Prophet model\\n\\t- frequency: number of periods to extend data\\n\\t   frame\\n\\n\\treturns:\\n\\t- pandas dataframe with ds extended into the \\n\\t   future\\n\\t'\nfuture_df = model.make_future_dataframe(periods=frequency)\nforecast = model.predict(future_df)\nreturn forecast\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image_and_label","method":"\n'Preprocesses the image and label.\\n\\n  Args:\\n    image: Input image.\\n    label: Ground truth annotation label.\\n    crop_height: The height value used to crop the image and label.\\n    crop_width: The width value used to crop the image and label.\\n    min_resize_value: Desired size of the smaller image side.\\n    max_resize_value: Maximum allowed size of the larger image side.\\n    resize_factor: Resized dimensions are multiple of factor plus one.\\n    min_scale_factor: Minimum scale factor value.\\n    max_scale_factor: Maximum scale factor value.\\n    scale_factor_step_size: The step size from min scale factor to max scale\\n      factor. The input is randomly scaled based on the value of\\n      (min_scale_factor, max_scale_factor, scale_factor_step_size).\\n    ignore_label: The label value which will be ignored for training and\\n      evaluation.\\n    is_training: If the preprocessing is used for training or not.\\n    model_variant: Model variant (string) for choosing how to mean-subtract the\\n      images. See feature_extractor.network_map for supported model variants.\\n\\n  Returns:\\n    original_image: Original image (could be resized).\\n    processed_image: Preprocessed image.\\n    label: Preprocessed ground truth segmentation label.\\n\\n  Raises:\\n    ValueError: Ground truth label not provided during training.\\n  '\nif (is_training and (label is None)):\n    raise ValueError('During training, label must be provided.')\nif (model_variant is None):\n    tf.logging.warning('Default mean-subtraction is performed. Please specify a model_variant. See feature_extractor.network_map for supported model variants.')\noriginal_image = image\nprocessed_image = tf.cast(image, tf.float32)\nif (label is not None):\n    label = tf.cast(label, tf.int32)\nif ((min_resize_value is not None) or (max_resize_value is not None)):\n    [processed_image, label] = preprocess_utils.resize_to_range(image=processed_image, label=label, min_size=min_resize_value, max_size=max_resize_value, factor=resize_factor, align_corners=True)\n    original_image = tf.identity(processed_image)\nscale = preprocess_utils.get_random_scale(min_scale_factor, max_scale_factor, scale_factor_step_size)\n(processed_image, label) = preprocess_utils.randomly_scale_image_and_label(processed_image, label, scale)\nprocessed_image.set_shape([None, None, 3])\nimage_shape = tf.shape(processed_image)\nimage_height = image_shape[0]\nimage_width = image_shape[1]\ntarget_height = (image_height + tf.maximum((crop_height - image_height), 0))\ntarget_width = (image_width + tf.maximum((crop_width - image_width), 0))\nmean_pixel = tf.reshape(feature_extractor.mean_pixel(model_variant), [1, 1, 3])\nprocessed_image = preprocess_utils.pad_to_bounding_box(processed_image, 0, 0, target_height, target_width, mean_pixel)\nif (label is not None):\n    label = preprocess_utils.pad_to_bounding_box(label, 0, 0, target_height, target_width, ignore_label)\nif (is_training and (label is not None)):\n    (processed_image, label) = preprocess_utils.random_crop([processed_image, label], crop_height, crop_width)\nprocessed_image.set_shape([crop_height, crop_width, 3])\nif (label is not None):\n    label.set_shape([crop_height, crop_width, 1])\nif is_training:\n    (processed_image, label, _) = preprocess_utils.flip_dim([processed_image, label], _PROB_OF_FLIP, dim=1)\nreturn (original_image, processed_image, label)\n"}
{"label_name":"predict","label":4,"method_name":"fuse_prediction_results","method":"\n'\\n\\n    :param gps: list of graph patterns.\\n    :param target_candidate_lists: a list of target_candidate lists as\\n        returned by predict_target_candidates() in same order as gps.\\n    :param fusion_methods: None for all or a list of strings naming the fusion\\n        methods to return.\\n    :return: A dict like {method: ranked_res_list}, where ranked_res_list is a\\n        list result list produced by method of (predicted_target, score) pairs\\n        ordered decreasingly by score. For methods see above.\\n    '\nassert (len(gps) == len(target_candidate_lists))\ntargets_vecs = gp_tcs_to_vecs(gps, target_candidate_lists)\nres = OrderedDict()\nfor fm in get_fusion_methods_from_str(fusion_methods):\n    try:\n        res[fm.name] = fm.fuse(gps, target_candidate_lists, targets_vecs)\n    except NotImplementedError:\n        logger.warning('seems %s is not implemented yet, but called', fm.name)\n        res[fm.name] = []\nreturn res\n"}
{"label_name":"predict","label":4,"method_name":"predictive","method":"\n'\\n    .. warning::\\n        This function is deprecated and will be removed in a future release.\\n        Use the :class:`~pyro.infer.predictive.Predictive` class instead.\\n\\n    Run model by sampling latent parameters from `posterior_samples`, and return\\n    values at sample sites from the forward run. By default, only sites not contained in\\n    `posterior_samples` are returned. This can be modified by changing the `return_sites`\\n    keyword argument.\\n\\n    :param model: Python callable containing Pyro primitives.\\n    :param dict posterior_samples: dictionary of samples from the posterior.\\n    :param args: model arguments.\\n    :param kwargs: model kwargs; and other keyword arguments (see below).\\n\\n    :Keyword Arguments:\\n        * **num_samples** (``int``) - number of samples to draw from the predictive distribution.\\n          This argument has no effect if ``posterior_samples`` is non-empty, in which case, the\\n          leading dimension size of samples in ``posterior_samples`` is used.\\n        * **return_sites** (``list``) - sites to return; by default only sample sites not present\\n          in `posterior_samples` are returned.\\n        * **return_trace** (``bool``) - whether to return the full trace. Note that this is vectorized\\n          over `num_samples`.\\n        * **parallel** (``bool``) - predict in parallel by wrapping the existing model\\n          in an outermost `plate` messenger. Note that this requires that the model has\\n          all batch dims correctly annotated via :class:`~pyro.plate`. Default is `False`.\\n\\n    :return: dict of samples from the predictive distribution, or a single vectorized\\n        `trace` (if `return_trace=True`).\\n    '\nwarnings.warn('The `mcmc.predictive` function is deprecated and will be removed in a future release. Use the `pyro.infer.Predictive` class instead.', FutureWarning)\nnum_samples = kwargs.pop('num_samples', None)\nreturn_sites = kwargs.pop('return_sites', None)\nreturn_trace = kwargs.pop('return_trace', False)\nparallel = kwargs.pop('parallel', False)\nmax_plate_nesting = _guess_max_plate_nesting(model, args, kwargs)\nmodel_trace = prune_subsample_sites(poutine.trace(model).get_trace(*args, **kwargs))\nreshaped_samples = {}\nfor (name, sample) in posterior_samples.items():\n    (batch_size, sample_shape) = (sample.shape[0], sample.shape[1:])\n    if (num_samples is None):\n        num_samples = batch_size\n    elif (num_samples != batch_size):\n        warnings.warn(\"Sample's leading dimension size {} is different from the provided {} num_samples argument. Defaulting to {}.\".format(batch_size, num_samples, batch_size), UserWarning)\n        num_samples = batch_size\n    sample = sample.reshape((((num_samples,) + ((1,) * (max_plate_nesting - len(sample_shape)))) + sample_shape))\n    reshaped_samples[name] = sample\nif (num_samples is None):\n    raise ValueError('No sample sites in model to infer `num_samples`.')\nreturn_site_shapes = {}\nfor site in (model_trace.stochastic_nodes + model_trace.observation_nodes):\n    site_shape = ((num_samples,) + model_trace.nodes[site]['value'].shape)\n    if return_sites:\n        if (site in return_sites):\n            return_site_shapes[site] = site_shape\n    elif (site not in reshaped_samples):\n        return_site_shapes[site] = site_shape\nif (not parallel):\n    return _predictive_sequential(model, posterior_samples, args, kwargs, num_samples, return_site_shapes.keys(), return_trace)\n\ndef _vectorized_fn(fn):\n    '\\n        Wraps a callable inside an outermost :class:`~pyro.plate` to parallelize\\n        sampling from the posterior predictive.\\n\\n        :param fn: arbitrary callable containing Pyro primitives.\\n        :return: wrapped callable.\\n        '\n\n    def wrapped_fn(*args, **kwargs):\n        with pyro.plate('_num_predictive_samples', num_samples, dim=((- max_plate_nesting) - 1)):\n            return fn(*args, **kwargs)\n    return wrapped_fn\ntrace = poutine.trace(poutine.condition(_vectorized_fn(model), reshaped_samples)).get_trace(*args, **kwargs)\nif return_trace:\n    return trace\npredictions = {}\nfor (site, shape) in return_site_shapes.items():\n    value = trace.nodes[site]['value']\n    if (value.numel() < reduce((lambda x, y: (x * y)), shape)):\n        predictions[site] = value.expand(shape)\n    else:\n        predictions[site] = value.reshape(shape)\nreturn predictions\n"}
{"label_name":"predict","label":4,"method_name":"predictable","method":"\n(alpha, labels, lpygx, mis, lasttc) = out[:5]\n(ns, m) = labels.shape\n(m, nv) = mis.shape\nhys = [entropy(labels[:, j]) for j in range(m)]\nnmis = []\nixys = []\nfor j in range(m):\n    if (hys[j] > 0):\n        ixy = max(0.0, (np.dot(alpha[j], mis[j]) - lasttc[(- 1)][j]))\n        ixys.append(ixy)\n        tcn = ((np.sum(np.sort((alpha[j] * mis[j]))[(- topk):]) - ixy) \/ ((topk - 1) * hys[j]))\n        nmis.append(tcn)\n    else:\n        ixys.append(0)\n        nmis.append(0)\nf = safe_open((prefix + outfile), 'w+')\nprint(list(enumerate(np.argsort((- np.array(nmis))))))\nprint(','.join(list(map(str, list(np.argsort((- np.array(nmis))))))))\nfor (i, top) in enumerate(np.argsort((- np.array(nmis)))):\n    f.write(('Group num: %d, Score: %0.3f\\n' % (top, nmis[top])))\n    inds = np.where((alpha[top] > athresh))[0]\n    inds = inds[np.argsort((- mis[(top, inds)]))]\n    for ind in inds:\n        f.write((wdict[ind] + (', %0.3f\\n' % (mis[(top, ind)] \/ np.log(2)))))\n    if wdict:\n        print(','.join(list(map((lambda q: wdict[q]), inds))))\n        print(','.join(list(map(str, inds))))\n    print(top)\n    print(nmis[top], ixys[top], hys[top], (ixys[top] \/ hys[top]))\n    if graphs:\n        print(inds)\n        if (len(inds) >= 2):\n            plot_rels(data[:, inds[:5]], list(map((lambda q: wdict[q]), inds[:5])), outfile=((('relationships\/' + str(i)) + '_group_num=') + str(top)), latent=out[1][:, top], alpha=tvalue)\nf.close()\nreturn nmis\n"}
{"label_name":"train","label":0,"method_name":"_get_variables_to_train","method":"\n'Returns a list of variables to train.\\n\\n  Returns:\\n    A list of variables to train by the optimizer.\\n  '\nif (FLAGS.trainable_scopes is None):\n    return tf.trainable_variables()\nelse:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\nvariables_to_train = []\nfor scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\nreturn variables_to_train\n"}
{"label_name":"predict","label":4,"method_name":"predict_static","method":"\npaddle.enable_static()\nexe = fluid.Executor(place)\n[inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(MODEL_SAVE_DIR, executor=exe, model_filename=MODEL_FILENAME, params_filename=PARAMS_FILENAME)\npred_res = exe.run(inference_program, feed={feed_target_names[0]: data}, fetch_list=fetch_targets)\nreturn pred_res[0]\n"}
{"label_name":"train","label":0,"method_name":"train_callback","method":"\nif ((it.total_iteration_index % 10) == 0):\n    print('it:{it:>5}, cost:{cost:6.2f}'.format(it=it.total_iteration_index, cost=it.cost))\n"}
{"label_name":"save","label":1,"method_name":"save_feature_map","method":"\n'Save the feature map to disk.\\n\\n    Parameters\\n    ----------\\n    model : alphapy.Model\\n        The model object containing the feature map.\\n    timestamp : str\\n        Date in yyyy-mm-dd format.\\n\\n    Returns\\n    -------\\n    None : None\\n\\n    '\nlogger.info('Saving Feature Map')\ndirectory = model.specs['directory']\nfilename = (('feature_map_' + timestamp) + '.pkl')\nfull_path = SSEP.join([directory, 'model', filename])\nlogger.info('Writing feature map to %s', full_path)\njoblib.dump(model.feature_map, full_path)\n"}
{"label_name":"train","label":0,"method_name":"train_evaluate","method":"\npipeline_manager.train(pipeline_name, validation_size, dev_mode)\npipeline_manager.evaluate(pipeline_name, validation_size, dev_mode)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\noptimizer = optim.Adam(net.parameters(), lr=lr)\nfor epoch in range(max_epochs):\n    for (i, data) in enumerate(loader, 0):\n        (inputs, labels) = data\n        (inputs, labels) = (Variable(inputs.float()), Variable(labels.long()))\n        optimizer.zero_grad()\n        y_pred = net(inputs)\n        loss = criterion(y_pred, labels.long())\n        loss.backward()\n        optimizer.step()\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\nsince = time.time()\nbest_model = model\nbest_acc = 0.0\nfor epoch in range(num_epochs):\n    print('Epoch {}\/{}'.format(epoch, (num_epochs - 1)))\n    print(('-' * 10))\n    for phase in ['train', 'val']:\n        if (phase == 'train'):\n            optimizer = lr_scheduler(optimizer, epoch)\n            model.train(True)\n        else:\n            model.train(False)\n        running_loss = 0.0\n        running_corrects = 0\n        for data in dset_loaders[phase]:\n            (inputs, labels) = data\n            if use_gpu:\n                (inputs, labels) = (Variable(inputs.cuda()), Variable(labels.cuda()))\n            else:\n                (inputs, labels) = (Variable(inputs), Variable(labels))\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            (_, preds) = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            if (phase == 'train'):\n                loss.backward()\n                optimizer.step()\n            running_loss += loss.data[0]\n            running_corrects += torch.sum((preds == labels.data))\n        epoch_loss = (running_loss \/ dset_sizes[phase])\n        epoch_acc = (running_corrects \/ dset_sizes[phase])\n        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n        if ((phase == 'val') and (epoch_acc > best_acc)):\n            best_acc = epoch_acc\n            best_model = copy.deepcopy(model)\n            if (save_prefix != ''):\n                torch.save(model.state_dict(), (((('.\/author_pred_' + save_prefix) + '_') + str(best_acc)) + '.pth'))\n            else:\n                torch.save(model.state_dict(), (('.\/author_pred_net_' + str(best_acc)) + '.pth'))\n    print()\ntime_elapsed = (time.time() - since)\nprint('Training complete in {:.0f}m {:.0f}s'.format((time_elapsed \/\/ 60), (time_elapsed % 60)))\nprint('Best val Acc: {:4f}'.format(best_acc))\nreturn best_model\n"}
{"label_name":"process","label":2,"method_name":"process_term","method":"\nx = re_not_decimal.sub('', x)\nreturn int(x)\n"}
{"label_name":"save","label":1,"method_name":"save_options","method":"\n'Helper to save chosen options'\n(sample_ids, targets, out_dir, user_feature_paths, user_feature_type, fs_subject_dir, train_perc, num_rep_cv, positive_class, subgroups, reduced_dim_size, num_procs, grid_search_level, pred_model_name, dim_red_method) = options_to_save\nuser_options = {'sample_ids': sample_ids, 'targets': targets, 'pred_model_name': pred_model_name, 'dim_red_method': dim_red_method, 'gs_level': grid_search_level, 'reduced_dim_size': reduced_dim_size, 'num_procs': num_procs, 'num_rep_cv': num_rep_cv, 'positive_class': positive_class, 'sub_groups': subgroups, 'train_perc': train_perc, 'fs_subject_dir': fs_subject_dir, 'user_feature_type': user_feature_type, 'user_feature_paths': user_feature_paths, 'out_dir': out_dir}\ntry:\n    options_path = pjoin(out_dir_in, cfg.file_name_options)\n    with open(options_path, 'wb') as opt_file:\n        pickle.dump(user_options, opt_file)\nexcept:\n    raise IOError('Unable to save the options to\\n {}'.format(out_dir_in))\nreturn (user_options, options_path)\n"}
{"label_name":"train","label":0,"method_name":"create_training_instances","method":"\n'Create `TrainingInstance`s from raw text.\\n\\n    The expected input file format is the following:\\n\\n    (1) One sentence per line. These should ideally be actual sentences, not\\n    entire paragraphs or arbitrary spans of text. (Because we use the\\n    sentence boundaries for the \"next sentence prediction\" task).\\n    (2) Blank lines between documents. Document boundaries are needed so\\n    that the \"next sentence prediction\" task doesn\\'t span between documents.\\n\\n    The function expect arguments packed in a tuple as described below.\\n\\n    Parameters\\n    ----------\\n    input_files : list of str\\n        List of paths to input text files.\\n    tokenizer : BaseTokenizer\\n        The BERT tokenizer\\n    max_seq_length : int\\n        The hard limit of maximum sequence length of sentence pairs\\n    dupe_factor : int\\n        Duplication factor.\\n    short_seq_prob : float\\n        The probability of sampling sequences shorter than the max_seq_length.\\n    masked_lm_prob : float\\n        The probability of replacing texts with masks\/random words\/original words.\\n    max_predictions_per_seq : int\\n        The hard limit of the number of predictions for masked words\\n    whole_word_mask : bool\\n        Whether to do masking for whole words\\n    vocab : BERTVocab\\n        The BERTVocab\\n    nworker : int\\n        The number of processes to help processing texts in parallel\\n    worker_pool : multiprocessing.Pool\\n        Must be provided if nworker > 1. The caller is responsible for the destruction of\\n        the worker pool.\\n    output_file : str or None\\n        Path to the output file. If None, the result is not serialized. If provided,\\n        results are  stored in the order of (input_ids, segment_ids, masked_lm_positions,\\n        masked_lm_ids, masked_lm_weights, next_sentence_labels, valid_lengths).\\n\\n    Returns\\n    -------\\n    A tuple of np.ndarray : input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights\\n                            next_sentence_labels, segment_ids, valid_lengths\\n    '\n(input_files, tokenizer, max_seq_length, short_seq_prob, masked_lm_prob, max_predictions_per_seq, whole_word_mask, vocab, dupe_factor, nworker, worker_pool, output_file, random_next_sentence) = x\ntime_start = time.time()\nif (nworker > 1):\n    assert (worker_pool is not None)\nall_documents = [[]]\nfor input_file in input_files:\n    logging.debug('*** Tokenizing file %s***', input_file)\n    with io.open(input_file, 'r', encoding='utf-8') as reader:\n        lines = reader.readlines()\n        num_lines = len(lines)\n        num_lines_per_worker = (((num_lines + nworker) - 1) \/\/ nworker)\n        process_args = []\n        for worker_idx in range(nworker):\n            start = (worker_idx * num_lines_per_worker)\n            end = min(((worker_idx + 1) * num_lines_per_worker), num_lines)\n            process_args.append((lines[start:end], tokenizer))\n        if worker_pool:\n            tokenized_results = worker_pool.map(tokenize_lines_fn, process_args)\n        else:\n            tokenized_results = [tokenize_lines_fn(process_args[0])]\n        for tokenized_result in tokenized_results:\n            for line in tokenized_result:\n                if (not line):\n                    if all_documents[(- 1)]:\n                        all_documents.append([])\n                else:\n                    all_documents[(- 1)].append(line)\nall_documents = [x for x in all_documents if x]\nrandom.shuffle(all_documents)\ninstances = []\nif worker_pool:\n    process_args = []\n    for document_index in range(len(all_documents)):\n        process_args.append((all_documents, document_index, max_seq_length, short_seq_prob, masked_lm_prob, max_predictions_per_seq, whole_word_mask, vocab, tokenizer, random_next_sentence))\n    for _ in range(dupe_factor):\n        instances_results = worker_pool.map(create_instances_from_document, process_args)\n        for instances_result in instances_results:\n            instances.extend(instances_result)\n    random.shuffle(instances)\n    npz_instances = worker_pool.apply(convert_to_npz, (instances, max_seq_length))\nelse:\n    for _ in range(dupe_factor):\n        for document_index in range(len(all_documents)):\n            instances.extend(create_instances_from_document((all_documents, document_index, max_seq_length, short_seq_prob, masked_lm_prob, max_predictions_per_seq, whole_word_mask, vocab, tokenizer, random_next_sentence)))\n    random.shuffle(instances)\n    npz_instances = convert_to_npz(instances, max_seq_length)\n(input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights, next_sentence_labels, segment_ids, valid_lengths) = npz_instances\nif output_file:\n    features = (input_ids, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights, next_sentence_labels, valid_lengths)\n    logging.debug('*** Writing to output file %s ***', output_file)\n    write_to_files_np(features, tokenizer, max_seq_length, max_predictions_per_seq, [output_file])\n    features = None\nelse:\n    features = (input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights, next_sentence_labels, segment_ids, valid_lengths)\ntime_end = time.time()\nlogging.debug('Process %d files took %.1f s', len(input_files), (time_end - time_start))\nreturn features\n"}
{"label_name":"process","label":2,"method_name":"preprocess_blizzard","method":"\nin_dir = os.path.join(args.base_dir, 'Blizzard2012')\nout_dir = os.path.join(args.base_dir, args.output)\nos.makedirs(out_dir, exist_ok=True)\nmetadata = blizzard.build_from_path(in_dir, out_dir, args.num_workers, tqdm=tqdm)\nwrite_metadata(metadata, out_dir)\n"}
{"label_name":"train","label":0,"method_name":"train_mnist_mxnet","method":"\nbatch_size = config['batch_size']\ntrain_iter = mx.io.NDArrayIter(mnist['train_data'], mnist['train_label'], batch_size, shuffle=True)\nval_iter = mx.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size)\ndata = mx.sym.var('data')\ndata = mx.sym.flatten(data=data)\nfc1 = mx.sym.FullyConnected(data=data, num_hidden=config['layer_1_size'])\nact1 = mx.sym.Activation(data=fc1, act_type='relu')\nfc2 = mx.sym.FullyConnected(data=act1, num_hidden=config['layer_2_size'])\nact2 = mx.sym.Activation(data=fc2, act_type='relu')\nfc3 = mx.sym.FullyConnected(data=act2, num_hidden=10)\nmlp = mx.sym.SoftmaxOutput(data=fc3, name='softmax')\nmlp_model = mx.mod.Module(symbol=mlp, context=mx.cpu())\nmlp_model.fit(train_iter, eval_data=val_iter, optimizer='sgd', optimizer_params={'learning_rate': config['lr']}, eval_metric='acc', batch_end_callback=mx.callback.Speedometer(batch_size, 100), eval_end_callback=TuneReportCallback({'mean_accuracy': 'accuracy'}), epoch_end_callback=TuneCheckpointCallback(filename='mxnet_cp', frequency=3), num_epoch=num_epochs)\n"}
{"label_name":"process","label":2,"method_name":"preprocess_input","method":"\n(new_h, new_w, _) = image.shape\nif ((float(net_w) \/ new_w) < (float(net_h) \/ new_h)):\n    new_h = ((new_h * net_w) \/\/ new_w)\n    new_w = net_w\nelse:\n    new_w = ((new_w * net_h) \/\/ new_h)\n    new_h = net_h\nresized = cv2.resize((cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \/ 255.0), (new_w, new_h))\nnew_image = (np.ones((net_h, net_w, 3)) * 0.5)\nnew_image[((net_h - new_h) \/\/ 2):((net_h + new_h) \/\/ 2), ((net_w - new_w) \/\/ 2):((net_w + new_w) \/\/ 2), :] = resized\nnew_image = np.expand_dims(new_image, 0)\nreturn new_image\n"}
{"label_name":"save","label":1,"method_name":"save_corpus","method":"\n' Save gensim corpus and dictionary.\\n\\n    :param dictionary: dictionary to save\\n    :param corpus: corpus to save\\n    :param prefix: prefix of the files to save\\n    :return: None\\n    :type dictionary: gensim.corpora.Dictionary\\n    :type corpus: list\\n    :type prefix: str\\n    '\ndictionary.save((prefix + '_dictionary.dict'))\ngensim.corpora.MmCorpus.serialize((prefix + '_corpus.mm'), corpus)\n"}
{"label_name":"forward","label":3,"method_name":"row_conv_forward","method":"\nout = np.zeros_like(x)\nnum_sequences = len(lod[0])\nseq_info = [0]\nfor seq_len in lod[0]:\n    seq_info.append((seq_info[(- 1)] + seq_len))\ncontext_length = wt.shape[0]\nfor i in range(num_sequences):\n    start = seq_info[i]\n    end = seq_info[(i + 1)]\n    curinput = x[start:end, :]\n    curoutput = out[start:end, :]\n    cur_timesteps = (end - start)\n    for j in range(cur_timesteps):\n        for k in range(context_length):\n            if ((j + k) >= cur_timesteps):\n                continue\n            curoutput[j, :] += (curinput[(j + k), :] * wt[k, :])\nreturn out\n"}
{"label_name":"process","label":2,"method_name":"build_preprocessors","method":"\n' Build the default set of preprocessors used by Markdown. '\npreprocessors = odict.OrderedDict()\npreprocessors['normalize_whitespace'] = NormalizeWhitespace(md_instance)\nif (md_instance.safeMode != 'escape'):\n    preprocessors['html_block'] = HtmlBlockPreprocessor(md_instance)\npreprocessors['reference'] = ReferencePreprocessor(md_instance)\nreturn preprocessors\n"}
{"label_name":"save","label":1,"method_name":"save_image","method":"\nfeature = cv2.resize(feature, output_size)\nlabel = cv2.resize(label, output_size)\ncv2.imwrite((('feature-images\/' + str(index)) + '.jpg'), feature)\ncv2.imwrite((('label-images\/' + str(index)) + '.jpg'), label)\n"}
{"label_name":"process","label":2,"method_name":"process_buffer_texas","method":"\np = parse(line)\nreturn (p.visits[0].key.encode('utf-8'), p.SerializeToString())\n"}
{"label_name":"process","label":2,"method_name":"process_log_line","method":"\nif (not isinstance(log_line, str)):\n    log_line = log_line.decode('utf-8')\nreturn V1Log.process_log_line(value=log_line.strip(), node=None, pod=None, container=None)\n"}
{"label_name":"save","label":1,"method_name":"save_csv","method":"\ndataset_name = (date.today().strftime('%Y-%m-%d') + '-purchase-suppliers.xz')\ndataset_path = os.path.join('data', dataset_name)\ndf.to_csv(dataset_path, compression='xz', encoding='utf-8', index=False)\n"}
{"label_name":"train","label":0,"method_name":"train_split","method":"\n\ndef func(dataset, y):\n    ds_train = type(dataset)(dataset.X[:2], dataset.y[:2])\n    ds_valid = type(dataset)(dataset.X[2:], dataset.y[2:])\n    return (ds_train, ds_valid)\nreturn func\n"}
{"label_name":"predict","label":4,"method_name":"generate_predict","method":"\n'\\n        \u8f93\u5165\u8bad\u7ec3\u96c6\uff0c\u6700\u8fd1\u90bb\u7684\u70b9\u4e2a\u6570\u4ee5\u8fd4\u56de\u5224\u65ad\u51fd\u6570\\n    '\nkd_tree = KDTree.generate_tree(trainning_set)\n\ndef distance_measure(point0, point1):\n    '\\n            # \u8ddd\u79bb\u5ea6\u91cf\u51fd\u6570\uff0cp=2 \u4e3a\u6b27\u6c0f\u8ddd\u79bb\uff0c p=1\u4e3a\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\\n            # point: (x1, x2, x3, ...)\\n            # L = (\u03a3|point0_j - point1_j|**p) ** 1\/p\\n            # \u5982\u679cp\u662f\u221e L= max|point0_j - point1_j|\\n            \u8fd4\u56depoint0\uff0c point1\u4e4b\u95f4\u7684\u6b27\u5f0f\u8ddd\u79bb\\n        '\n    point0 = np.array(point0)\n    point1 = np.array(point1)\n    return np.sqrt(np.sum(np.square((point0 - point1))))\n\ndef predict(feature):\n    '\\n            \u5224\u65ad\u51fd\u6570\uff0c\u8f93\u5165\u7279\u5f81\u503c\u8f93\u51fa\u5224\u65ad\u7ed3\u679c\\n        '\n    if (not (neighbor_k % 2)):\n        warnings.warn('K\u503c\u4e3a\u5076\u6570\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u51b3\u7b56\u65f6\u6b63\u53cd\u6837\u672c\u6570\u76f8\u7b49\u7684\u60c5\u51b5')\n    if ((neighbor_k > len(trainning_set)) or (neighbor_k < 0)):\n        raise RuntimeError('K\u503c\u4e0d\u5408\u6cd5')\n    if (neighbor_k > (len(trainning_set) ** 0.5)):\n        warnings.warn('K\u503c\u8d85\u8fc7\u4e86\u6837\u672c\u603b\u6570\u7684\u5f00\u65b9')\n    dots = trainning_set[:neighbor_k]\n    near_dots_heap = MaxHeap(map((lambda item: (distance_measure(item[0], feature), item)), dots))\n\n    def search(tree, near_dots_heap, stack=None):\n        '\\n                \u6839\u636e\u8f93\u5165\u7684KD\u6811\u4ee5\u53ca\u5176\u5212\u5206\u7b56\u7565\uff0c\u68c0\u7d22\u6837\u672c\u70b9\u6240\u5728\u533a\u57df\uff08\u6216\u8005\u6700\u8fd1\u533a\u57df\uff09\u3002\u5e76\u5c06\u9014\u7ecf\u8fc7\u7684\u70b9\u5165\u6700\u5927\u503c\u5806\u5e76\u5c06\u5176\u5165\u6808\\n                tree\uff1aKD_tree\\n            '\n        if tree.is_leaf:\n            '\\n                    \u5982\u679c\u67e5\u627e\u5230\u4e86\u6700\u63a5\u8fd1\u6837\u672c\u70b9\u7684\u53f6\u8282\u70b9\uff0c\u5219\u5f00\u59cb\u56de\u6eaf\\n                '\n            return (near_dots_heap, stack)\n        if (not stack):\n            stack = []\n        distance = distance_measure(tree.dot[0], feature)\n        if (distance < near_dots_heap.max_key):\n            near_dots_heap.pushpop((distance, tree.dot))\n        if (feature[tree.axis] < tree.edge):\n            stack.append(('left_tree', tree))\n            return search(tree.left_tree, near_dots_heap, stack)\n        stack.append(('right_tree', tree))\n        return search(tree.right_tree, near_dots_heap, stack)\n    (near_dots_heap, stack) = search(kd_tree, near_dots_heap)\n    '\\n            \u8fd4\u56de\u904d\u5386\u6808\u4ee5\u65b9\u4fbf\u540e\u9762\u7684\u56de\u6eaf\\n        '\n\n    def review(stack, near_dots_heap):\n        '\\n                \u56de\u6eaf\uff0c\u8f93\u5165\u56de\u6eaf\u6808\uff0c\u6700\u5927\u503c\u5806\\n                \u8fd4\u56de\u6700\u5927\u503c\u5806\\n            '\n        if (not stack):\n            return near_dots_heap\n        radius = near_dots_heap.max_key\n        (label, tree) = stack.pop()\n        if (label == 'left_tree'):\n            subtree = tree.right_tree\n        elif (label == 'right_tree'):\n            subtree = tree.left_tree\n        if (((feature[tree.axis] - radius) < tree.edge) and ((feature[tree.axis] + radius) > tree.edge)):\n            near_dots_heap = search(subtree, near_dots_heap)[0]\n        return review(stack, near_dots_heap)\n    near_dots_heap = review(stack, near_dots_heap)\n    result = (1 if (sum(map((lambda item: item[1][1]), near_dots_heap.to_list())) > 0) else (- 1))\n    if more_details:\n        return (result, near_dots_heap.to_list(), kd_tree)\n    return result\nreturn predict\n"}
{"label_name":"train","label":0,"method_name":"trained_async","method":"\n\nasync def _train(*args: Any, output_path: Optional[Text]=None, **kwargs: Any) -> Optional[Text]:\n    if (output_path is None):\n        output_path = str(tmpdir_factory.mktemp('models'))\n    result = (await train_async(*args, output=output_path, **kwargs))\n    return result.model\nreturn _train\n"}
{"label_name":"train","label":0,"method_name":"create_training_data","method":"\ny = [d[0] for d in docs]\ncorpus = [d[1] for d in docs]\nvectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None, stop_words='english')\nX = vectorizer.fit_transform(corpus)\nprint(('Vectorizer shape: ' + str(X.shape)))\nreturn (vectorizer, X, y)\n"}
{"label_name":"train","label":0,"method_name":"train_dygraph","method":"\nwith fluid.dygraph.guard(place):\n    if (SEED is not None):\n        paddle.seed(SEED)\n        paddle.framework.random._manual_program_seed(SEED)\n    train_loader = fluid.io.DataLoader.from_generator(capacity=10)\n    train_loader.set_batch_generator(batch_generator, places=place)\n    transformer = Transformer(args.src_vocab_size, args.trg_vocab_size, (args.max_length + 1), args.n_layer, args.n_head, args.d_key, args.d_value, args.d_model, args.d_inner_hid, args.prepostprocess_dropout, args.attention_dropout, args.relu_dropout, args.preprocess_cmd, args.postprocess_cmd, args.weight_sharing, args.bos_idx, args.eos_idx)\n    criterion = CrossEntropyCriterion(args.label_smooth_eps)\n    learning_rate = fluid.layers.learning_rate_scheduler.noam_decay(args.d_model, args.warmup_steps, args.learning_rate)\n    optimizer = fluid.optimizer.Adam(learning_rate=learning_rate, beta1=args.beta1, beta2=args.beta2, epsilon=float(args.eps), parameter_list=transformer.parameters())\n    loss_normalizer = (- (((1.0 - args.label_smooth_eps) * np.log((1.0 - args.label_smooth_eps))) + (args.label_smooth_eps * np.log(((args.label_smooth_eps \/ (args.trg_vocab_size - 1)) + 1e-20)))))\n    ce_time = []\n    ce_ppl = []\n    avg_loss = []\n    step_idx = 0\n    for pass_id in range(args.epoch):\n        pass_start_time = time.time()\n        batch_id = 0\n        for input_data in train_loader():\n            (src_word, src_pos, src_slf_attn_bias, trg_word, trg_pos, trg_slf_attn_bias, trg_src_attn_bias, lbl_word, lbl_weight) = input_data\n            logits = transformer(src_word, src_pos, src_slf_attn_bias, trg_word, trg_pos, trg_slf_attn_bias, trg_src_attn_bias)\n            (sum_cost, avg_cost, token_num) = criterion(logits, lbl_word, lbl_weight)\n            avg_cost.backward()\n            optimizer.minimize(avg_cost)\n            transformer.clear_gradients()\n            if ((step_idx % args.print_step) == 0):\n                total_avg_cost = (avg_cost.numpy() * trainer_count)\n                avg_loss.append(total_avg_cost[0])\n                if (step_idx == 0):\n                    logging.info(('step_idx: %d, epoch: %d, batch: %d, avg loss: %f, normalized loss: %f, ppl: %f' % (step_idx, pass_id, batch_id, total_avg_cost, (total_avg_cost - loss_normalizer), np.exp([min(total_avg_cost, 100)]))))\n                    avg_batch_time = time.time()\n                else:\n                    logging.info(('step_idx: %d, epoch: %d, batch: %d, avg loss: %f, normalized loss: %f, ppl: %f, speed: %.2f steps\/s' % (step_idx, pass_id, batch_id, total_avg_cost, (total_avg_cost - loss_normalizer), np.exp([min(total_avg_cost, 100)]), (args.print_step \/ (time.time() - avg_batch_time)))))\n                    ce_ppl.append(np.exp([min(total_avg_cost, 100)]))\n                    avg_batch_time = time.time()\n            batch_id += 1\n            step_idx += 1\n            if (step_idx == STEP_NUM):\n                if args.save_dygraph_model_path:\n                    model_dir = os.path.join(args.save_dygraph_model_path)\n                    if (not os.path.exists(model_dir)):\n                        os.makedirs(model_dir)\n                    fluid.save_dygraph(transformer.state_dict(), os.path.join(model_dir, 'transformer'))\n                    fluid.save_dygraph(optimizer.state_dict(), os.path.join(model_dir, 'transformer'))\n                break\n        time_consumed = (time.time() - pass_start_time)\n        ce_time.append(time_consumed)\n    return np.array(avg_loss)\n"}
{"label_name":"predict","label":4,"method_name":"generate_prediction","method":"\n'\\n    Generate out of fold prediction for the training dataset, used for level 2 model training.\\n\\n    Predictions are saved in individual files in the output\/prediction_train_frames directory\\n    and need to be combined with save_combined_train_results()\\n\\n    :param model_name: name of level 1 model\\n    :param weights: path to model weights\\n    :param fold: fold to generate predictions for\\n    :return:\\n    '\nmodel = MODELS[model_name].factory(lock_base_model=True)\nmodel.load_weights(str(weights), by_name=False)\noutput_dir = (((cnnensemble_path \/ 'output') \/ 'prediction_train_frames') \/ f'{model_name}_{fold}')\noutput_dir.mkdir(parents=True, exist_ok=True)\ndataset = SingleFrameCNNDataset(preprocess_input_func=MODELS[model_name].preprocess_input, batch_size=1, validation_batch_size=1, fold=fold)\nconverted_files = set()\nprocessed_files = 0\nfor video_id in dataset.test_clips:\n    res_fn = (output_dir.resolve() \/ f'{video_id}.csv')\n    if res_fn.exists():\n        processed_files += 1\n        converted_files.add(video_id)\ntest_clips = sorted(list((set(dataset.test_clips) - converted_files)))\n\ndef load_file(video_id):\n    X = dataset.frames_from_saved_images(video_id=video_id)\n    y = dataset.training_set_labels_ds.loc[[video_id]].as_matrix(columns=CLASSES)\n    return (video_id, X, y)\nstart_time = time.time()\npool = ThreadPool(8)\nprev_res = None\nfor batch in tqdm(utils.chunks(test_clips, 8, add_empty=True), total=(len(test_clips) \/\/ 8), desc='Predict OOF samples'):\n    if (prev_res is not None):\n        results = prev_res.get()\n    else:\n        results = []\n    prev_res = pool.map_async(load_file, batch)\n    for (video_id, X, y) in results:\n        processed_files += 1\n        res_fn = (output_dir.resolve() \/ f'{video_id}.csv')\n        have_data_time = time.time()\n        prediction = model.predict(X, batch_size=1)\n        ds = pd.DataFrame(index=([(- 1)] + PREDICT_FRAMES), data=np.row_stack([y, prediction]), columns=CLASSES)\n        ds.to_csv(res_fn, index_label='frame', float_format='%.5f')\n        have_prediction_time = time.time()\n        prepare_ms = int(((have_data_time - start_time) * 1000))\n        predict_ms = int(((have_prediction_time - have_data_time) * 1000))\n        start_time = time.time()\n"}
{"label_name":"process","label":2,"method_name":"unprocess","method":"\nreturn (image + mean_pixel)\n"}
{"label_name":"train","label":0,"method_name":"prepare_pretrain_bucket_sampler","method":"\n'Create data sampler based on the dataset'\nif isinstance(dataset, NumpyDataset):\n    lengths = dataset.get_field('valid_lengths')\nelse:\n    lengths = dataset.transform((lambda input_ids, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights, next_sentence_labels, valid_lengths: valid_lengths), lazy=False)\nsampler = FixedBucketSampler(lengths, batch_size=batch_size, num_buckets=num_buckets, ratio=0, shuffle=shuffle)\nlogging.debug('Sampler created for a new dataset:\\n%s', sampler)\nreturn sampler\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nif (NETWORK.use_landmarks or NETWORK.use_hog_and_landmarks or NETWORK.use_hog_sliding_window_and_landmarks):\n    face_rects = [dlib.rectangle(left=0, top=0, right=NETWORK.input_size, bottom=NETWORK.input_size)]\n    face_landmarks = np.array([get_landmarks(image, face_rects, shape_predictor)])\n    features = face_landmarks\n    if NETWORK.use_hog_sliding_window_and_landmarks:\n        hog_features = sliding_hog_windows(image)\n        hog_features = np.asarray(hog_features)\n        face_landmarks = face_landmarks.flatten()\n        features = np.concatenate((face_landmarks, hog_features))\n    else:\n        (hog_features, _) = hog(image, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualise=True)\n        hog_features = np.asarray(hog_features)\n        face_landmarks = face_landmarks.flatten()\n        features = np.concatenate((face_landmarks, hog_features))\n    tensor_image = image.reshape([(- 1), NETWORK.input_size, NETWORK.input_size, 1])\n    predicted_label = model.predict([tensor_image, features.reshape((1, (- 1)))])\n    return get_emotion(predicted_label[0])\nelse:\n    tensor_image = image.reshape([(- 1), NETWORK.input_size, NETWORK.input_size, 1])\n    predicted_label = model.predict(tensor_image)\n    return get_emotion(predicted_label[0])\nreturn None\n"}
{"label_name":"process","label":2,"method_name":"pre_processing","method":"\nprocessed_observe = np.uint8((resize(rgb2gray(observe), (84, 84), mode='constant') * 255))\nreturn processed_observe\n"}
{"label_name":"predict","label":4,"method_name":"predictOneVsAll","method":"\nm = X.shape[0]\nX = np.vstack((np.ones(m), X.T)).T\nreturn np.argmax(sigmoid(np.dot(all_theta, X.T)), axis=0)\n"}
{"label_name":"process","label":2,"method_name":"preprocess","method":"\nif (not PREPROCESS):\n    return x\npelvis_r = x[0]\npelvis_x = x[1]\npelvis_y = x[2]\npelvis_v_r = x[3]\npelvis_v_x = x[4]\npelvis_v_y = x[5]\nresult = [pelvis_r, pelvis_x, pelvis_y, pelvis_v_r, pelvis_v_x, pelvis_v_y, (x[6] - pelvis_r), (x[7] - pelvis_r), (x[8] - pelvis_r), (x[9] - pelvis_r), (x[10] - pelvis_r), (x[11] - pelvis_r), (x[12] - pelvis_v_r), (x[13] - pelvis_v_r), (x[14] - pelvis_v_r), (x[15] - pelvis_v_r), (x[16] - pelvis_v_r), (x[17] - pelvis_v_r), (x[18] - pelvis_x), (x[19] - pelvis_y), (x[20] - pelvis_v_x), (x[21] - pelvis_v_y), (x[22] - pelvis_x), (x[23] - pelvis_y), (x[26] - pelvis_x), (x[27] - pelvis_y), (x[28] - pelvis_x), (x[29] - pelvis_y), (x[30] - pelvis_x), (x[31] - pelvis_y), (x[32] - pelvis_x), (x[33] - pelvis_y), (x[34] - pelvis_x), (x[35] - pelvis_y), x[36], x[37], x[38], x[39], x[40]]\nresult.extend(([0] * 3))\nif (preprocess.old is None):\n    preprocess.old = list(result)\n    zeros = (OBSERVATION_DIM - len(preprocess.old))\n    preprocess.old.extend(([0] * zeros))\ndx = (pelvis_x - preprocess.old[1])\nset_obstacles(result, dx, step)\nadd_velocities(result, preprocess.old, [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])\nadd_accelerations(result, preprocess.old)\nadd_ground_touch(result, [27, 29, 31, 33])\nadd_clear_ahead(result, pelvis_x)\npreprocess.old = list(result)\nresult[1] \/= 100.0\nresult[2] = ((result[2] - 0.9) \/ 0.1)\nresult[3] = double_sqrt((result[3] - 0.3))\nresult[4] = double_sqrt((result[4] - 1.0))\nresult[5] = double_sqrt((result[5] - 0.3))\nfor i in range(6, 12):\n    result[i] = double_sqrt(result[i])\nfor i in range(12, 18):\n    result[i] = double_sqrt((result[i] \/ 5.0))\nresult[18] = ((result[18] + 0.14) \/ 0.05)\nresult[19] = ((result[19] - 0.07) \/ 0.03)\nfor i in range(20, 22):\n    result[i] = double_sqrt(result[i])\nresult[22] = ((result[22] + 0.15) \/ 0.25)\nresult[23] = ((result[23] - 0.61) \/ 0.02)\nresult[24] = ((result[24] + 0.1) \/ 0.02)\nresult[25] = ((result[25] - 0.08) \/ 0.02)\nfor i in [26, 28, 30, 32]:\n    result[i] = (result[i] + 0.1)\nfor i in [27, 29, 31, 33]:\n    result[i] = ((result[i] + 0.9) \/ 0.3)\nfor i in range(34, 36):\n    result[i] = ((result[i] - 1.0) \/ 0.2)\nfor i in range(42, 46):\n    result[i] = double_sqrt(result[i])\nfor i in [46, 48, 50, 52]:\n    result[i] = (double_sqrt(result[i]) \/ 2.5)\nfor i in [47, 49, 51, 53]:\n    result[i] = double_sqrt(result[i])\nif verbose:\n    auto_normalize(result, range(OBSERVATION_DIM), step)\n    print('---')\n    print(step)\n    print('')\n    print(('x %s' % x))\n    print('')\n    print(('result %s' % result))\n    print('')\n    print(('min %s' % preprocess.min))\n    print('')\n    print(('max %s' % preprocess.max))\n    print('')\n    print(('mean %s' % preprocess.mean))\n    print('')\n    if (step > 2):\n        print(('std %s' % map((lambda m: (m \/ (step - 1))), preprocess.m2)))\n    print('')\nassert (len(result) == OBSERVATION_DIM)\nreturn result\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nfor i in range(MAX_EPISODES):\n    s = env.reset()\n    ep_r = 0.0\n    for j in range(MAX_EP_STEPS):\n        env.render()\n        a = rl.choose_action(s)\n        (s_, r, done) = env.step(a)\n        rl.store_transition(s, a, r, s_)\n        ep_r += r\n        if rl.memory_full:\n            rl.learn()\n        s = s_\n        if (done or (j == (MAX_EP_STEPS - 1))):\n            print(('Ep: %i | %s | ep_r: %.1f | step: %i' % (i, ('---' if (not done) else 'done'), ep_r, j)))\n            break\nrl.save()\n"}
{"label_name":"predict","label":4,"method_name":"prediction","method":"\nposted_data = request.data.decode('utf-8')\ntry:\n    processing_result = DataProcessor(posted_data).call()\nexcept NoFlightFoundError:\n    return (jsonify({'error': 'no flight data'}), 422)\nreturn jsonify(processing_result)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nerrorCount = 0\nfor j in range(testCount):\n    try:\n        (pP, pN, pNeu, smsType) = classify(PosWords, NegWords, NeutralWords, prior_Pos, prior_Neg, prior_Neutral, test_word_array[j])\n        if (smsType != test_word_arrayLabel[j]):\n            errorCount += 1\n    except Exception as e:\n        traceback.print_exc(e)\nprint((errorCount \/ testCount))\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n(n_clusters, n) = h.shape\ne = torch.eye(n, dtype=torch.float32).to(device)\n(labels, success) = (torch.zeros((n,), dtype=torch.int64).to(device), True)\nfor _ in range(max_iter):\n    h_e = (h.unsqueeze(1) - e.unsqueeze(0))\n    l = torch.einsum('kni,ij,knj->kn', [h_e, K, h_e]).argmin(dim=0)\n    if torch.all((labels == l)):\n        break\n    labels = l\n    U = torch.zeros((n, n_clusters), dtype=torch.float32).to(device)\n    U[(range(n), labels)] = 1\n    nn = U.sum(dim=0, keepdim=True)\n    if torch.any((nn == 0)):\n        success = False\n        break\n    h = (U \/ nn).transpose(0, 1)\ninertia = _inertia(h, e, K, labels)\nmodularity = (_modularity(A, labels) if (A is not None) else None)\nreturn (labels, inertia, modularity, success)\n"}
{"label_name":"save","label":1,"method_name":"_python_save_name","method":"\n(first, rest) = (name[0], name[1:])\nname = (re.sub('[^a-zA-Z_]', '_', first) + re.sub('[^a-zA-Z_0-9]', '_', rest))\nif (name in used):\n    nr = 1\n    while ((name + ('_%d' % nr)) in used):\n        nr += 1\n    name = (name + ('_%d' % nr))\nreturn name\n"}
{"label_name":"save","label":1,"method_name":"save_progress","method":"\n\"\\n    This function saves our progress either in an existing file structure or writes a new file.\\n    :param save_dir: STRING: The directory where to save the progress.\\n    :param model: DICTIONARY: The model that we wish to save.\\n    :param Delta_accountant: LIST: The list of deltas that we allocared so far.\\n    :param Accuracy_accountant: LIST: The list of accuracies that we allocated so far.\\n    :param PrivacyAgent: CLASS INSTANCE: The privacy agent that we used (specifically the m's that we used for Federated training.)\\n    :param FLAGS: CLASS INSTANCE: The FLAGS passed to the learning procedure.\\n    :return: nothing\\n    \"\nfilehandler = open((save_dir + '\/model.pkl'), 'wb')\npickle.dump(model, filehandler)\nfilehandler.close()\nif (FLAGS.relearn == False):\n    with open((save_dir + '\/specs.csv'), 'wb') as csvfile:\n        writer = csv.writer(csvfile, delimiter=',')\n        if (FLAGS.priv_agent == True):\n            writer.writerow(([0] + [PrivacyAgent.get_m(r) for r in range((len(Delta_accountant) - 1))]))\n        if (FLAGS.priv_agent == False):\n            writer.writerow(([0] + ([FLAGS.m] * (len(Delta_accountant) - 1))))\n        writer.writerow(Delta_accountant)\n        writer.writerow(Accuracy_accountant)\nif (FLAGS.relearn == True):\n    if ((len(Accuracy_accountant) > 1) or ((len(Accuracy_accountant) == 1) and (FLAGS.loaded is True))):\n        with open((save_dir + '\/specs.csv'), 'r+w') as csvfile:\n            csvReader = csv.reader(csvfile, delimiter=',')\n            lines = []\n            for row in csvReader:\n                lines.append([float(i) for i in row])\n            lines = lines[:(- 1)]\n        with open((save_dir + '\/specs.csv'), 'wb') as csvfile:\n            writer = csv.writer(csvfile, delimiter=',')\n            for line in lines:\n                writer.writerow(line)\n    with open((save_dir + '\/specs.csv'), 'a') as csvfile:\n        writer = csv.writer(csvfile, delimiter=',')\n        writer.writerow(Accuracy_accountant)\n"}
{"label_name":"save","label":1,"method_name":"save_fig","method":"\nimport matplotlib.pyplot as plt\nif tight:\n    plt.tight_layout()\nplt.savefig('{}.pdf'.format(filename))\nplt.savefig('{}.pgf'.format(filename))\nplt.savefig('{}.png'.format(filename), dpi=dpi)\n"}
{"label_name":"predict","label":4,"method_name":"predict_ratings_nn","method":"\n'Predict ratings using nearest neighbor prediction.\\n\\n    Arguments:\\n        Y(ndarray): A matrix of ratings where each row represents a\\n                    user and each column represents a movie.\\n                    -1 represents and unknown rating.\\n        S(ndarray): A num_users by num_users matrix where\\n                    entry (a,b) contains the similarity\\n                    between users a and b.\\n        KNN(dict): A dictionary mapping user\/movie tuples (a,i)\\n                   to a numpy array of most similar user indices.\\n\\n    Returns:\\n        A matrix with the predicted ratings for all users\/movies.\\n    '\n(num_users, num_movies) = Y.shape\nX = np.zeros(Y.shape)\nuser_mean_movie_ratings = np.zeros(num_users)\nfor a in range(num_users):\n    user_mean_movie_ratings[a] = np.mean(Y[a][np.where((Y[a] != (- 1)))])\nfor a in trange(num_users):\n    for i in range(num_movies):\n        similar_users_deviation = np.dot(S[(a, KNN[(a, i)])], (Y[(KNN[(a, i)], i)] - user_mean_movie_ratings[KNN[(a, i)]]))\n        total_similarity = np.sum(np.abs(S[(a, KNN[(a, i)])]))\n        if (total_similarity == 0):\n            X[(a, i)] = user_mean_movie_ratings[a]\n        else:\n            X[(a, i)] = (user_mean_movie_ratings[a] + (similar_users_deviation \/ total_similarity))\nreturn X\n"}
{"label_name":"process","label":2,"method_name":"_process_stats_update","method":"\n(stats_type, market, msg) = (stats_obj[0], stats_obj[1], stats_obj[2])\nif ((stats_type == 'order_book_incr') or (stats_type == 'order_book_bulk')):\n    with open((MARKET_STATS_FILE % (market.exchange_name, market.product_id)), 'w') as fd:\n        st = str(market)\n        fd.write(st)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nprint('Starting training...')\nparams = {}\nparams.update(paramdict)\nprint('Passed params: ', params)\nprint(platform.uname())\nsuffix = ((('btchFixmod_' + ''.join([((str(x) + '_') if ((pair[0] is not 'nbsteps') and (pair[0] is not 'rngseed') and (pair[0] is not 'save_every') and (pair[0] is not 'test_every') and (pair[0] is not 'pe')) else '') for pair in sorted(zip(params.keys(), params.values()), key=(lambda x: x[0])) for x in pair])[:(- 1)]) + '_rngseed_') + str(params['rngseed']))\nprint('Setting random seeds')\nnp.random.seed(params['rngseed'])\nrandom.seed(params['rngseed'])\ntorch.manual_seed(params['rngseed'])\nprint('Initializing network')\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(('cuda' if use_cuda else 'cpu'))\nnet = Network(TOTALNBINPUTS, params['hs']).to(device)\nprint('Shape of all optimized parameters:', [x.size() for x in net.parameters()])\nallsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]\nprint('Size (numel) of all optimized elements:', allsizes)\nprint('Total size (numel) of all optimized elements:', sum(allsizes))\nprint('Initializing optimizer')\noptimizer = torch.optim.Adam(net.parameters(), lr=(1.0 * params['lr']), eps=0.0001, weight_decay=params['l2'])\nBATCHSIZE = params['bs']\nLABSIZE = params['msize']\nlab = np.ones((LABSIZE, LABSIZE))\nCTR = (LABSIZE \/\/ 2)\nlab[1:(LABSIZE - 1), 1:(LABSIZE - 1)].fill(0)\nfor row in range(1, (LABSIZE - 1)):\n    for col in range(1, (LABSIZE - 1)):\n        if (((row % 2) == 0) and ((col % 2) == 0)):\n            lab[(row, col)] = 1\nlab[(CTR, CTR)] = 0\nall_losses = []\nall_grad_norms = []\nall_losses_objective = []\nall_total_rewards = []\nall_losses_v = []\nlossbetweensaves = 0\nnowtime = time.time()\nmeanrewards = np.zeros((LABSIZE, LABSIZE))\nmeanrewardstmp = np.zeros((LABSIZE, LABSIZE, params['eplen']))\npos = 0\nhidden = net.initialZeroState(BATCHSIZE)\nhebb = net.initialZeroHebb(BATCHSIZE)\nprint('Starting episodes!')\nfor numiter in range(params['nbiter']):\n    PRINTTRACE = 0\n    if (((numiter + 1) % params['pe']) == 0):\n        PRINTTRACE = 1\n    posr = {}\n    posc = {}\n    rposr = {}\n    rposc = {}\n    for nb in range(BATCHSIZE):\n        myrposr = 0\n        myrposc = 0\n        while ((lab[(myrposr, myrposc)] == 1) or ((myrposr == CTR) and (myrposc == CTR))):\n            myrposr = np.random.randint(1, (LABSIZE - 1))\n            myrposc = np.random.randint(1, (LABSIZE - 1))\n        rposr[nb] = myrposr\n        rposc[nb] = myrposc\n        posc[nb] = CTR\n        posr[nb] = CTR\n    optimizer.zero_grad()\n    loss = 0\n    lossv = 0\n    hidden = net.initialZeroState(BATCHSIZE).to(device)\n    hebb = net.initialZeroHebb(BATCHSIZE).to(device)\n    numactionchosen = 0\n    reward = np.zeros(BATCHSIZE)\n    sumreward = np.zeros(BATCHSIZE)\n    rewards = []\n    vs = []\n    logprobs = []\n    dist = 0\n    numactionschosen = np.zeros(BATCHSIZE, dtype='int32')\n    for numstep in range(params['eplen']):\n        inputs = np.zeros((BATCHSIZE, TOTALNBINPUTS), dtype='float32')\n        labg = lab.copy()\n        for nb in range(BATCHSIZE):\n            inputs[nb, 0:(RFSIZE * RFSIZE)] = (labg[(posr[nb] - (RFSIZE \/\/ 2)):((posr[nb] + (RFSIZE \/\/ 2)) + 1), (posc[nb] - (RFSIZE \/\/ 2)):((posc[nb] + (RFSIZE \/\/ 2)) + 1)].flatten() * 1.0)\n            inputs[(nb, ((RFSIZE * RFSIZE) + 1))] = 1.0\n            inputs[(nb, ((RFSIZE * RFSIZE) + 2))] = (numstep \/ params['eplen'])\n            inputs[(nb, ((RFSIZE * RFSIZE) + 3))] = (1.0 * reward[nb])\n            inputs[(nb, (((RFSIZE * RFSIZE) + ADDITIONALINPUTS) + numactionschosen[nb]))] = 1\n        inputsC = torch.from_numpy(inputs).to(device)\n        (y, v, (hidden, hebb)) = net(inputsC, (hidden, hebb))\n        y = torch.softmax(y, dim=1)\n        distrib = torch.distributions.Categorical(y)\n        actionschosen = distrib.sample()\n        logprobs.append(distrib.log_prob(actionschosen))\n        numactionschosen = actionschosen.data.cpu().numpy()\n        reward = np.zeros(BATCHSIZE, dtype='float32')\n        for nb in range(BATCHSIZE):\n            myreward = 0\n            numactionchosen = numactionschosen[nb]\n            tgtposc = posc[nb]\n            tgtposr = posr[nb]\n            if (numactionchosen == 0):\n                tgtposr -= 1\n            elif (numactionchosen == 1):\n                tgtposr += 1\n            elif (numactionchosen == 2):\n                tgtposc -= 1\n            elif (numactionchosen == 3):\n                tgtposc += 1\n            else:\n                raise ValueError('Wrong Action')\n            reward[nb] = 0.0\n            if (lab[tgtposr][tgtposc] == 1):\n                reward[nb] -= params['wp']\n            else:\n                posc[nb] = tgtposc\n                posr[nb] = tgtposr\n            if ((rposr[nb] == posr[nb]) and (rposc[nb] == posc[nb])):\n                reward[nb] += params['rew']\n                posr[nb] = np.random.randint(1, (LABSIZE - 1))\n                posc[nb] = np.random.randint(1, (LABSIZE - 1))\n                while ((lab[(posr[nb], posc[nb])] == 1) or ((rposr[nb] == posr[nb]) and (rposc[nb] == posc[nb]))):\n                    posr[nb] = np.random.randint(1, (LABSIZE - 1))\n                    posc[nb] = np.random.randint(1, (LABSIZE - 1))\n        rewards.append(reward)\n        vs.append(v)\n        sumreward += reward\n        loss += ((params['bent'] * y.pow(2).sum()) \/ BATCHSIZE)\n    R = torch.zeros(BATCHSIZE).to(device)\n    gammaR = params['gr']\n    for numstepb in reversed(range(params['eplen'])):\n        R = ((gammaR * R) + torch.from_numpy(rewards[numstepb]).to(device))\n        ctrR = (R - vs[numstepb][0])\n        lossv += (ctrR.pow(2).sum() \/ BATCHSIZE)\n        loss -= ((logprobs[numstepb] * ctrR.detach()).sum() \/ BATCHSIZE)\n    loss += (params['blossv'] * lossv)\n    loss \/= params['eplen']\n    if PRINTTRACE:\n        if True:\n            print('lossv: ', float(lossv))\n        print('Total reward for this episode (all):', sumreward, 'Dist:', dist)\n    loss.backward()\n    all_grad_norms.append(torch.nn.utils.clip_grad_norm(net.parameters(), params['gc']))\n    if (numiter > 100):\n        optimizer.step()\n    lossnum = float(loss)\n    lossbetweensaves += lossnum\n    all_losses_objective.append(lossnum)\n    all_total_rewards.append(sumreward.mean())\n    if (((numiter + 1) % params['pe']) == 0):\n        print(numiter, '====')\n        print('Mean loss: ', (lossbetweensaves \/ params['pe']))\n        lossbetweensaves = 0\n        print('Mean reward (across batch and last', params['pe'], 'eps.): ', (np.sum(all_total_rewards[(- params['pe']):]) \/ params['pe']))\n        previoustime = nowtime\n        nowtime = time.time()\n        print('Time spent on last', params['pe'], 'iters: ', (nowtime - previoustime))\n    if (((numiter + 1) % params['save_every']) == 0):\n        print('Saving files...')\n        losslast100 = np.mean(all_losses_objective[(- 100):])\n        print('Average loss over the last 100 episodes:', losslast100)\n        print('Saving local files...')\n        with open((('grad_' + suffix) + '.txt'), 'w') as thefile:\n            for item in all_grad_norms[::10]:\n                thefile.write(('%s\\n' % item))\n        with open((('loss_' + suffix) + '.txt'), 'w') as thefile:\n            for item in all_total_rewards[::10]:\n                thefile.write(('%s\\n' % item))\n        torch.save(net.state_dict(), (('torchmodel_' + suffix) + '.dat'))\n        with open((('params_' + suffix) + '.dat'), 'wb') as fo:\n            pickle.dump(params, fo)\n        if os.path.isdir('\/mnt\/share\/tmiconi'):\n            print('Transferring to NFS storage...')\n            for fn in [(('params_' + suffix) + '.dat'), (('loss_' + suffix) + '.txt'), (('torchmodel_' + suffix) + '.dat')]:\n                result = os.system('cp {} {}'.format(fn, ('\/mnt\/share\/tmiconi\/modulmaze\/' + fn)))\n            print('Done!')\n"}
{"label_name":"train","label":0,"method_name":"build_train_generator","method":"\n'\\n    Builds the generator that yields features and their labels.\\n\\n    :param X: features.\\n    :param y: binary labels.\\n    :param batch_size: higher values better utilize GPUs.\\n    :return: generator of features and their labels.\\n    '\nassert (X.shape[0] == y.shape[0]), 'Number of samples mismatch in X and y.'\n\ndef xy_generator():\n    while True:\n        n_batches = (X.shape[0] \/\/ batch_size)\n        if ((n_batches * batch_size) < X.shape[0]):\n            n_batches += 1\n        for i in range(n_batches):\n            start = (i * batch_size)\n            end = min(((i + 1) * batch_size), X.shape[0])\n            (yield (X[start:end], y[start:end]))\nreturn xy_generator()\n"}
{"label_name":"train","label":0,"method_name":"pretrain","method":"\ngen_data_loader = Gen_Data_loader(BATCH_SIZE)\ngen_data_loader.create_batches(positive_samples)\nresults = OrderedDict({'exp_name': PREFIX})\nprint('Start pre-training...')\nstart = time.time()\nfor epoch in tqdm(range(PRE_EPOCH_NUM)):\n    print(' gen pre-train')\n    loss = pre_train_epoch(sess, generator, gen_data_loader)\n    if ((epoch == 10) or ((epoch % 40) == 0)):\n        samples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\n        likelihood_data_loader.create_batches(samples)\n        test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n        print('\\t test_loss {}, train_loss {}'.format(test_loss, loss))\n        mm.compute_results(samples, train_samples, ord_dict, results)\nsamples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\nlikelihood_data_loader.create_batches(samples)\ntest_loss = target_loss(sess, target_lstm, likelihood_data_loader)\nsamples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\nlikelihood_data_loader.create_batches(samples)\nprint('Start training discriminator...')\nfor i in tqdm(range(dis_alter_epoch)):\n    print(' discriminator pre-train')\n    (d_loss, acc) = train_discriminator()\nend = time.time()\nprint('Total time was {:.4f}s'.format((end - start)))\nreturn\n"}
{"label_name":"process","label":2,"method_name":"process_batch","method":"\n\"\\n    Process a list of files with the given Processor in batch mode.\\n\\n    Parameters\\n    ----------\\n    processor : :class:`Processor` instance\\n        Processor to be processed.\\n    files : list\\n        Input file(s) (handles).\\n    output_dir : str, optional\\n        Output directory.\\n    output_suffix : str, optional\\n        Output suffix (e.g. '.txt' including the dot).\\n    strip_ext : bool, optional\\n        Strip off the extension from the input files.\\n    num_workers : int, optional\\n        Number of parallel working threads.\\n    shuffle : bool, optional\\n        Shuffle the `files` before distributing them to the working threads\\n\\n    Notes\\n    -----\\n    Either `output_dir` and\/or `output_suffix` must be set. If `strip_ext` is\\n    True, the extension of the input file names is stripped off before the\\n    `output_suffix` is appended to the input file names.\\n\\n    Use `shuffle` if you experience out of memory errors (can occur for certain\\n    methods with high memory consumptions if consecutive files are rather\\n    long).\\n\\n    \"\nif ((output_dir is None) and (output_suffix is None)):\n    raise ValueError('either output directory or suffix must be given')\nif (output_dir is not None):\n    try:\n        os.mkdir(output_dir)\n    except OSError:\n        pass\ntasks = mp.JoinableQueue()\nprocesses = [_ParallelProcess(tasks) for _ in range(num_workers)]\nfor p in processes:\n    p.daemon = True\n    p.start()\nif shuffle:\n    from random import shuffle\n    shuffle(files)\nfor input_file in files:\n    if (output_dir is not None):\n        output_file = ('%s\/%s' % (output_dir, os.path.basename(input_file)))\n    else:\n        output_file = input_file\n    if strip_ext:\n        output_file = os.path.splitext(output_file)[0]\n    if (output_suffix is not None):\n        output_file += output_suffix\n    tasks.put((processor, input_file, output_file, kwargs))\ntasks.join()\n"}
{"label_name":"save","label":1,"method_name":"image_save","method":"\nDimension = (prediction.ndim - 1)\nPixelDimension = prediction.shape[(- 1)]\nprint('Dimension:', Dimension, 'PixelDimension:', PixelDimension)\nif (PixelDimension < 7):\n    if ((PixelDimension >= 3) and (os.path.splitext(img_obj['out'])[1] not in ['.jpg', '.png'])):\n        ComponentType = itk.ctype('float')\n        PixelType = itk.Vector[(ComponentType, PixelDimension)]\n    elif (PixelDimension == 3):\n        PixelType = itk.RGBPixel.UC\n        prediction = np.absolute(prediction)\n        prediction = np.around(prediction).astype(np.uint16)\n    else:\n        PixelType = itk.ctype('float')\n    OutputImageType = itk.Image[(PixelType, Dimension)]\n    out_img = OutputImageType.New()\nelse:\n    ComponentType = itk.ctype('float')\n    if (Dimension == 1):\n        OutputImageType = itk.VectorImage[(ComponentType, 2)]\n    else:\n        OutputImageType = itk.VectorImage[(ComponentType, Dimension)]\n    out_img = OutputImageType.New()\n    out_img.SetNumberOfComponentsPerPixel(PixelDimension)\nsize = itk.Size[OutputImageType.GetImageDimension()]()\nsize.Fill(1)\nprint('Prediction shape:', prediction.shape)\nprediction_shape = list(prediction.shape[0:(- 1)])\nprediction_shape.reverse()\nif (Dimension == 1):\n    size[1] = prediction_shape[0]\nelse:\n    for (i, s) in enumerate(prediction_shape):\n        size[i] = s\nindex = itk.Index[OutputImageType.GetImageDimension()]()\nindex.Fill(0)\nRegionType = itk.ImageRegion[OutputImageType.GetImageDimension()]\nregion = RegionType()\nregion.SetIndex(index)\nregion.SetSize(size)\nout_img.SetRegions(region)\nif (Dimension == img.GetImageDimension()):\n    out_img.SetDirection(img.GetDirection())\n    out_img.SetOrigin(img.GetOrigin())\n    out_img.SetSpacing(img.GetSpacing())\nout_img.Allocate()\nout_img_np = itk.GetArrayViewFromImage(out_img)\nout_img_np.setfield(np.reshape(prediction, out_img_np.shape), out_img_np.dtype)\nprint('Writing:', img_obj['out'])\nwriter = itk.ImageFileWriter.New(FileName=img_obj['out'], Input=out_img)\nwriter.UseCompressionOn()\nwriter.Update()\n"}
{"label_name":"train","label":0,"method_name":"pytorch_train","method":"\n'\\n    This function transfers the neural network to the right device, \\n    trains it for a certain number of epochs, tests at each epoch on\\n    the validation set and outputs the results on the test set at the\\n    end of training.\\n    \\n    Args:\\n        pytorch_network (torch.nn.Module): The neural network to train.\\n        \\n    Example:\\n        This function displays something like this:\\n        \\n        .. code-block:: python\\n\\n            Epoch 1\/5: loss: 0.5026924496193726, acc: 84.26666259765625, val_loss: 0.17258917854229608, val_acc: 94.75\\n            Epoch 2\/5: loss: 0.13690324830015502, acc: 95.73332977294922, val_loss: 0.14024296019474666, val_acc: 95.68333435058594\\n            Epoch 3\/5: loss: 0.08836929737279813, acc: 97.29582977294922, val_loss: 0.10380942322810491, val_acc: 96.66666412353516\\n            Epoch 4\/5: loss: 0.06714504160980383, acc: 97.91874694824219, val_loss: 0.09626663728555043, val_acc: 97.18333435058594\\n            Epoch 5\/5: loss: 0.05063822727650404, acc: 98.42708587646484, val_loss: 0.10017542181412378, val_acc: 96.95833587646484\\n            Test:\\n                Loss: 0.09501855444908142\\n                Accuracy: 97.12999725341797\\n    '\nprint(pytorch_network)\npytorch_network.to(device)\noptimizer = optim.SGD(pytorch_network.parameters(), lr=learning_rate)\nloss_function = nn.CrossEntropyLoss()\nfor epoch in range(1, (num_epochs + 1)):\n    (train_loss, train_acc) = pytorch_train_one_epoch(pytorch_network, optimizer, loss_function)\n    (valid_loss, valid_acc) = pytorch_test(pytorch_network, valid_loader, loss_function)\n    print('Epoch {}\/{}: loss: {}, acc: {}, val_loss: {}, val_acc: {}'.format(epoch, num_epochs, train_loss, train_acc, valid_loss, valid_acc))\n(test_loss, test_acc) = pytorch_test(pytorch_network, test_loader, loss_function)\nprint('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))\n"}
{"label_name":"predict","label":4,"method_name":"get_prediction_fpath","method":"\nfname = '{:s}_{:s}'.format(basename, (dset + c.PRED_FILE_EXT))\nreturn os.path.join(cfg.PATHS['predictions'], fname)\n"}
{"label_name":"train","label":0,"method_name":"build_train_step","method":"\nassert (optimizer in ['momentum', 'adam', 'rmsprop'])\nif (optimizer == 'momentum'):\n    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\nelif (optimizer == 'rmsprop'):\n    optimizer = tf.train.RMSPropOptimizer(learning_rate, 0.9, 0.9)\nelif (optimizer == 'adam'):\n    optimizer = tf.train.AdamOptimizer(learning_rate)\nelse:\n    raise AssertionError('invalid optimizer type {}'.format(optimizer))\ntf.summary.scalar('learning_rate', learning_rate)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    if (grad_clip_norm is not None):\n        grads_and_vars = optimizer.compute_gradients(loss)\n        grads = [x[0] for x in grads_and_vars]\n        vars = [x[1] for x in grads_and_vars]\n        (grads, _) = tf.clip_by_global_norm(grads, grad_clip_norm)\n        return optimizer.apply_gradients(zip(grads, vars), global_step=global_step)\n    else:\n        return optimizer.minimize(loss, global_step=global_step)\n"}
{"label_name":"forward","label":3,"method_name":"forward_pre_hook1","method":"\ninput_return = ((input[0] * 2), input[1])\nreturn input_return\n"}
{"label_name":"train","label":0,"method_name":"lms_train","method":"\n\ndef error(p, y, args):\n    l = len(p)\n    f = p[(l - 1)]\n    for i in range(len(args)):\n        f += (p[i] * args[i])\n    return (f - y)\nPara = leastsq(error, p0, args=(Zi, Data))\nreturn Para[0]\n"}
{"label_name":"save","label":1,"method_name":"_save_params","method":"\ncurrent_map = float(current_map)\nif (current_map > best_map[0]):\n    logger.info('[Epoch {}] mAP {} higher than current best {} saving to {}'.format(epoch, current_map, best_map, '{:s}_best.params'.format(prefix)))\n    best_map[0] = current_map\n    net.save_parameters('{:s}_best.params'.format(prefix))\n    with open((prefix + '_best_map.log'), 'a') as log_file:\n        log_file.write('\\n{:04d}:\\t{:.4f}'.format(epoch, current_map))\nif (save_interval and (((epoch + 1) % save_interval) == 0)):\n    logger.info('[Epoch {}] Saving parameters to {}'.format(epoch, '{:s}_{:04d}_{:.4f}.params'.format(prefix, epoch, current_map)))\n    net.save_parameters('{:s}_{:04d}_{:.4f}.params'.format(prefix, epoch, current_map))\n"}
{"label_name":"train","label":0,"method_name":"get_specified_train_val_filenames","method":"\nreturn\n"}
{"label_name":"process","label":2,"method_name":"_preprocess_sgm","method":"\n'Preprocessing to strip tags in SGM files.'\nif (not is_sgm):\n    return line\nif (line.startswith('<srcset') or line.startswith('<\/srcset')):\n    return ''\nif (line.startswith('<doc') or line.startswith('<\/doc')):\n    return ''\nif (line.startswith('<p>') or line.startswith('<\/p>')):\n    return ''\nline = line.strip()\nif (line.startswith('<seg') and line.endswith('<\/seg>')):\n    i = line.index('>')\n    return line[(i + 1):(- 6)]\n"}
{"label_name":"process","label":2,"method_name":"html_preprocessing","method":"\nfor tag in soup.find_all('script'):\n    tag.clear()\nfor tag in soup.find_all('style'):\n    tag.clear()\nfor tag in soup.find_all('link'):\n    tag.clear()\nfor tag in soup.findAll('meta'):\n    tag.attrs = None\nfor tag in soup.findAll('stript'):\n    tag.attrs = None\nfor tag in soup.findAll('link'):\n    tag.attrs = None\nphrase_tag_list = ['em', 'strong', 'code', 'samp', 'kbd', 'var']\nfor tag in phrase_tag_list:\n    for match in soup.find_all(tag):\n        match.replaceWithChildren()\ntexthighlight_tag_list = ['b', 'mark']\nfor tag in texthighlight_tag_list:\n    for match in soup.find_all(tag):\n        match.replaceWithChildren()\nfor head in soup('head'):\n    soup.head.extract()\nsoup = BeautifulSoup(str(soup).replace('<br>', '(br)'))\nfor tag in soup.findAll('a', href=True):\n    tag.wrap(soup.new_tag('grit'))\n    tag.replaceWithChildren()\nsoup = BeautifulSoup(str(soup).replace('<\/grit>', ' endhyper'))\nsoup = BeautifulSoup(str(soup).replace('<grit>', 'starthyper '))\nreturn soup\n"}
{"label_name":"save","label":1,"method_name":"save_config_file_from_dict","method":"\ns = b'wandb_version: 1'\nif config_dict:\n    s += (b'\\n\\n' + yaml.dump(config_dict, Dumper=yaml.SafeDumper, default_flow_style=False, allow_unicode=True, encoding='utf-8'))\ndata = s.decode('utf-8')\nfilesystem._safe_makedirs(os.path.dirname(config_filename))\nwith open(config_filename, 'w') as conf_file:\n    conf_file.write(data)\n"}
{"label_name":"train","label":0,"method_name":"train_nb0","method":"\nnum_train_docs = len(train_matrix)\nnum_words = len(train_matrix[0])\np_abusive = (sum(train_category) \/ float(num_train_docs))\np0_num = ones(num_words)\np1_num = ones(num_words)\np0_denom = 2.0\np1_denom = 2.0\nfor i in range(num_train_docs):\n    if (train_category[i] == 1):\n        p1_num += train_matrix[i]\n        p1_denom += 1\n    else:\n        p0_num += train_matrix[i]\n        p0_denom += 1\np1_vect = log((p1_num \/ p1_denom))\np0_vect = log((p0_num \/ p0_denom))\nreturn (p0_vect, p1_vect, p_abusive)\n"}
{"label_name":"save","label":1,"method_name":"create_tracker_with_partially_saved_events","method":"\nsender_id = uuid.uuid4().hex\nevents = [UserUttered('hello'), BotUttered('what')]\ntracker = DialogueStateTracker.from_events(sender_id, events)\ntracker_store.save(tracker)\nevents = [ActionExecuted(ACTION_LISTEN_NAME), UserUttered('123'), BotUttered('yes')]\nfor event in events:\n    tracker.update(event)\nreturn (events, tracker)\n"}
{"label_name":"train","label":0,"method_name":"_train_on_generated_data","method":"\n'The training portion of parameter recovery tests.'\nrandom_seed.set_random_seed(seed)\ngenerate_graph = ops.Graph()\nwith generate_graph.as_default():\n    with session.Session(graph=generate_graph):\n        generative_model.initialize_graph()\n        (time_series_reader, true_parameters) = generate_fn(generative_model)\n        true_parameters = {tensor.name: value for (tensor, value) in true_parameters.items()}\neval_input_fn = input_pipeline.WholeDatasetInputFn(time_series_reader)\neval_state_manager = state_management.PassthroughStateManager()\ntrue_parameter_eval_graph = ops.Graph()\nwith true_parameter_eval_graph.as_default():\n    generative_model.initialize_graph()\n    ignore_params = ignore_params_fn(generative_model)\n    (feature_dict, _) = eval_input_fn()\n    eval_state_manager.initialize_graph(generative_model)\n    feature_dict[TrainEvalFeatures.VALUES] = math_ops.cast(feature_dict[TrainEvalFeatures.VALUES], generative_model.dtype)\n    model_outputs = eval_state_manager.define_loss(model=generative_model, features=feature_dict, mode=estimator_lib.ModeKeys.EVAL)\n    with session.Session(graph=true_parameter_eval_graph) as sess:\n        variables.global_variables_initializer().run()\n        coordinator = coordinator_lib.Coordinator()\n        queue_runner_impl.start_queue_runners(sess, coord=coordinator)\n        true_param_loss = model_outputs.loss.eval(feed_dict=true_parameters)\n        true_transformed_params = {param: param.eval(feed_dict=true_parameters) for param in derived_param_test_fn(generative_model)}\n        coordinator.request_stop()\n        coordinator.join()\nsaving_hook = _SavingTensorHook(tensors=true_parameters.keys(), every_n_iter=(train_iterations - 1))\n\nclass _RunConfig(estimator_lib.RunConfig):\n\n    @property\n    def tf_random_seed(self):\n        return seed\nestimator = estimators.TimeSeriesRegressor(model=generative_model, config=_RunConfig(), state_manager=train_state_manager, optimizer=adam.AdamOptimizer(learning_rate))\ntrain_input_fn = train_input_fn_type(time_series_reader=time_series_reader)\ntrained_loss = estimator.train(input_fn=train_input_fn, max_steps=train_iterations, hooks=[saving_hook]).evaluate(input_fn=eval_input_fn, steps=1)['loss']\nlogging.info('Final trained loss: %f', trained_loss)\nlogging.info('True parameter loss: %f', true_param_loss)\nreturn (ignore_params, true_parameters, true_transformed_params, trained_loss, true_param_loss, saving_hook, true_parameter_eval_graph)\n"}
{"label_name":"process","label":2,"method_name":"deprocess","method":"\nNormalize = transforms.Compose([transforms.Normalize(mean=[(- 103.939), (- 116.779), (- 123.68)], std=[1, 1, 1])])\nbgr2rgb = transforms.Compose([transforms.Lambda((lambda x: x[torch.LongTensor([2, 1, 0])]))])\nResizeImage = transforms.Compose([transforms.Resize(image_size)])\noutput_tensor = (bgr2rgb(Normalize(output_tensor.squeeze(0))) \/ 256)\noutput_tensor.clamp_(0, 1)\nImage2PIL = transforms.ToPILImage()\nimage = Image2PIL(output_tensor)\nimage = ResizeImage(image)\nimage.save(str(output_name))\n"}
{"label_name":"train","label":0,"method_name":"train_step","method":"\nwith tf.GradientTape() as tape:\n    y_pred = model(x)\n    loss = mse_loss(y_pred, y)\ngradients = tape.gradient(loss, vars(model).values())\noptimizer.apply_gradients(zip(gradients, vars(model).values()))\n"}
{"label_name":"save","label":1,"method_name":"save_config","method":"\nwith open(path, 'w') as f:\n    f.write(('Epochs: %d\\n' % args.epochs))\n    f.write(('Batchsize: %d\\n' % args.batch_size))\n    f.write(('Learning rate: %f\\n' % args.lr))\n    f.write(('Beta_1: %f\\n' % args.beta_1))\n    f.write(('Beta_2: %f\\n' % args.beta_2))\n"}
{"label_name":"predict","label":4,"method_name":"model_predicted_label_config","method":"\npredictor = LinearLearnerPredictor(model_name, sagemaker_session=sagemaker_session)\nresult = predictor.predict(training_set[0].astype(np.float32))\npredictions = [float(record.label['score'].float32_tensor.values[0]) for record in result]\nprobability_threshold = statistics.median(predictions)\nreturn ModelPredictedLabelConfig(label='score', probability_threshold=probability_threshold)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nwith open(xFile, 'rb') as file_r:\n    X = pickle.load(file_r)\nX = reshape(X, (212841, (- 1)))\nwith open(yFile, 'r') as yFile_r:\n    labelLines = [_.strip('\\n') for _ in yFile_r.readlines()]\nvalues = array(labelLines)\nlabelEncoder = LabelEncoder()\nintegerEncoded = labelEncoder.fit_transform(values)\nintegerEncoded = integerEncoded.reshape(len(integerEncoded), 1)\nY = integerEncoded.reshape(212841)\n(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.3, random_state=42)\nclf = GBC(loss='deviance', subsample=0.8, criterion='friedman_mse')\nclf.fit(X_train, Y_train)\npredict = clf.predict(X_test)\ncount = 0\nfor (p, t) in zip(predict, Y_test):\n    if (p == t):\n        count += 1\nprint('GradientBoosting  Accuracy is:', (count \/ len(Y_test)))\n"}
{"label_name":"save","label":1,"method_name":"save_distance_to_cache","method":"\n'\\n    \u4ece\u7f13\u51b2\u4e2d\u83b7\u5f97\u8ddd\u79bb\u548c\u8def\u5f84\\n    '\nif ((source, target) in DISTANCE_CACHE):\n    assert (get_distance_from_cache(source, target)[0] == distance)\nDISTANCE_CACHE[(source, target)] = (distance, vertex_path, road_path)\n"}
{"label_name":"predict","label":4,"method_name":"show_prediction_labels_on_image","method":"\n'\\n    Shows the face recognition results visually.\\n\\n    :param img_path: path to image to be recognized\\n    :param predictions: results of the predict function\\n    :return:\\n    '\npil_image = Image.open(img_path).convert('RGB')\ndraw = ImageDraw.Draw(pil_image)\nfor (name, (top, right, bottom, left)) in predictions:\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n    name = name.encode('UTF-8')\n    (text_width, text_height) = draw.textsize(name)\n    draw.rectangle(((left, ((bottom - text_height) - 10)), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text(((left + 6), ((bottom - text_height) - 5)), name, fill=(255, 255, 255, 255))\ndel draw\npil_image.show()\n"}
{"label_name":"save","label":1,"method_name":"save_results","method":"\nif (results_rows is not None):\n    df = pd.DataFrame(results_rows)\n    df.to_csv('{}_results.csv'.format(folder), index=False)\nmodel_saver = tf.train.Saver()\nckpt_dir = os.path.join(params['CHK_PATH'], folder)\nif (not os.path.exists(ckpt_dir)):\n    os.makedirs(ckpt_dir)\nckpt_file = os.path.join(ckpt_dir, '{}.ckpt'.format(name))\npath = model_saver.save(sess, ckpt_file)\nprint('Model saved at {}'.format(path))\nreturn\n"}
{"label_name":"train","label":0,"method_name":"backwardTConstraint","method":"\narg = innermostFunction(left)\nreturn (arg.dir().is_forward() and arg.res().is_primitive())\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\ntotal_steps = 0\nsteps = []\nepisodes = []\nfor i_episode in range(20):\n    observation = env.reset()\n    while True:\n        action = RL.choose_action(observation)\n        (observation_, reward, done, info) = env.step(action)\n        if done:\n            reward = 10\n        RL.store_transition(observation, action, reward, observation_)\n        if (total_steps > MEMORY_SIZE):\n            RL.learn()\n        if done:\n            print('episode ', i_episode, ' finished')\n            steps.append(total_steps)\n            episodes.append(i_episode)\n            break\n        observation = observation_\n        total_steps += 1\nreturn np.vstack((episodes, steps))\n"}
{"label_name":"train","label":0,"method_name":"set_c1_spiketrains","method":"\nfor (size, layers_as_dicts) in ddict.items():\n    for layer_as_dict in layers_as_dicts:\n        spiketrains = layer_as_dict['segment'].spiketrains\n        dimensionless_sts = [[s for s in st] for st in spiketrains]\n        the_layer_iter = filter((lambda layer: (layer.population.label == layer_as_dict['label'])), layer_collection['C1'][size])\n        the_layer_iter.__next__().population.set(spike_times=dimensionless_sts)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\nprint(f'Training {parameters}')\nstart = time.time()\nlstm = keras_lstm_gen.Model()\nhistory = lstm.train(climbset, parameters)\nprint(f'Training {parameters} took {(time.time() - start)}')\nreturn (parameters, min(history.history['loss']), min(history.history['val_loss']), (time.time() - start))\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\njson_data = request.get_json()\nif (not json_data):\n    return (jsonify({'message': 'No input data provided'}), 400)\nproduct = ProductSchema().load(json_data)\nif product.errors:\n    return (jsonify(product.errors), 422)\ncategory = 'Cachorro, Ra\u00e7\u00f5es, Ra\u00e7\u00e3o Seca'\nreturn jsonify({'prediction': {'category': category}})\n"}
{"label_name":"train","label":0,"method_name":"set_training_params","method":"\n'User selects training parameters.\\n    '\nif use_defaults:\n    (n_epochs, batch_size, epsilon) = default_training_params()\n    return (n_epochs, batch_size, epsilon)\nprint('Select number of epochs to train (default 100):')\nn_epochs = int(input())\nprint('Select batch size (default 64):')\nbatch_size = int(input())\nprint('Select learning rate (default 0.0001):')\nepsilon = float(input())\nreturn (n_epochs, batch_size, epsilon)\n"}
{"label_name":"train","label":0,"method_name":"split_train","method":"\ntrainX = X[0:split_point]\ntrainX = np.asarray(trainX)\ntrainY = Y[0:split_point]\ntrainY = np.asarray(trainY)\nreturn (trainX, trainY)\n"}
{"label_name":"process","label":2,"method_name":"_deprocess_image","method":"\nif (np.ndim(x) > 3):\n    x = np.squeeze(x)\nx -= x.mean()\nx \/= (x.std() + 1e-05)\nx *= 0.1\nx += 0.5\nx = np.clip(x, 0, 1)\nx *= 255\nx = np.clip(x, 0, 255).astype('uint8')\nreturn x\n"}
{"label_name":"save","label":1,"method_name":"save_obj","method":"\ndill.dump(obj, open(file_path, 'wb'))\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\nends = (Xr.lengths.cumsum() - 1)\nY = cast(OutT, Xr.dataXd[ends])\nx_shape = Xr.dataXd.shape\nlengths = Xr.lengths\n\ndef backprop(dY: OutT) -> Ragged:\n    dX = cast(OutT, model.ops.alloc(x_shape, dtype=dY.dtype))\n    dX[ends] = dY\n    return Ragged(dX, lengths)\nreturn (Y, backprop)\n"}
{"label_name":"save","label":1,"method_name":"save_labels_to_file","method":"\nwith open((output + '\/labels'), 'w') as target:\n    target.write(str(labels))\n"}
{"label_name":"train","label":0,"method_name":"train_and_export","method":"\ncls = get_trainer_class(algo_name)\nalg = cls(config={}, env='CartPole-v0')\nfor _ in range(num_steps):\n    alg.train()\nalg.export_policy_checkpoint(ckpt_dir, filename_prefix=prefix)\nalg.export_policy_model(model_dir)\n"}
{"label_name":"save","label":1,"method_name":"retrieve_init_savers","method":"\n'Retrieve a dictionary of all the initial savers for the models.\\n\\n  Args:\\n    hparams:  MaskGAN hyperparameters.\\n  '\ninit_savers = {}\nif FLAGS.maskgan_ckpt:\n    gen_vars = [v for v in tf.trainable_variables() if v.op.name.startswith('gen')]\n    init_saver = tf.train.Saver(var_list=gen_vars)\n    init_savers['init_saver'] = init_saver\n    if (FLAGS.discriminator_model == 'seq2seq_vd'):\n        dis_variable_maps = variable_mapping.dis_seq2seq_vd(hparams)\n        dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)\n        init_savers['dis_init_saver'] = dis_init_saver\nif FLAGS.language_model_ckpt_dir:\n    if (FLAGS.maskgan_ckpt is None):\n        if (FLAGS.generator_model == 'rnn_nas'):\n            gen_variable_maps = variable_mapping.rnn_nas(hparams, model='gen')\n            gen_init_saver = tf.train.Saver(var_list=gen_variable_maps)\n            init_savers['gen_init_saver'] = gen_init_saver\n        elif (FLAGS.generator_model == 'seq2seq_nas'):\n            gen_encoder_variable_maps = variable_mapping.gen_encoder_seq2seq_nas(hparams)\n            gen_encoder_init_saver = tf.train.Saver(var_list=gen_encoder_variable_maps)\n            gen_decoder_variable_maps = variable_mapping.gen_decoder_seq2seq_nas(hparams)\n            gen_decoder_init_saver = tf.train.Saver(var_list=gen_decoder_variable_maps)\n            init_savers['gen_encoder_init_saver'] = gen_encoder_init_saver\n            init_savers['gen_decoder_init_saver'] = gen_decoder_init_saver\n        elif ((FLAGS.generator_model == 'seq2seq_zaremba') or (FLAGS.generator_model == 'seq2seq_vd')):\n            gen_encoder_variable_maps = variable_mapping.gen_encoder_seq2seq(hparams)\n            gen_encoder_init_saver = tf.train.Saver(var_list=gen_encoder_variable_maps)\n            gen_decoder_variable_maps = variable_mapping.gen_decoder_seq2seq(hparams)\n            gen_decoder_init_saver = tf.train.Saver(var_list=gen_decoder_variable_maps)\n            init_savers['gen_encoder_init_saver'] = gen_encoder_init_saver\n            init_savers['gen_decoder_init_saver'] = gen_decoder_init_saver\n        else:\n            raise NotImplementedError\n    if (FLAGS.discriminator_model == 'rnn_nas'):\n        dis_variable_maps = variable_mapping.rnn_nas(hparams, model='dis')\n        dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)\n        init_savers['dis_init_saver'] = dis_init_saver\n    elif ((FLAGS.discriminator_model == 'rnn_zaremba') or (FLAGS.discriminator_model == 'rnn_vd')):\n        dis_variable_maps = variable_mapping.rnn_zaremba(hparams, model='dis')\n        dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)\n        init_savers['dis_init_saver'] = dis_init_saver\n    elif ((FLAGS.discriminator_model == 'bidirectional_zaremba') or (FLAGS.discriminator_model == 'bidirectional_vd')):\n        dis_fwd_variable_maps = variable_mapping.dis_fwd_bidirectional(hparams)\n        dis_bwd_variable_maps = variable_mapping.dis_bwd_bidirectional(hparams)\n        dis_fwd_init_saver = tf.train.Saver(var_list=dis_fwd_variable_maps)\n        dis_bwd_init_saver = tf.train.Saver(var_list=dis_bwd_variable_maps)\n        init_savers['dis_fwd_init_saver'] = dis_fwd_init_saver\n        init_savers['dis_bwd_init_saver'] = dis_bwd_init_saver\n    elif (FLAGS.discriminator_model == 'cnn'):\n        dis_variable_maps = variable_mapping.cnn()\n        dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)\n        init_savers['dis_init_saver'] = dis_init_saver\n    elif (FLAGS.discriminator_model == 'seq2seq_vd'):\n        dis_encoder_variable_maps = variable_mapping.dis_encoder_seq2seq(hparams)\n        dis_encoder_init_saver = tf.train.Saver(var_list=dis_encoder_variable_maps)\n        dis_decoder_variable_maps = variable_mapping.dis_decoder_seq2seq(hparams)\n        dis_decoder_init_saver = tf.train.Saver(var_list=dis_decoder_variable_maps)\n        init_savers['dis_encoder_init_saver'] = dis_encoder_init_saver\n        init_savers['dis_decoder_init_saver'] = dis_decoder_init_saver\nreturn init_savers\n"}
{"label_name":"process","label":2,"method_name":"conv3d_transpose_ncdhw_preprocess","method":"\n'Preprocess data and kernel to make the compute pattern\\n    of conv3d_transpose the same as conv3d'\n(batch, in_c, in_d, in_h, in_w) = data.shape\n(_, out_c, filter_d, filter_h, filter_w) = kernel.shape\n(stride_d, stride_h, stride_w) = strides\n(opad_d, opad_h, opad_w) = output_padding\nassert ((opad_d < stride_d) and (opad_h < stride_h) and (opad_w < stride_w))\ndata_dilate = dilate(data, [1, 1, stride_d, stride_h, stride_w], name='data_dilate')\n(fpad_front, fpad_top, fpad_left, fpad_back, fpad_bottom, fpad_right) = get_pad_tuple3d(padding, (filter_d, filter_h, filter_w))\nbpad_front = ((filter_d - 1) - fpad_front)\nbpad_back = (((filter_d - 1) - fpad_back) + opad_d)\nbpad_top = ((filter_h - 1) - fpad_top)\nbpad_bottom = (((filter_h - 1) - fpad_bottom) + opad_h)\nbpad_left = ((filter_w - 1) - fpad_left)\nbpad_right = (((filter_w - 1) - fpad_right) + opad_w)\ndata_pad = pad(data_dilate, [0, 0, bpad_front, bpad_top, bpad_left], [0, 0, bpad_back, bpad_bottom, bpad_right], name='data_pad')\nkernel_transform = te.compute((out_c, in_c, filter_d, filter_h, filter_w), (lambda o, i, d, h, w: kernel[i][o][((filter_d - 1) - d)][((filter_h - 1) - h)][((filter_w - 1) - w)]), name='kernel_transform')\nreturn (data_pad, kernel_transform)\n"}
{"label_name":"train","label":0,"method_name":"split_train_valid","method":"\n'\\n    k-fold\u4ea4\u53c9\u9a8c\u8bc1,\u9ed8\u8ba4k=10\\n    df_train:\u8bad\u7ec3\u6570\u636e\\n    '\n(X_train, X_vali, y_train, y_vali) = train_test_split(df_train[features], df_train[label], test_size=test_size, random_state=40000)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_vali, label=y_vali)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\nreturn (dtrain, dvalid, watchlist)\n"}
{"label_name":"predict","label":4,"method_name":"filter_nodule_predictions","method":"\nsrc_dir = settings.NDSB3_NODULE_DETECTION_DIR\nfor (csv_index, csv_path) in enumerate(glob.glob((src_dir + '*.csv'))):\n    file_name = ntpath.basename(csv_path)\n    patient_id = file_name.replace('.csv', '')\n    print(csv_index, ': ', patient_id)\n    if ((only_patient_id is not None) and (patient_id != only_patient_id)):\n        continue\n    df_nodule_predictions = pandas.read_csv(csv_path)\n    filter_patient_nodules_predictions(df_nodule_predictions, patient_id, CUBE_SIZE)\n    df_nodule_predictions.to_csv(csv_path, index=False)\n"}
{"label_name":"process","label":2,"method_name":"_process_operator","method":"\nsteps = []\nop_name = operator[0]\nif (op_name == 'CombineDFs'):\n    steps.append(_combine_dfs(operator[1], operator[2], operators))\nelse:\n    (input_name, args) = (operator[1], operator[2:])\n    tpot_op = get_by_name(op_name, operators)\n    if (input_name != 'input_matrix'):\n        steps.extend(_process_operator(input_name, operators, (depth + 1)))\n    if (tpot_op.root and (depth > 0)):\n        steps.append('StackingEstimator(estimator={})'.format(tpot_op.export(*args)))\n    else:\n        steps.append(tpot_op.export(*args))\nreturn steps\n"}
{"label_name":"save","label":1,"method_name":"register_save","method":"\n'\\n    Registers an image save function.  This function should not be\\n    used in application code.\\n\\n    :param id: An image format identifier.\\n    :param driver: A function to save images in this format.\\n    '\nSAVE[id.upper()] = driver\n"}
{"label_name":"save","label":1,"method_name":"create_module_spec_from_saved_model","method":"\n'Experimental: Create a ModuleSpec out of a SavedModel from TF1.\\n\\n  Warning: Deprecated. This belongs to the hub.Module API and TF1 Hub format.\\n  For TF2, TensorFlow Hub ships plain SavedModels, removing the need for\\n  conversions like this.\\n\\n  Define a ModuleSpec from a SavedModel. Note that this is not guaranteed to\\n  work in all cases and it assumes the SavedModel has followed some conventions:\\n\\n  - The serialized SaverDef can be ignored and instead can be reconstructed.\\n  - The init op and main op can be ignored and instead the module can be\\n    initialized by using the conventions followed by\\n    `tf.train.MonitoredSession`.\\n\\n  Note that the set of features supported can increase over time and have side\\n  effects that were not previously visible. The pattern followed to avoid\\n  surprises is forcing users to declare which features to ignore (even\\n  if they are not supported).\\n\\n  Note that this function creates a ModuleSpec that when exported exports a\\n  Module (based on a modified copy of the original SavedModel) and not a\\n  SavedModel.\\n\\n  THIS FUNCTION IS DEPRECATED.\\n\\n  Args:\\n    saved_model_path: Directory with the SavedModel to use.\\n    drop_collections: Additionally list of collection to drop.\\n\\n  Returns:\\n    A ModuleSpec.\\n  '\nsaved_model_handler = saved_model_lib.load(saved_model_path)\ncheckpoint_filename = saved_model_lib.get_variables_path(saved_model_path)\ndrop_collections = (set(_ALWAYS_DROPPED_COLLECTIONS) | (set(drop_collections) if drop_collections else set()))\n_drop_collections(saved_model_handler, drop_collections)\nreturn native_module._ModuleSpec(saved_model_handler, checkpoint_filename)\n"}
{"label_name":"process","label":2,"method_name":"plot_gaussian_process","method":"\n'Plots the optimization results and the gaussian process\\n    for 1-D objective functions.\\n\\n    Parameters\\n    ----------\\n    res :  `OptimizeResult`\\n        The result for which to plot the gaussian process.\\n\\n    ax : `Axes`, optional\\n        The matplotlib axes on which to draw the plot, or `None` to create\\n        a new one.\\n\\n    n_calls : int, default: -1\\n        Can be used to evaluate the model at call `n_calls`.\\n\\n    objective : func, default: None\\n        Defines the true objective function. Must have one input parameter.\\n\\n    n_points : int, default: 1000\\n        Number of data points used to create the plots\\n\\n    noise_level : float, default: 0\\n        Sets the estimated noise level\\n\\n    show_legend : boolean, default: True\\n        When True, a legend is plotted.\\n\\n    show_title : boolean, default: True\\n        When True, a title containing the found minimum value\\n        is shown\\n\\n    show_acq_func : boolean, default: False\\n        When True, the acquisition function is plotted\\n\\n    show_next_point : boolean, default: False\\n        When True, the next evaluated point is plotted\\n\\n    show_observations : boolean, default: True\\n        When True, observations are plotted as dots.\\n\\n    show_mu : boolean, default: True\\n        When True, the predicted model is shown.\\n\\n    Returns\\n    -------\\n    ax : `Axes`\\n        The matplotlib axes.\\n    '\nax = kwargs.get('ax', None)\nn_calls = kwargs.get('n_calls', (- 1))\nobjective = kwargs.get('objective', None)\nnoise_level = kwargs.get('noise_level', 0)\nshow_legend = kwargs.get('show_legend', True)\nshow_title = kwargs.get('show_title', True)\nshow_acq_func = kwargs.get('show_acq_func', False)\nshow_next_point = kwargs.get('show_next_point', False)\nshow_observations = kwargs.get('show_observations', True)\nshow_mu = kwargs.get('show_mu', True)\nn_points = kwargs.get('n_points', 1000)\nif (ax is None):\n    ax = plt.gca()\nn_dims = res.space.n_dims\nassert (n_dims == 1), 'Space dimension must be 1'\ndimension = res.space.dimensions[0]\n(x, x_model) = _evenly_sample(dimension, n_points)\nx = x.reshape((- 1), 1)\nx_model = x_model.reshape((- 1), 1)\nif ((res.specs is not None) and ('args' in res.specs)):\n    n_random = res.specs['args'].get('n_random_starts', None)\n    acq_func = res.specs['args'].get('acq_func', 'EI')\n    acq_func_kwargs = res.specs['args'].get('acq_func_kwargs', {})\nif (acq_func_kwargs is None):\n    acq_func_kwargs = {}\nif ((acq_func is None) or (acq_func == 'gp_hedge')):\n    acq_func = 'EI'\nif (n_random is None):\n    n_random = (len(res.x_iters) - len(res.models))\nif (objective is not None):\n    fx = np.array([objective(x_i) for x_i in x])\nif (n_calls < 0):\n    model = res.models[(- 1)]\n    curr_x_iters = res.x_iters\n    curr_func_vals = res.func_vals\nelse:\n    model = res.models[n_calls]\n    curr_x_iters = res.x_iters[:(n_random + n_calls)]\n    curr_func_vals = res.func_vals[:(n_random + n_calls)]\nif (objective is not None):\n    ax.plot(x, fx, 'r--', label='True (unknown)')\n    ax.fill(np.concatenate([x, x[::(- 1)]]), np.concatenate(([(fx_i - (1.96 * noise_level)) for fx_i in fx], [(fx_i + (1.96 * noise_level)) for fx_i in fx[::(- 1)]])), alpha=0.2, fc='r', ec='None')\nif show_mu:\n    (y_pred, sigma) = model.predict(x_model, return_std=True)\n    ax.plot(x, y_pred, 'g--', label='$\\\\mu_{GP}(x)$')\n    ax.fill(np.concatenate([x, x[::(- 1)]]), np.concatenate([(y_pred - (1.96 * sigma)), (y_pred + (1.96 * sigma))[::(- 1)]]), alpha=0.2, fc='g', ec='None')\nif show_observations:\n    ax.plot(curr_x_iters, curr_func_vals, 'r.', markersize=8, label='Observations')\nif ((show_mu or show_observations or (objective is not None)) and show_acq_func):\n    ax_ei = ax.twinx()\n    ax_ei.set_ylabel((str(acq_func) + '(x)'))\n    plot_both = True\nelse:\n    ax_ei = ax\n    plot_both = False\nif show_acq_func:\n    acq = _gaussian_acquisition(x_model, model, y_opt=np.min(curr_func_vals), acq_func=acq_func, acq_func_kwargs=acq_func_kwargs)\n    next_x = x[np.argmin(acq)]\n    next_acq = acq[np.argmin(acq)]\n    acq = (- acq)\n    next_acq = (- next_acq)\n    ax_ei.plot(x, acq, 'b', label=(str(acq_func) + '(x)'))\n    if (not plot_both):\n        ax_ei.fill_between(x.ravel(), 0, acq.ravel(), alpha=0.3, color='blue')\n    if (show_next_point and (next_x is not None)):\n        ax_ei.plot(next_x, next_acq, 'bo', markersize=6, label='Next query point')\nif show_title:\n    ax.set_title(('x* = %.4f, f(x*) = %.4f' % (res.x[0], res.fun)))\nax.grid()\nax.set_xlabel('x')\nax.set_ylabel('f(x)')\nif show_legend:\n    if plot_both:\n        (lines, labels) = ax.get_legend_handles_labels()\n        (lines2, labels2) = ax_ei.get_legend_handles_labels()\n        ax_ei.legend((lines + lines2), (labels + labels2), loc='best', prop={'size': 6}, numpoints=1)\n    else:\n        ax.legend(loc='best', prop={'size': 6}, numpoints=1)\nreturn ax\n"}
{"label_name":"process","label":2,"method_name":"postprocess_nstep_and_prio","method":"\nif (policy.config['n_step'] > 1):\n    _adjust_nstep(policy.config['n_step'], policy.config['gamma'], batch[SampleBatch.CUR_OBS], batch[SampleBatch.ACTIONS], batch[SampleBatch.REWARDS], batch[SampleBatch.NEXT_OBS], batch[SampleBatch.DONES])\nif (PRIO_WEIGHTS not in batch):\n    batch[PRIO_WEIGHTS] = np.ones_like(batch[SampleBatch.REWARDS])\nif ((batch.count > 0) and policy.config['worker_side_prioritization']):\n    td_errors = policy.compute_td_error(batch[SampleBatch.CUR_OBS], batch[SampleBatch.ACTIONS], batch[SampleBatch.REWARDS], batch[SampleBatch.NEXT_OBS], batch[SampleBatch.DONES], batch[PRIO_WEIGHTS])\n    new_priorities = (np.abs(convert_to_numpy(td_errors)) + policy.config['prioritized_replay_eps'])\n    batch[PRIO_WEIGHTS] = new_priorities\nreturn batch\n"}
{"label_name":"process","label":2,"method_name":"process_sample","method":"\n'\\n    Processes a sample after it has been submitted.  This runs as a thread.\\n\\n    :param submission_id:  The ID of the submission in the database.\\n    :return: Nothing.\\n    '\nsys.modules['library'] = library\ntry:\n    submission = Submission.query.filter_by(id=submission_id).first()\nexcept Exception as exc:\n    app.logger.exception('Exception while looking up submission: {0}'.format(exc))\n    return\ntry:\n    ml = ML.load_classifier(os.path.join('..', '..', 'classifier'))\n    s = Sample(fromfile=os.path.join('\/samples', submission.sha256))\n    y = ml.predict_sample(s)\n    submission.status = 'Done'\n    submission.classification = y\n    app.logger.info('Finished processing sample: {0} as: {1}'.format(s.sha256, y))\nexcept Exception as exc:\n    submission.status = 'Error'\n    app.logger.exception('Error processing sample: {0} - Exception: {1}'.format(s.sha256, exc))\ndb.session.add(submission)\ndb.session.commit()\n"}
{"label_name":"process","label":2,"method_name":"gen_preprocess_options","method":"\ninclude_dirs = quote_args(include_dirs)\nreturn _distutils_gen_preprocess_options(macros, include_dirs)\n"}
{"label_name":"process","label":2,"method_name":"process_file","method":"\nlines = resolve_includes(source)\nreturn process_str(''.join(lines))\n"}
{"label_name":"train","label":0,"method_name":"plot_train_loss","method":"\nlosses = [((10 * n), float(i)) for (n, i) in parse_log(args.log_file, 'Train loss: (.*)')]\nplt.clf()\n(it, losses) = zip(*losses)\nplt.plot(it, losses)\nplt.title('Train Loss')\nplt.xlabel('Iteration')\nif (args.y_max != 0):\n    plt.ylim(ymax=args.y_max)\nif (args.y_min != 0):\n    plt.ylim(ymin=args.y_min)\nplt.grid()\nplt.savefig(os.path.join(args.img_dir, 'train_loss.png'))\nif (not args.no_show):\n    plt.show()\n"}
{"label_name":"predict","label":4,"method_name":"evaluate_prediction","method":"\n\ndef _report(y_p, y_t, pad=''):\n    with catch_warnings_ignore(Warning):\n        z_ = np.concatenate(y_p, axis=0)\n        z = np.concatenate(y_t, axis=0)\n        print(pad, ('*** %s ***' % ctext('Frame-level', 'lightcyan')))\n        print(pad, '#Samples:', ctext(len(z), 'cyan'))\n        print(pad, 'Log loss:', log_loss(y_true=z, y_pred=z_, labels=labels))\n        print(pad, 'Accuracy:', accuracy_score(y_true=z, y_pred=np.argmax(z_, axis=(- 1))))\n        z_ = np.concatenate([np.mean(i, axis=0, keepdims=True) for i in y_p], axis=0)\n        z = np.array([i[0] for i in y_t])\n        print(pad, ('*** %s ***' % ctext('Utterance-level', 'lightcyan')))\n        print(pad, '#Samples:', ctext(len(z), 'cyan'))\n        print(pad, 'Log loss:', log_loss(y_true=z, y_pred=z_, labels=labels))\n        print(pad, 'Accuracy:', accuracy_score(y_true=z, y_pred=np.argmax(z_, axis=(- 1))))\ndatasets_2_samples = defaultdict(list)\nfor (name, y_p, y_t) in zip(name_list, y_pred, y_true):\n    dsname = ds['dsname'][name]\n    datasets_2_samples[dsname].append((name, y_p, y_t))\nprint(('=' * 12), ctext(title, 'lightyellow'), ('=' * 12))\n_report(y_p=y_pred, y_t=y_true)\nfor (dsname, data) in sorted(datasets_2_samples.items(), key=(lambda x: x[0])):\n    print(ctext(dsname, 'yellow'), ':')\n    y_pred = [i[1] for i in data]\n    y_true = [i[2] for i in data]\n    _report(y_p=y_pred, y_t=y_true, pad='  ')\n"}
{"label_name":"forward","label":3,"method_name":"recurrent_forward_tanh","method":"\noutdim = W.shape[(- 1)]\n(time, batch, indim) = X.shape\nO = np.zeros((time, batch, outdim))\nZ = np.zeros((time, batch, (indim + outdim)))\nfor t in range(time):\n    Z[t] = np.concatenate((X[t], O[(t - 1)]), axis=(- 1))\n    preact = (np.dot(Z[t], W) + b)\n    O[t] = tanh(preact.ravel()).reshape(*preact.shape)\nreturn np.concatenate((O.ravel(), Z.ravel()))\n"}
{"label_name":"save","label":1,"method_name":"load_or_calc_and_save","method":"\n\ndef my_decorator(func):\n\n    def wrapped(n_graphs, n_params, n_jobs):\n        if os.path.exists(filename):\n            print(f'{func.__name__}: cache file {filename} found! Skip calculations')\n            if (not ignore_if_exist):\n                with open(filename, 'rb') as f:\n                    result = pickle.load(f)\n            else:\n                result = None\n        else:\n            print(f'{func.__name__}: RECALC {filename}. n_graphs={n_graphs}, n_params={n_params}, n_jobs={n_jobs}')\n            result = func(n_graphs=n_graphs, n_params=n_params, n_jobs=n_jobs)\n            with open(filename, 'wb') as f:\n                pickle.dump(result, f)\n        return result\n    return wrapped\nreturn my_decorator\n"}
{"label_name":"train","label":0,"method_name":"train_one_batch","method":"\ncompression = (hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none)\nwith tf.GradientTape() as tape:\n    probs = model(data, training=True)\n    loss = tf.losses.sparse_categorical_crossentropy(target, probs)\ntape = hvd.DistributedGradientTape(tape, compression=compression)\ngradients = tape.gradient(loss, model.trainable_variables)\nopt.apply_gradients(zip(gradients, model.trainable_variables))\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\nglobal NC_PATH_MODELS\nsave_path = (NC_PATH_MODELS + '{}.h5'.format(name.lower()))\nmodel.save(save_path)\nsave_model = (NC_PATH_MODELS + '{}.model.json'.format(name.lower()))\nwith open(save_model, 'w') as json_file:\n    json_file.write(model.to_json())\nsave_weights = (NC_PATH_MODELS + '{}.weights.h5'.format(name.lower()))\nmodel.save_weights(save_weights)\n"}
{"label_name":"process","label":2,"method_name":"process_waveform","method":"\naudio_matrix_ref = np.zeros(1024)\naudio_matrix = np.array(audio_data, dtype=np.int16)\naudio_matrix_interp = interp.interp1d(np.arange(audio_matrix.size), audio_matrix)\naudio_matrix_interp_ref = audio_matrix_interp(np.linspace(0, (audio_matrix.size - 1), audio_matrix_ref.size))\nprint(str(len(audio_matrix_interp_ref)))\nfft_stores = []\nfreq_stores = []\nmax_fft_amplitude = 0\nfor x in range(32):\n    sub = audio_matrix_interp_ref[(x * 32):((x * 32) + 32)]\n    fft = np.abs(np.fft.fft((sub.flatten() * np.blackman(32))))\n    freq = np.fft.fftfreq(32, (1.0 \/ 32.0))\n    fft = fft[4:int((len(fft) \/ 2))]\n    freq = freq[4:int((len(freq) \/ 2))]\n    fft_stores.append(fft)\n    freq_stores.append(freq)\n    fft_rev = fft[::(- 1)]\n    i_max = np.argmax(fft_rev)\n    if (fft_rev[i_max] > max_fft_amplitude):\n        max_fft_amplitude = fft_rev[i_max]\nperc_stores = np.zeros((32, 32))\nfor x in range(32):\n    for y in range(12):\n        perc_stores[x][y] = (fft_stores[x][y] \/ max_fft_amplitude)\nreturn perc_stores\n"}
{"label_name":"process","label":2,"method_name":"process_image","method":"\nnewargs = AttributeDict(args)\nmain(newargs)\nprint('*** Retorno do main: ', classification_status)\nreturn classification_status\n"}
{"label_name":"process","label":2,"method_name":"preprocess_x_sorted_bins","method":"\n'\\n        Pre-process results of l1 model prediction to be used as an input to l2 model.\\n\\n        This method is used for MLP model.\\n\\n        Per class predictions are sorted and each bin of 4 values is averaged.\\n\\n        :param data: np.ndarray, predictions of the single l1 model with the shape of (NB_CLIPS, NB_FRAMES, NB_CLASSES)\\n        :return: pre-processed X tensor with shape (NB_CLIPS, X_WIDTH)\\n    '\nrows = []\nfor row in data:\n    items = []\n    for col in range(row.shape[1]):\n        sorted = np.sort(row[:, col])\n        items.append(sorted.reshape((- 1), config.L2_SORTED_BINS_DOWNSAMPLE).mean(axis=1))\n    rows.append(np.hstack(items))\nreturn np.array(rows)\n"}
{"label_name":"save","label":1,"method_name":"save_network_to_path","method":"\n'Save dataframe to a tab-separated file at path.'\nreturn interactions.to_csv(path, sep='\\t', index=False, na_rep=str(None))\n"}
{"label_name":"train","label":0,"method_name":"load_training_data","method":"\n'\\n    Load all the training-data for the CIFAR-10 data-set.\\n\\n    The data-set is split into 5 data-files which are merged here.\\n\\n    Returns the images, class-numbers and one-hot encoded class-labels.\\n    '\nimages = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\ncls = np.zeros(shape=[_num_images_train], dtype=int)\nbegin = 0\nfor i in range(_num_files_train):\n    (images_batch, cls_batch) = _load_data(filename=('data_batch_' + str((i + 1))))\n    num_images = len(images_batch)\n    end = (begin + num_images)\n    images[begin:end, :] = images_batch\n    cls[begin:end] = cls_batch\n    begin = end\nreturn (images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes))\n"}
{"label_name":"save","label":1,"method_name":"save_marginals","method":"\n'Save marginal probabilities for a set of Candidates to db.\\n\\n    :param X: A list of arbitrary objects with candidate ids accessible via a\\n        .id attrib\\n    :param marginals: A dense M x K matrix of marginal probabilities, where\\n        K is the cardinality of the candidates, OR a M-dim list\/array if K=2.\\n    :param training: If True, these are training marginals \/ labels; else they\\n        are saved as end model predictions.\\n\\n    Note: The marginals for k=0 are not stored, only for k = 1,...,K\\n    '\nlogger = logging.getLogger(__name__)\ntry:\n    shape = marginals.shape\nexcept Exception:\n    marginals = np.array(marginals)\n    shape = marginals.shape\nif (len(shape) == 1):\n    marginals = np.vstack([(1 - marginals), marginals]).T\nmarginal_tuples = []\nfor i in range(shape[0]):\n    for k in range(1, (shape[1] if (len(shape) > 1) else 2)):\n        if (marginals[(i, k)] > 0):\n            marginal_tuples.append((i, k, marginals[(i, k)]))\nsession.query(Marginal).filter((Marginal.training == training)).delete(synchronize_session='fetch')\nq = Marginal.__table__.insert()\ninsert_vals = []\nfor (i, k, p) in marginal_tuples:\n    cid = X[i].id\n    insert_vals.append({'candidate_id': cid, 'training': training, 'value': k, 'probability': float(p)})\nsession.execute(q, insert_vals)\nsession.commit()\nlogger.info(f'Saved {len(marginals)} marginals')\n"}
{"label_name":"process","label":2,"method_name":"process","method":"\noutput = ''\nfor i in range(len(queryPack)):\n    output += '{}\\t'.format(queryPack[i])\n    for j in range(len(response[i])):\n        output += '{} '.format(response[i][j])\n    output += '\\n'\nreturn output\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n'Instantiates and trains the model.'\nif args.check_numerics:\n    tf.debugging.enable_check_numerics()\nmodel = BMSHJ2018Model(args.lmbda, args.num_filters, args.num_scales, args.scale_min, args.scale_max)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\nif args.train_glob:\n    train_dataset = get_custom_dataset('train', args)\n    validation_dataset = get_custom_dataset('validation', args)\nelse:\n    train_dataset = get_dataset('clic', 'train', args)\n    validation_dataset = get_dataset('clic', 'validation', args)\nvalidation_dataset = validation_dataset.take(args.max_validation_steps)\nmodel.fit(train_dataset.prefetch(8), epochs=args.epochs, steps_per_epoch=args.steps_per_epoch, validation_data=validation_dataset.cache(), validation_freq=1, callbacks=[tf.keras.callbacks.TerminateOnNaN(), tf.keras.callbacks.TensorBoard(log_dir=args.train_path, histogram_freq=1, update_freq='epoch'), tf.keras.callbacks.experimental.BackupAndRestore(args.train_path)], verbose=int(args.verbose))\nmodel.save(args.model_path)\n"}
{"label_name":"forward","label":3,"method_name":"point_wise_feed_forward_network","method":"\nreturn tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)])\n"}
{"label_name":"predict","label":4,"method_name":"do_predict","method":"\nimg = cv2.imread(input_file, cv2.IMREAD_COLOR)\nresults = predict_image(img, pred_func)\nif cfg.MODE_MASK:\n    final = draw_final_outputs_blackwhite(img, results)\nelse:\n    final = draw_final_outputs(img, results)\nviz = np.concatenate((img, final), axis=1)\ncv2.imwrite('output.png', viz)\nlogger.info('Inference output for {} written to output.png'.format(input_file))\ntpviz.interactive_imshow(viz)\n"}
{"label_name":"train","label":0,"method_name":"remove_trainer_send_op","method":"\npersistables = block_var_detaile[heter_block_index]['persistables']\nneed_remove_send_op = []\nneed_remove_grad_var = []\nfor op in find_send_op(program):\n    (input_list, _) = find_op_input_output(program, program.global_block(), op)\n    for var_name in input_list:\n        origin_var_name = var_name.split('@GRAD')[0]\n        if (origin_var_name in persistables):\n            need_remove_send_op.append(op)\n            need_remove_grad_var.append(var_name)\nneed_remove_send_op = list(set(need_remove_send_op))\ndelete_ops(program.global_block(), need_remove_send_op)\nfor grad_var_name in need_remove_grad_var:\n    config.remove_var_pair_by_grad(grad_var_name)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\ndom = Domain(args.domain_file, n_jobs=args.parallel, no_constraints=args.no_constraints, timeout=1)\neq_data = load_equations()\n(X, Y) = (eq_data.data, eq_data.target)\n(X, Y) = (X[:args.n_samples], Y[:args.n_samples])\n(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.2, random_state=42)\nprint('Splitting dataset: {} training examples, {} test examples'.format(X_train.shape[0], X_test.shape[0]))\nsp = SSG(dom, inference='map', n_jobs=args.parallel)\nbs = (2 * args.parallel)\nlosses = []\ntimes = []\nfor (i, (X_b, Y_b)) in enumerate(batches(X_train, Y_train, batch_size=bs)):\n    t0 = time()\n    Y_pred = sp.predict(X_b)\n    infer_time = (time() - t0)\n    losses.append(loss(Y_pred, Y_b, n_jobs=args.parallel).mean())\n    avg_loss = (sum(losses) \/ len(losses))\n    t0 = time()\n    sp.partial_fit(X_b, Y_b)\n    learn_time = (time() - t0)\n    times.append((infer_time, learn_time))\n    print('Batch {}'.format((i + 1)))\n    print('Examples: {}'.format(((i * bs) + X_b.shape[0])))\n    print('Training loss = {}'.format(losses[(- 1)]))\n    print('Average training loss = {}'.format(avg_loss))\n    print('Infer time = {}'.format(infer_time))\n    print('Learn time = {}\\n'.format(learn_time))\nprint('Training complete!')\nprint('Inference on the test set...')\nt0 = time()\nY_pred = sp.predict(X_test)\ninfer_time = (time() - t0)\ntest_losses = loss(Y_pred, Y_test, n_jobs=args.parallel)\nprint('Test loss = {}'.format(test_losses.mean()))\nprint('Infer time = {}\\n'.format(infer_time))\nsave({'train-losses': losses, 'test-losses': test_losses, 'times': times}, args.output)\n"}
{"label_name":"save","label":1,"method_name":"save_smi","method":"\nif (not os.path.exists('epoch_data')):\n    os.makedirs('epoch_data')\nsmi_file = os.path.join('epoch_data', '{}.smi'.format(name))\nwith open(smi_file, 'w') as afile:\n    afile.write('\\n'.join(smiles))\nreturn\n"}
{"label_name":"save","label":1,"method_name":"_save","method":"\nresolution = im.encoderinfo.get('resolution', 72.0)\nis_appending = im.encoderinfo.get('append', False)\ntitle = im.encoderinfo.get('title', None)\nauthor = im.encoderinfo.get('author', None)\nsubject = im.encoderinfo.get('subject', None)\nkeywords = im.encoderinfo.get('keywords', None)\ncreator = im.encoderinfo.get('creator', None)\nproducer = im.encoderinfo.get('producer', None)\nif is_appending:\n    existing_pdf = PdfParser.PdfParser(f=fp, filename=filename, mode='r+b')\nelse:\n    existing_pdf = PdfParser.PdfParser(f=fp, filename=filename, mode='w+b')\nif title:\n    existing_pdf.info.Title = title\nif author:\n    existing_pdf.info.Author = author\nif subject:\n    existing_pdf.info.Subject = subject\nif keywords:\n    existing_pdf.info.Keywords = keywords\nif creator:\n    existing_pdf.info.Creator = creator\nif producer:\n    existing_pdf.info.Producer = producer\nim.load()\nexisting_pdf.start_writing()\nexisting_pdf.write_header()\nexisting_pdf.write_comment(('created by PIL PDF driver ' + __version__))\nims = [im]\nif save_all:\n    append_images = im.encoderinfo.get('append_images', [])\n    for append_im in append_images:\n        append_im.encoderinfo = im.encoderinfo.copy()\n        ims.append(append_im)\nnumberOfPages = 0\nimage_refs = []\npage_refs = []\ncontents_refs = []\nfor im in ims:\n    im_numberOfPages = 1\n    if save_all:\n        try:\n            im_numberOfPages = im.n_frames\n        except AttributeError:\n            pass\n    numberOfPages += im_numberOfPages\n    for i in range(im_numberOfPages):\n        image_refs.append(existing_pdf.next_object_id(0))\n        page_refs.append(existing_pdf.next_object_id(0))\n        contents_refs.append(existing_pdf.next_object_id(0))\n        existing_pdf.pages.append(page_refs[(- 1)])\nexisting_pdf.write_catalog()\npageNumber = 0\nfor imSequence in ims:\n    for im in ImageSequence.Iterator(imSequence):\n        bits = 8\n        params = None\n        if (im.mode == '1'):\n            filter = 'ASCIIHexDecode'\n            colorspace = PdfParser.PdfName('DeviceGray')\n            procset = 'ImageB'\n            bits = 1\n        elif (im.mode == 'L'):\n            filter = 'DCTDecode'\n            colorspace = PdfParser.PdfName('DeviceGray')\n            procset = 'ImageB'\n        elif (im.mode == 'P'):\n            filter = 'ASCIIHexDecode'\n            palette = im.im.getpalette('RGB')\n            colorspace = [PdfParser.PdfName('Indexed'), PdfParser.PdfName('DeviceRGB'), 255, PdfParser.PdfBinary(palette)]\n            procset = 'ImageI'\n        elif (im.mode == 'RGB'):\n            filter = 'DCTDecode'\n            colorspace = PdfParser.PdfName('DeviceRGB')\n            procset = 'ImageC'\n        elif (im.mode == 'CMYK'):\n            filter = 'DCTDecode'\n            colorspace = PdfParser.PdfName('DeviceCMYK')\n            procset = 'ImageC'\n        else:\n            raise ValueError(('cannot save mode %s' % im.mode))\n        op = io.BytesIO()\n        if (filter == 'ASCIIHexDecode'):\n            if (bits == 1):\n                data = im.tobytes('raw', '1')\n                im = Image.new('L', (len(data), 1), None)\n                im.putdata(data)\n            ImageFile._save(im, op, [('hex', ((0, 0) + im.size), 0, im.mode)])\n        elif (filter == 'DCTDecode'):\n            Image.SAVE['JPEG'](im, op, filename)\n        elif (filter == 'FlateDecode'):\n            ImageFile._save(im, op, [('zip', ((0, 0) + im.size), 0, im.mode)])\n        elif (filter == 'RunLengthDecode'):\n            ImageFile._save(im, op, [('packbits', ((0, 0) + im.size), 0, im.mode)])\n        else:\n            raise ValueError(('unsupported PDF filter (%s)' % filter))\n        (width, height) = im.size\n        existing_pdf.write_obj(image_refs[pageNumber], stream=op.getvalue(), Type=PdfParser.PdfName('XObject'), Subtype=PdfParser.PdfName('Image'), Width=width, Height=height, Filter=PdfParser.PdfName(filter), BitsPerComponent=bits, DecodeParams=params, ColorSpace=colorspace)\n        existing_pdf.write_page(page_refs[pageNumber], Resources=PdfParser.PdfDict(ProcSet=[PdfParser.PdfName('PDF'), PdfParser.PdfName(procset)], XObject=PdfParser.PdfDict(image=image_refs[pageNumber])), MediaBox=[0, 0, int(((width * 72.0) \/ resolution)), int(((height * 72.0) \/ resolution))], Contents=contents_refs[pageNumber])\n        page_contents = PdfParser.make_bytes(('q %d 0 0 %d 0 0 cm \/image Do Q\\n' % (int(((width * 72.0) \/ resolution)), int(((height * 72.0) \/ resolution)))))\n        existing_pdf.write_obj(contents_refs[pageNumber], stream=page_contents)\n        pageNumber += 1\nexisting_pdf.write_xref_and_trailer()\nif hasattr(fp, 'flush'):\n    fp.flush()\nexisting_pdf.close()\n"}
{"label_name":"process","label":2,"method_name":"process_fermi_hubbard_onsite_sum","method":"\nfrom qmla.process_string_to_matrix import process_basic_operator\nsites = []\nfor c in constituents:\n    if (c[0] == 'd'):\n        num_sites = int(c[1:])\n    else:\n        sites.append(int(c))\nmtx = None\nfor s in sites:\n    onsite_term = 'FHonsite_{s}_d{N}'.format(s=s, N=num_sites)\n    if (mtx is None):\n        mtx = process_basic_operator(onsite_term)\n    else:\n        mtx += process_basic_operator(onsite_term)\nreturn mtx\n"}
{"label_name":"predict","label":4,"method_name":"_predictor","method":"\n'Make a feature random noise.\\n\\n    Parameters\\n    ----------\\n    args : list\\n        A list of arguments:\\n\\n        index : int\\n            The index of the feature to be shuffled.\\n        train_features : array\\n            The original training data matrix.\\n        test_features : array\\n            The original test data matrix.\\n\\n    Returns\\n    -------\\n    f : int\\n        Feature index.\\n    result : float or list\\n        Importance and optionally metadata as following elements in a list.\\n    '\nf = args[0]\ntrain_features = args[1]\ntest_features = args[2]\ntest_targets = args[4]\npredict = args[5]\ntransform = args[6]\ntest_predict = args[7]\n(train, test) = transform((f, train_features, test_features))\nresult = test_predict(predict, test, test_targets)\nif isinstance(result, list):\n    error = result[0]\n    meta = result[1:]\n    return (f, error, meta)\nreturn (f, result)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nfor (i, _) in enumerate(x):\n    print(('i=%d' % i))\n    x_ = x[[i]]\n    y_ = y[[i]]\n    lengths_ = lengths[i]\n    num_blocks = (((lengths_ - 1) \/\/ backprop_steps) + 1)\n    for j in range(num_blocks):\n        start_idx = (j * backprop_steps)\n        stop_idx = ((j + 1) * backprop_steps)\n        model.train(x_[:, start_idx:stop_idx], y_, ([backprop_steps] if (lengths_ > backprop_steps) else [lengths_]), num_epochs=iters_per_example, start_stop_info=False, progress_info=False)\n        if (lengths_ > backprop_steps):\n            lengths_ -= backprop_steps\n        else:\n            break\n"}
{"label_name":"process","label":2,"method_name":"vgg_preprocessing_numpy","method":"\n'Performs VGG-style preprocessing of an image.\\n\\n    The image is resized (aspect-preserving) to the desired size and then\\n    centered by `IMG_MEAN_IMAGENET`.\\n\\n    Args:\\n        img: an image\\n        height: desired height after preprocessing\\n        width: desired width after preprocessing\\n\\n    Returns:\\n        the preprocessed images, in float32 format\\n    '\nif etai.is_gray(img):\n    img = etai.gray_to_rgb(img)\nelif etai.has_alpha(img):\n    img = img[:, :, :3]\nif (img.shape[0] < img.shape[1]):\n    img = etai.resize(img, height=256)\nelse:\n    img = etai.resize(img, width=256)\nimg = etai.central_crop(img, shape=(height, width))\nimg = (np.asarray(img, dtype=np.float32) - IMG_MEAN_IMAGENET)\nreturn img\n"}
{"label_name":"process","label":2,"method_name":"process_single_run","method":"\nexp_dir = os.listdir(in_dir)\nassert (('params.json' in exp_dir) and ('progress.csv' in exp_dir)), 'params.json or progress.csv not found in {}!'.format(in_dir)\nos.makedirs(out_dir, exist_ok=True)\nfor file in exp_dir:\n    absfile = os.path.join(in_dir, file)\n    if (file == 'params.json'):\n        assert os.path.isfile(absfile), '{} not a file!'.format(file)\n        with open(absfile) as fp:\n            contents = json.load(fp)\n        with open(os.path.join(out_dir, 'config.yaml'), 'w') as fp:\n            yaml.dump(contents, fp)\n    elif (file == 'progress.csv'):\n        assert os.path.isfile(absfile), '{} not a file!'.format(file)\n        col_idx_to_filter = []\n        with open(absfile) as fp:\n            col_names_orig = fp.readline().strip().split(',')\n            cols_to_filter = args.results_filter.split(',')\n            for (i, c) in enumerate(col_names_orig):\n                if (c in cols_to_filter):\n                    col_idx_to_filter.insert(0, i)\n            col_names = col_names_orig.copy()\n            for idx in col_idx_to_filter:\n                col_names.pop(idx)\n            absfile_out = os.path.join(out_dir, 'progress.csv')\n            with open(absfile_out, 'w') as out_fp:\n                print(','.join(col_names), file=out_fp)\n                while True:\n                    line = fp.readline().strip()\n                    if (not line):\n                        break\n                    line = re.sub('(,{2,})', (lambda m: ((',None' * (len(m.group()) - 1)) + ',')), line)\n                    cols = re.findall('\".+?\"|[^,]+', line)\n                    if (len(cols) != len(col_names_orig)):\n                        continue\n                    for idx in col_idx_to_filter:\n                        cols.pop(idx)\n                    print(','.join(cols), file=out_fp)\n        out_size = os.path.getsize(absfile_out)\n        max_size = (args.results_max_size * 1024)\n        if (0 < max_size < out_size):\n            ratio = (out_size \/ max_size)\n            if (ratio > 2.0):\n                nth = (out_size \/\/ max_size)\n                os.system(\"awk 'NR==1||NR%{}==0' {} > {}.new\".format(nth, absfile_out, absfile_out))\n            else:\n                nth = (out_size \/\/ (out_size - max_size))\n                os.system(\"awk 'NR==1||NR%{}!=0' {} > {}.new\".format(nth, absfile_out, absfile_out))\n            os.remove(absfile_out)\n            os.rename((absfile_out + '.new'), absfile_out)\n        zip_file = os.path.join(out_dir, 'results.zip')\n        try:\n            os.remove(zip_file)\n        except FileNotFoundError:\n            pass\n        os.system('zip -j {} {}'.format(zip_file, os.path.join(out_dir, 'progress.csv')))\n        os.remove(os.path.join(out_dir, 'progress.csv'))\n    elif re.search('^(events\\\\.out\\\\.|params\\\\.pkl)', file):\n        assert os.path.isfile(absfile), '{} not a file!'.format(file)\n        shutil.copyfile(absfile, os.path.join(out_dir, file))\n"}
{"label_name":"save","label":1,"method_name":"save_fig","method":"\nif tight_layout:\n    plt.tight_layout()\nplt.savefig((fig_id + '.png'), format='png', dpi=300)\n"}
{"label_name":"process","label":2,"method_name":"ResolvePreprocessedId","method":"\n'Compute the hash of a corpus of preprocessed contentfiles.\\n\\n  The hash is computed from the ID of the input files and the serialized\\n  representation of the preprocessor pipeline.\\n  '\nif config.pre_encoded_corpus_url:\n    return 'null'\nreturn crypto.sha1_list(content_id, *config.preprocessor)\n"}
{"label_name":"predict","label":4,"method_name":"predictor","method":"\n'Returns the prediction of labels for the test set.\\n\\n    Args:\\n        model (Keras model instance): A trained Keras model instance.\\n        config (Bunch object): The JSON configuration Bunch object.\\n        species_json (str): Path to a json file stating a species name and key\\n            for returning the name after Softmax, instead of the key. Defaults\\n            to None.\\n        test_dir (str): Directory that holds test images. Optional, if not\\n            set, the default test directory (created by\\n            `split_in_directory()`) will be used. Defaults to None.\\n\\n        Not implemented yet:\\n            plot (Bool): whether to plot images or not. Defaults to None.\\n            n_img: The maximum number of images to plot. Defaults to None.\\n            n_cols (int): divide the maximum number of images in a number of\\n                columns. Rows will be calculated directly. Defaults to None.\\n\\n    Returns:\\n        Y_true (list): True labels\\n        Y_pred (list): Predicted labels\\n        labels (list): Species names\\n        class_indices (dict): Integer: species\\n\\n    '\nif (plot == False):\n    (n_img, n_cols) = (None, None)\ndataset = config.data_set\nshottype = config.shottype\nif (test_dir is None):\n    (test_data_gen_dir, classes, class_indices) = _generator_dir(config=config, target_gen='test', data_dir=None)\nif (test_dir is not None):\n    print(\"Predicting directory: '{}'.\".format(test_dir))\n    (test_data_gen_dir, classes, class_indices) = _generator_dir(config=config, target_gen='test', data_dir=test_dir)\nnb_samples = test_data_gen_dir.samples\nif (species_json is not None):\n    with open(species_json) as sjson:\n        class_indices = json.load(sjson)\nlabels = class_indices.keys()\nY_true = classes\nY_pred = model.predict_generator(test_data_gen_dir, steps=nb_samples, verbose=0)\nY_true = [i for i in Y_true]\nY_pred = [i.argmax() for i in Y_pred]\nreturn (Y_true, Y_pred, labels, class_indices)\n"}
{"label_name":"train","label":0,"method_name":"_validate_trainable_layers","method":"\nif (not pretrained):\n    if (trainable_backbone_layers is not None):\n        warnings.warn('Changing trainable_backbone_layers has not effect if neither pretrained nor pretrained_backbone have been set to True, falling back to trainable_backbone_layers={} so that all layers are trainable'.format(max_value))\n    trainable_backbone_layers = max_value\nif (trainable_backbone_layers is None):\n    trainable_backbone_layers = default_value\nassert (0 <= trainable_backbone_layers <= max_value)\nreturn trainable_backbone_layers\n"}
{"label_name":"process","label":2,"method_name":"process","method":"\n(trn_text, trn_labels) = read(trn_fname)\nfex = BoWFeatures(encoding=encoding, min_df=min_df, dtype=dtype)\nfex.fit(trn_text)\ntrn_features = fex.transform(trn_text)\ndel trn_text\n(tst_text, tst_labels) = read(tst_fname)\ntst_features = fex.transform(tst_text)\ndel tst_text\nmax_ind = max_feature_index(trn_fname, tst_labels)\ntrn_labels = ll_to_sparse(trn_labels, shape=(len(trn_labels), max_feature_index))\ntst_labels = ll_to_sparse(tst_labels, shape=(len(tst_labels), max_feature_index))\nreturn (trn_features, trn_labels, tst_features, tst_labels)\n"}
{"label_name":"train","label":0,"method_name":"train_lightgbm","method":"\nddata = lgb.Dataset(X.values, y.values, free_raw_data=False)\nwith Timer() as t:\n    clf = lgb.train(parameters, ddata, num_boost_round=num_rounds)\nreturn (clf, t.interval)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nheaders = {'Content-Type': 'application\/json', 'Ocp-Apim-Subscription-Key': 'dc5a9336dc024bc69d4a4d49b481be47'}\nparams = urllib.parse.urlencode({})\nbody = \"{ 'url': 'http:\/\/www.freepngimg.com\/download\/happy_person\/2-2-happy-person-free-download-png.png' }\"\ntry:\n    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n    conn.request('POST', ('\/emotion\/v1.0\/recognize?%s' % params), body, headers)\n    response = conn.getresponse()\n    data = response.read()\n    my_json = data.decode('utf8')\n    data = json.loads(my_json)\n    maxval = 0\n    emotionfound = ''\n    for emotion in Emotions:\n        if (maxval < data[0]['scores'][emotion]):\n            maxval = data[0]['scores'][emotion]\n            emotionfound = emotion\n    print(emotionfound)\n    conn.close()\nexcept Exception as e:\n    print(e.args)\n"}
{"label_name":"train","label":0,"method_name":"train_generator","method":"\nG.zero_grad()\nnoise = torch.randn(batch_size, 128)\nif use_cuda:\n    noise = noise.cuda()\nnoisev = Variable(noise)\nfake = G(noisev)\ng = D(fake)\ng = g.mean()\ng.backward(mone)\ng_loss = (- g)\noptim_G.step()\nreturn g_loss\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n'\\n    Main function. Trains a model using the configuration parameters.\\n    :param path_trainingset: Path to access the trainingset.\\n    :param path_model: Path indicating where to save the model.\\n    :param config: Dict, containing the configuration parameters of the network.\\n    :param path_model_init: Path to where the model to use for initialization is stored.\\n    :param save_trainable: Boolean. If True, only saves in the model variables that are trainable (evolve from gradient)\\n    :param gpu: String, name of the gpu to use. Prefer use of CUDA_VISIBLE_DEVICES environment variable.\\n    :param debug_mode: Boolean. If activated, saves more information about the distributions of\\n    most trainable variables, and also outputs more information.\\n    :param gpu_per: Float, between 0 and 1. Percentage of GPU to use.\\n    :return: Nothing.\\n    '\npath_trainingset = convert_path(path_trainingset)\npath_model = convert_path(path_model)\nif (not path_model.exists()):\n    path_model.mkdir(parents=True)\nlearning_rate = config['learning_rate']\nbatch_size = config['batch_size']\nepochs = config['epochs']\nimage_size = config['trainingset_patchsize']\nthresh_indices = config['thresholds']\nif ('checkpoint' in config):\n    checkpoint = config['checkpoint']\nelse:\n    checkpoint = None\nif ('checkpoint_period' in config):\n    checkpoint_period = config['checkpoint_period']\nelse:\n    checkpoint_period = 5\npath_training_set = (path_trainingset \/ 'Train')\npath_validation_set = (path_trainingset \/ 'Validation')\nno_train_images = int((len(os.listdir(path_training_set)) \/ 2))\ntrain_ids = [str(i) for i in range(no_train_images)]\nno_valid_images = int((len(os.listdir(path_validation_set)) \/ 2))\nvalid_ids = [str(i) for i in range(no_valid_images)]\nshifting = (config['da-0-shifting-activate'],)\nrescaling = config['da-1-rescaling-activate']\nrotation = config['da-2-random_rotation-activate']\nelastic = config['da-3-elastic-activate']\nflipping = config['da-4-flipping-activate']\ngaussian_blur = False\nif ('da-5-gaussian_blur-activate' in config):\n    gaussian_blur = config['da-5-gaussian_blur-activate']\nelif ('da-5-noise_addition-activate' in config):\n    gaussian_blur = config['da-5-noise_addition-activate']\nreflection_border = config['da-6-reflection_border-activate']\nif reflection_border:\n    border_mode = cv2.BORDER_REFLECT_101\nelse:\n    border_mode = cv2.BORDER_CONSTANT\np_shift = p_rescale = p_rotate = p_elastic = p_flip = p_blur = 0\nif shifting:\n    p_shift = 0.5\nif rotation:\n    p_rotate = 0.5\nif flipping:\n    p_flip = 0.5\nif gaussian_blur:\n    p_blur = 0.5\nif elastic:\n    p_elastic = 0.5\nalpha_max = 9\nsigma = 3\nalpha = random.choice(list(range(1, alpha_max)))\nlow_bound = 5\nhigh_bound = 89\npercentage_max = 0.1\nsize_shift = int((percentage_max * image_size))\nlow_limit = 0\nhigh_limit = (((2 * size_shift) - 1) \/ image_size)\nAUGMENTATIONS_TRAIN = Compose([Flip(p=p_flip), ShiftScaleRotate(shift_limit=(low_limit, high_limit), scale_limit=(0, 0), rotate_limit=(0, 0), border_mode=border_mode, p=p_shift, interpolation=cv2.INTER_NEAREST), ElasticTransform(alpha=alpha, sigma=sigma, p=p_elastic, alpha_affine=alpha, interpolation=cv2.INTER_NEAREST), GaussianBlur(p=p_blur), Rotate(limit=(low_bound, high_bound), border_mode=border_mode, p=p_rotate, interpolation=cv2.INTER_NEAREST)])\nAUGMENTATIONS_TEST = Compose([])\ntrain_generator = DataGen(train_ids, path_training_set, batch_size=batch_size, image_size=image_size, thresh_indices=thresh_indices, augmentations=AUGMENTATIONS_TRAIN)\nvalid_generator = DataGen(valid_ids, path_validation_set, batch_size=batch_size, image_size=image_size, thresh_indices=thresh_indices, augmentations=AUGMENTATIONS_TEST)\nmodel = uconv_net(config, bn_updated_decay=None, verbose=True)\ntensorboard = TensorBoard(log_dir=str(path_model))\nadam = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer=adam, loss=dice_coef_loss, metrics=['accuracy', dice_axon, dice_myelin])\ntrain_steps = (len(train_ids) \/\/ batch_size)\nvalid_steps = (len(valid_ids) \/\/ batch_size)\nfilepath_acc = (str(path_model) + '\/best_acc_model.ckpt')\ncheckpoint_acc = ModelCheckpoint(filepath_acc, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=checkpoint_period)\nfilepath_loss = (str(path_model) + '\/best_loss_model.ckpt')\ncheckpoint_loss = ModelCheckpoint(filepath_loss, monitor='val_loss', verbose=0, save_best_only=True, mode='min', period=checkpoint_period)\nif (checkpoint == 'loss'):\n    model.load_weights(filepath_loss)\nelif (checkpoint == 'accuracy'):\n    model.load_weights(filepath_acc)\nmodel.fit_generator(train_generator, validation_data=valid_generator, steps_per_epoch=train_steps, validation_steps=valid_steps, epochs=epochs, callbacks=[tensorboard, checkpoint_loss, checkpoint_acc])\nmodel.save(str((path_model \/ 'model.hdf5')))\nsaver = tf.train.Saver()\ncustom_objects = {'dice_axon': dice_axon, 'dice_myelin': dice_myelin, 'dice_coef_loss': dice_coef_loss}\nmodel = load_model((str(path_model) + '\/model.hdf5'), custom_objects=custom_objects)\nsess = K.get_session()\nsave_path = saver.save(sess, (str(path_model) + '\/model.ckpt'))\n"}
{"label_name":"process","label":2,"method_name":"_postprocess_conv2d_output","method":"\n'Transpose and cast the output from conv2d if needed.\\n\\n  Arguments:\\n      x: A tensor.\\n      data_format: string, one of \"channels_last\", \"channels_first\".\\n\\n  Returns:\\n      A tensor.\\n  '\nif (data_format == 'channels_first'):\n    x = array_ops.transpose(x, (0, 3, 1, 2))\nif (floatx() == 'float64'):\n    x = math_ops.cast(x, 'float64')\nreturn x\n"}
{"label_name":"predict","label":4,"method_name":"AuGMEnT_task_seq_prediction","method":"\nfrom TASKS.task_seq_prediction import data_construction\ntask = 'sequence_prediction'\ncues_vec = ['A', 'B', 'C', 'X']\ncues_vec_tot = ['A+', 'B+', 'C+', 'X+', 'A-', 'B-', 'C-', 'X-']\npred_vec = ['D', 'B', 'C', 'Y']\nif (params_task is None):\n    N = 20000\n    p_c = 0.5\n    perc_tr = 0.8\nelse:\n    N = (int(params_task[0]) if (params_task[0] != '') else 20000)\n    p_c = (float(params_task[1]) if (params_task[1] != '') else 0.5)\n    perc_tr = (float(params_task[2]) if (params_task[2] != '') else 0.8)\nprint('Dataset construction...')\n(S_tr, O_tr, S_test, O_test, dic_stim, dic_resp) = data_construction(N, p_c, perc_tr, model='0')\nreset_cond = ['A', 'X']\nS = np.shape(S_tr)[1]\nA = np.shape(O_tr)[1]\nR = 3\nM = 4\nlamb = 0.2\nbeta = 0.02\ndiscount = 0.9\nalpha = (1 - (lamb * discount))\neps = 0.025\ng = 4\nrew_system = ['RL', 'PL', 'SRL', 'BRL']\nrew = 'SRL'\nverb = 0\nif (params_bool is None):\n    do_training = True\n    do_test = True\n    do_weight_plots = True\n    do_error_plots = True\nelse:\n    do_training = params_bool[0]\n    do_test = params_bool[1]\n    do_weight_plots = params_bool[2]\n    do_error_plots = params_bool[3]\nreg_vec = []\nmem_vec = []\nfor i in range(R):\n    reg_vec.append(('R' + str((i + 1))))\nfor i in range(M):\n    mem_vec.append(('M' + str((i + 1))))\nmodel = AuGMEnT(S, R, M, A, alpha, beta, discount, eps, g, rew, dic_stim, dic_resp)\nfolder = 'AuGMEnT\/DATA'\nif do_training:\n    print('TRAINING...\\n')\n    (E, conv_iter) = model.training_ON_OFF(S_tr, O_tr, reset_cond, verb)\n    str_err = (((((folder + '\/') + task) + '_error_units') + str(M)) + '.txt')\n    np.savetxt(str_err, E)\n    str_conv = (((((folder + '\/') + task) + '_conv_units') + str(M)) + '.txt')\n    np.savetxt(str_conv, conv_iter)\n    str_V_r = (((((folder + '\/') + task) + '_weight_Vr_units') + str(M)) + '.txt')\n    np.savetxt(str_V_r, model.V_r)\n    str_V_m = (((((folder + '\/') + task) + '_weight_Vm_units') + str(M)) + '.txt')\n    np.savetxt(str_V_m, model.V_m)\n    str_W_r = (((((folder + '\/') + task) + '_weight_Wr_units') + str(M)) + '.txt')\n    np.savetxt(str_W_r, model.W_r)\n    str_W_m = (((((folder + '\/') + task) + '_weight_Wm_units') + str(M)) + '.txt')\n    np.savetxt(str_W_m, model.W_m)\n    str_W_r_back = (((((folder + '\/') + task) + '_weight_Wr_back_units') + str(M)) + '.txt')\n    np.savetxt(str_W_r_back, model.W_r_back)\n    str_W_m_back = (((((folder + '\/') + task) + '_weight_Wm_back_units') + str(M)) + '.txt')\n    np.savetxt(str_W_m_back, model.W_m_back)\n    print('Saved model.')\nelse:\n    str_err = (((((folder + '\/') + task) + '_error_units') + str(M)) + '.txt')\n    E = np.loadtxt(str_err)\n    str_conv = (((((folder + '\/') + task) + '_conv_units') + str(M)) + '.txt')\n    conv_iter = np.loadtxt(str_conv)\n    print(conv_iter)\n    str_V_r = (((((folder + '\/') + task) + '_weight_Vr_units') + str(M)) + '.txt')\n    model.V_r = np.loadtxt(str_V_r)\n    str_V_m = (((((folder + '\/') + task) + '_weight_Vm_units') + str(M)) + '.txt')\n    model.V_m = np.loadtxt(str_V_m)\n    str_W_r = (((((folder + '\/') + task) + '_weight_Wr_units') + str(M)) + '.txt')\n    model.W_r = np.loadtxt(str_W_r)\n    str_W_m = (((((folder + '\/') + task) + '_weight_Wm_units') + str(M)) + '.txt')\n    model.W_m = np.loadtxt(str_W_m)\n    str_W_r_back = (((((folder + '\/') + task) + '_weight_Wr_back_units') + str(M)) + '.txt')\n    model.W_r_back = np.loadtxt(str_W_r_back)\n    str_W_m_back = (((((folder + '\/') + task) + '_weight_Wm_back_units') + str(M)) + '.txt')\n    model.W_m_back = np.loadtxt(str_W_m_back)\n    print('Loaded model from disk.')\nprint('\\n------------------------------------------------------------------------------------------\\t\\t\\t\\t\\t\\n-----------------------------------------------------------------------------------------------------\\n')\nif do_test:\n    print('TEST...\\n')\n    model.test_ON_OFF(S_test, O_test, reset_cond, 0)\nfolder = 'AuGMEnT\/IMAGES'\nfontTitle = 26\nfontTicks = 22\nfontLabel = 22\nif do_weight_plots:\n    figW = plt.figure(figsize=(30, 8))\n    W = model.V_r\n    plt.subplot(1, 3, 1)\n    plt.pcolor(np.flipud(W), edgecolors='k', linewidths=1)\n    plt.set_cmap('Greens')\n    plt.colorbar()\n    tit = 'ASSOCIATION WEIGHTS'\n    plt.title(tit, fontweight='bold', fontsize=fontTitle)\n    plt.yticks(np.linspace(0.5, (S - 0.5), S, endpoint=True), np.flipud(cues_vec), fontsize=fontTicks)\n    plt.xticks(np.linspace(0.5, (R - 0.5), R, endpoint=True), reg_vec, fontsize=fontTicks)\n    plt.subplot(1, 3, 2)\n    X = model.V_m\n    plt.pcolor(np.flipud(X), edgecolors='k', linewidths=1)\n    plt.set_cmap('Greens')\n    plt.colorbar()\n    tit = 'MEMORY WEIGHTS'\n    plt.title(tit, fontweight='bold', fontsize=fontTitle)\n    plt.yticks(np.linspace(0.5, ((2 * S) - 0.5), (2 * S), endpoint=True), np.flipud(cues_vec_tot), fontsize=fontTicks)\n    plt.xticks(np.linspace(0.5, (M - 0.5), M, endpoint=True), mem_vec, fontsize=fontTicks)\n    WW = np.concatenate((model.W_r, model.W_m), axis=0)\n    plt.subplot(1, 3, 3)\n    plt.pcolor(np.flipud(WW), edgecolors='k', linewidths=1)\n    plt.set_cmap('Greens')\n    plt.colorbar()\n    tit = 'OUTPUT WEIGHTS'\n    plt.title(tit, fontweight='bold', fontsize=fontTitle)\n    plt.yticks(np.linspace(0.5, ((M + R) - 0.5), (M + R), endpoint=True), np.flipud(np.concatenate((reg_vec, mem_vec))), fontsize=fontTicks)\n    plt.xticks(np.linspace(0.5, (A - 0.5), A, endpoint=True), pred_vec, fontsize=fontTicks)\n    plt.show()\n    savestr = (((((folder + '\/') + task) + '_weights_') + rew) + '.png')\n    if (M == 0):\n        savestr = (((((folder + '\/') + task) + '_weights_') + rew) + '_nomemory.png')\n    figW.savefig(savestr)\nif do_error_plots:\n    N = len(E)\n    bin = round((N * 0.02))\n    END = np.floor((N \/ bin)).astype(int)\n    E = E[:(END * bin)]\n    N = len(E)\n    E_bin = np.reshape(E, ((- 1), bin))\n    E_bin = np.sum(E_bin, axis=1)\n    E_cum = np.cumsum(E)\n    E_norm = ((100 * E_cum) \/ (np.arange(N) + 1))\n    C = np.where((E == 0), 1, 0)\n    C_cum = ((100 * np.cumsum(C)) \/ (np.arange(N) + 1))\n    figE = plt.figure(figsize=(20, 8))\n    N_round = (np.around((N \/ 1000)).astype(int) * 1000)\n    plt.subplot(1, 2, 1)\n    plt.bar(((bin * np.arange(len(E_bin))) \/ 3), E_bin, width=(bin \/ 3), color='green', edgecolor='black', alpha=0.6)\n    if (conv_iter != 0):\n        plt.axvline(x=(conv_iter \/ 3), linewidth=5, color='g')\n    tit = 'Seq Predict: Training Convergence'\n    plt.title(tit, fontweight='bold', fontsize=fontTitle)\n    plt.xlabel('Training Trials', fontsize=fontLabel)\n    plt.ylabel('Number of Errors per bin', fontsize=fontLabel)\n    plt.xticks(np.linspace(0, (N_round \/ 3), 5, endpoint=True), fontsize=fontTicks)\n    plt.yticks(fontsize=fontTicks)\n    text = ('Bin = ' + str(bin))\n    plt.figtext(x=0.38, y=0.78, s=text, fontsize=fontLabel, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n    plt.subplot(1, 2, 2)\n    plt.plot((np.arange(N) \/ 3), E_cum, color='green', linewidth=7, alpha=0.6)\n    if (conv_iter != 0):\n        plt.axvline(x=(conv_iter \/ 3), linewidth=5, color='g')\n    tit = 'Seq Predict: Cumulative Training Error'\n    plt.title(tit, fontweight='bold', fontsize=fontTitle)\n    plt.xticks(np.linspace(0, (N_round \/ 3), 5, endpoint=True), fontsize=fontTicks)\n    plt.yticks(fontsize=fontTicks)\n    plt.xlabel('Training Trials', fontsize=fontLabel)\n    plt.ylabel('Cumulative Error', fontsize=fontLabel)\n    plt.show()\n    savestr = (((((folder + '\/') + task) + '_error_') + rew) + '.png')\n    if (M == 0):\n        savestr = (((((folder + '\/') + task) + '_error_') + rew) + '_nomemory.png')\n    figE.savefig(savestr)\n"}
{"label_name":"train","label":0,"method_name":"load_ims_train","method":"\nif os.path.isdir(dname):\n    fnames = get_listdir(dname)\n    if (not fnames):\n        raise FileNotFoundError('Files not found in the folder: {}.'.format(dname))\n    ims = []\n    for fname in fnames:\n        im = read_im_any(fname)\n        ims.extend(im)\nelse:\n    raise FileNotFoundError('Folder not found: {}.'.format(dname))\nims = np.asarray(ims, np.float32)\nif (len(ims.shape) == 2):\n    ims = ims.reshape(((- 1), ims.shape[0], ims.shape[1]))\nreturn ims\n"}
{"label_name":"predict","label":4,"method_name":"cross_val_predict","method":"\n\"Generate cross-validated estimates for each input data point\\n\\n    .. deprecated:: 0.18\\n        This module will be removed in 0.20.\\n        Use :func:`sklearn.model_selection.cross_val_predict` instead.\\n\\n    Read more in the :ref:`User Guide <cross_validation>`.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit' and 'predict'\\n        The object to use to fit the data.\\n\\n    X : array-like\\n        The data to fit. Can be, for example a list, or an array at least 2d.\\n\\n    y : array-like, optional, default: None\\n        The target variable to try to predict in the case of\\n        supervised learning.\\n\\n    cv : int, cross-validation generator or an iterable, optional\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 3-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - An object to be used as a cross-validation generator.\\n        - An iterable yielding train\/test splits.\\n\\n        For integer\/None inputs, if the estimator is a classifier and ``y`` is\\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\\n        other cases, :class:`KFold` is used.\\n\\n        Refer :ref:`User Guide <cross_validation>` for the various\\n        cross-validation strategies that can be used here.\\n\\n    n_jobs : integer, optional\\n        The number of CPUs to use to do the computation. -1 means\\n        'all CPUs'.\\n\\n    verbose : integer, optional\\n        The verbosity level.\\n\\n    fit_params : dict, optional\\n        Parameters to pass to the fit method of the estimator.\\n\\n    pre_dispatch : int, or string, optional\\n        Controls the number of jobs that get dispatched during parallel\\n        execution. Reducing this number can be useful to avoid an\\n        explosion of memory consumption when more jobs get dispatched\\n        than CPUs can process. This parameter can be:\\n\\n            - None, in which case all the jobs are immediately\\n              created and spawned. Use this for lightweight and\\n              fast-running jobs, to avoid delays due to on-demand\\n              spawning of the jobs\\n\\n            - An int, giving the exact number of total jobs that are\\n              spawned\\n\\n            - A string, giving an expression as a function of n_jobs,\\n              as in '2*n_jobs'\\n\\n    Returns\\n    -------\\n    preds : ndarray\\n        This is the result of calling 'predict'\\n\\n    Examples\\n    --------\\n    >>> from sklearn import datasets, linear_model\\n    >>> from sklearn.cross_validation import cross_val_predict\\n    >>> diabetes = datasets.load_diabetes()\\n    >>> X = diabetes.data[:150]\\n    >>> y = diabetes.target[:150]\\n    >>> lasso = linear_model.Lasso()\\n    >>> y_pred = cross_val_predict(lasso, X, y)\\n    \"\n(X, y) = indexable(X, y)\ncv = check_cv(cv, X, y, classifier=is_classifier(estimator))\nparallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\npreds_blocks = parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params) for (train, test) in cv))\npreds = [p for (p, _) in preds_blocks]\nlocs = np.concatenate([loc for (_, loc) in preds_blocks])\nif (not _check_is_partition(locs, _num_samples(X))):\n    raise ValueError('cross_val_predict only works for partitions')\ninv_locs = np.empty(len(locs), dtype=int)\ninv_locs[locs] = np.arange(len(locs))\nif sp.issparse(preds[0]):\n    preds = sp.vstack(preds, format=preds[0].format)\nelse:\n    preds = np.concatenate(preds)\nreturn preds[inv_locs]\n"}
{"label_name":"process","label":2,"method_name":"html_preprocessing","method":"\nfor tag in soup.find_all('script'):\n    tag.clear()\nfor tag in soup.find_all('style'):\n    tag.clear()\nfor tag in soup.find_all('link'):\n    tag.clear()\nfor tag in soup.findAll('meta'):\n    tag.attrs = None\nfor tag in soup.findAll('stript'):\n    tag.attrs = None\nfor tag in soup.findAll('link'):\n    tag.attrs = None\nphrase_tag_list = ['em', 'strong', 'code', 'samp', 'kbd', 'var']\nfor tag in phrase_tag_list:\n    for match in soup.find_all(tag):\n        match.replaceWithChildren()\ntexthighlight_tag_list = ['b', 'mark']\nfor tag in texthighlight_tag_list:\n    for match in soup.find_all(tag):\n        match.replaceWithChildren()\nfor head in soup('head'):\n    soup.head.extract()\nsoup = BeautifulSoup(str(soup).replace('<br>', '(br)'))\nfor tag in soup.findAll('a', href=True):\n    tag.wrap(soup.new_tag('grit'))\n    tag.replaceWithChildren()\nsoup = BeautifulSoup(str(soup).replace('<\/grit>', ' endhyper'))\nsoup = BeautifulSoup(str(soup).replace('<grit>', 'starthyper '))\nreturn soup\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nmodel = Model()\nwith tf.Graph().as_default():\n    (images, val_images, labels, val_labels) = mnist.load_train_data(FLAGS.train_data)\n    x = tf.placeholder(shape=[None, mnist.IMAGE_SIZE, mnist.IMAGE_SIZE, 1], dtype=tf.float32, name='x')\n    y = tf.placeholder(shape=[None, NUM_LABELS], dtype=tf.float32, name='y')\n    keep_prob = tf.placeholder(tf.float32, name='dropout_prob')\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    logits = model.inference(x, keep_prob=keep_prob)\n    loss = model.loss(logits=logits, labels=y)\n    accuracy = model.accuracy(logits, y)\n    summary_op = tf.summary.merge_all()\n    train_op = model.train(loss, global_step=global_step)\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n        writer = tf.summary.FileWriter(FLAGS.summary_dir, sess.graph)\n        sess.run(init)\n        for i in range(FLAGS.num_iter):\n            offset = ((i * FLAGS.batch_size) % (len(images) - FLAGS.batch_size))\n            (batch_x, batch_y) = (images[offset:(offset + FLAGS.batch_size), :], labels[offset:(offset + FLAGS.batch_size), :])\n            (_, cur_loss, summary) = sess.run([train_op, loss, summary_op], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n            writer.add_summary(summary, i)\n            print(i, cur_loss)\n            if ((i % 1000) == 0):\n                validation_accuracy = accuracy.eval(feed_dict={x: val_images, y: val_labels, keep_prob: 1.0})\n                print('Iter {} Accuracy: {}'.format(i, validation_accuracy))\n            if (i == (FLAGS.num_iter - 1)):\n                saver.save(sess, FLAGS.checkpoint_file_path, global_step)\n"}
{"label_name":"predict","label":4,"method_name":"get_prediction_images","method":"\nfiles = [x[2] for x in os.walk(prediction_dir)][0]\nl = []\nexts = ['.jpg', '.jpeg', '.png']\nfor file in files:\n    (_, ext) = os.path.splitext(file)\n    if (ext.lower() in exts):\n        l.append(os.path.join(prediction_dir, file))\nreturn l\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n'\\n    Prediction given the request input\\n    :param json_input: [dict], request input\\n    :return: [dict], prediction\\n    '\nmodel_input = None\nprediction = ModelService.predict(model_input)\nprint(prediction)\nresult = {}\nreturn result\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\nhashes = [[hash_unicode(word) for word in X] for X in Xs]\nhash_arrays = [model.ops.asarray2i(h, dtype='uint64') for h in hashes]\narrays = [model.ops.reshape2i(array, (- 1), 1) for array in hash_arrays]\n\ndef backprop(dX: OutT) -> InT:\n    return []\nreturn (arrays, backprop)\n"}
{"label_name":"predict","label":4,"method_name":"_predict_binary","method":"\n'Make predictions using a single binary estimator.'\nif is_regressor(estimator):\n    return estimator.predict(X)\ntry:\n    score = np.ravel(estimator.decision_function(X))\nexcept (AttributeError, NotImplementedError):\n    score = estimator.predict_proba(X)[:, 1]\nreturn score\n"}
{"label_name":"process","label":2,"method_name":"processing_regions_from_options","method":"\n'Computes the calling regions from our options.\\n\\n  This function does all of the work needed to read our input files and region\\n  specifications to determine the list of regions we should generate examples\\n  over. It also computes the confident regions needed to label variants.\\n\\n  Args:\\n    options: deepvariant.DeepTrioOptions proto containing information about our\\n      input data sources.\\n\\n  Raises:\\n    ValueError: if the regions to call is empty.\\n\\n  Returns:\\n    Two values. The first is a list of nucleus.genomics.v1.Range protos of the\\n    regions we should process. The second is a RangeSet containing the confident\\n    regions for labeling, or None if we are running in training mode.\\n  '\nref_contigs = fasta.IndexedFastaReader(options.reference_filename).header.contigs\nsam_contigs = sam.SamReader(options.reads_filename).header.contigs\nvcf_contigs = None\nif in_training_mode(options):\n    vcf_contigs = vcf.VcfReader(options.truth_variants_filename).header.contigs\ncontigs = _ensure_consistent_contigs(ref_contigs, sam_contigs, vcf_contigs, options.exclude_contigs, options.min_shared_contigs_basepairs)\nlogging.info('Common contigs are %s', [c.name for c in contigs])\ncalling_regions = build_calling_regions(ref_contigs, options.calling_regions, options.exclude_calling_regions)\nif (not calling_regions):\n    raise ValueError('The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use \"chr20\" for a BAM where contig names don\\'t have \"chr\"s (or vice versa).')\nregions = regions_to_process(contigs=contigs, partition_size=options.allele_counter_options.partition_size, calling_regions=calling_regions, task_id=options.task_id, num_shards=options.num_shards)\nreturn regions\n"}
{"label_name":"forward","label":3,"method_name":"recurrent_forward_relu","method":"\noutdim = W.shape[(- 1)]\n(time, batch, indim) = X.shape\nO = np.zeros((time, batch, outdim))\nfor t in range(time):\n    Z = np.concatenate((X[t], O[(t - 1)]), axis=(- 1))\n    preact = (np.dot(Z, W) + b)\n    O[t] = relu(preact.ravel()).reshape(*preact.shape)\nreturn O\n"}
{"label_name":"process","label":2,"method_name":"process_auto_candidates_patients","method":"\nfor subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\n    src_dir = (((settings.LUNA16_RAW_SRC_DIR + 'subset') + str(subject_no)) + '\/')\n    for (patient_index, src_path) in enumerate(glob.glob((src_dir + '*.mhd'))):\n        patient_id = ntpath.basename(src_path).replace('.mhd', '')\n        print('Patient: ', patient_index, ' ', patient_id)\n        process_auto_candidates_patient(src_path, patient_id, sample_count=200, candidate_type='edge')\n"}
{"label_name":"train","label":0,"method_name":"read_train_data","method":"\nfile_path = os.path.normpath(path)\nreader = surprise.Reader(line_format='timestamp user item rating', sep=',')\ndata = surprise.Dataset.load_from_file(file_path, reader=reader)\nreturn data\n"}
{"label_name":"process","label":2,"method_name":"process_inputs","method":"\nmax = 1\nmin = (- 1)\nrange = (max - min)\nlat = round(t['lat'], 7)\nlng = round(t['lng'], 7)\nweekday = t['weekday']\nhour = t['hour']\nminutes = ((t['min'] \/\/ 15) * 15)\nt = t['t']\nreturn [lat, lng, weekday, hour, minutes, t]\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\ndata_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\nargs.vocab_size = data_loader.vocab_size\nwith open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n    pickle.dump(args, f)\nwith open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n    pickle.dump((data_loader.chars, data_loader.vocab), f)\nmodel = Model(args)\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    saver = tf.train.Saver(tf.global_variables())\n    train_loss_iterations = {'iteration': [], 'epoch': [], 'train_loss': [], 'val_loss': []}\n    for e in range(args.num_epochs):\n        sess.run(tf.assign(model.lr, (args.learning_rate * (args.decay_rate ** e))))\n        data_loader.reset_batch_pointer()\n        state = sess.run(model.initial_state)\n        for b in range(data_loader.num_batches):\n            start = time.time()\n            (x, y) = data_loader.next_batch()\n            feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n            (train_loss, state, _) = sess.run([model.cost, model.final_state, model.train_op], feed)\n            end = time.time()\n            batch_idx = ((e * data_loader.num_batches) + b)\n            print('{}\/{} (epoch {}), train_loss = {:.3f}, time\/batch = {:.3f}'.format(batch_idx, (args.num_epochs * data_loader.num_batches), e, train_loss, (end - start)))\n            train_loss_iterations['iteration'].append(batch_idx)\n            train_loss_iterations['epoch'].append(e)\n            train_loss_iterations['train_loss'].append(train_loss)\n            if ((batch_idx % args.save_every) == 0):\n                state_val = sess.run(model.initial_state)\n                avg_val_loss = 0\n                for (x_val, y_val) in data_loader.val_batches:\n                    feed_val = {model.input_data: x_val, model.targets: y_val, model.initial_state: state_val}\n                    (val_loss, state_val, _) = sess.run([model.cost, model.final_state, model.train_op], feed_val)\n                    avg_val_loss += (val_loss \/ len(list(data_loader.val_batches)))\n                print('val_loss: {:.3f}'.format(avg_val_loss))\n                train_loss_iterations['val_loss'].append(avg_val_loss)\n                checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n                saver.save(sess, checkpoint_path, global_step=((e * data_loader.num_batches) + b))\n                print('model saved to {}'.format(checkpoint_path))\n            else:\n                train_loss_iterations['val_loss'].append(None)\n        pd.DataFrame(data=train_loss_iterations, columns=train_loss_iterations.keys()).to_csv(os.path.join(args.save_dir, 'log.csv'))\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\nif isinstance(Xseq, Ragged):\n    return _ragged_forward(cast(Model[(Ragged, Ragged)], model), cast(Ragged, Xseq), is_train)\nelif isinstance(Xseq, Padded):\n    return _padded_forward(cast(Model[(Padded, Padded)], model), cast(Padded, Xseq), is_train)\nelif (not isinstance(Xseq, (list, tuple))):\n    return model.layers[0](Xseq, is_train)\nelse:\n    return _list_forward(cast(Model[(List2d, List2d)], model), Xseq, is_train)\n"}
{"label_name":"process","label":2,"method_name":"postprocess_histogram_mean","method":"\n'\\n    Another strategy. Simply ensures sum(hist) == n. Also not as good.\\n\\n    hist: np.array() of floats\\n    '\nextra = (float((sum(hist) - n)) \/ m)\nhist -= extra\nreturn hist\n"}
{"label_name":"save","label":1,"method_name":"create_save_model","method":"\n\ndef save_model(path):\n    torch.save(model.state_dict(), path)\nreturn save_model\n"}
{"label_name":"process","label":2,"method_name":"complete_wiki_processing","method":"\nprocessed_data = []\nnum_bad_examples = 0\nfor example in data:\n    number_found = 0\n    if example.is_bad_example:\n        num_bad_examples += 1\n    if (not example.is_bad_example):\n        example.string_question = example.question[:]\n        example.processed_number_columns = example.processed_number_columns[:]\n        example.processed_word_columns = example.processed_word_columns[:]\n        (example.word_exact_match, word_match, matched_indices) = exact_match(example.string_question, example.original_wc, number=False)\n        (example.number_exact_match, number_match, _) = exact_match(example.string_question, example.original_nc, number=True)\n        if ((not pick_one(example.word_exact_match)) and (not pick_one(example.number_exact_match))):\n            assert (len(word_match) == 0)\n            assert (len(number_match) == 0)\n            (example.word_exact_match, word_match) = partial_match(example.string_question, example.original_wc, number=False)\n        example.word_group_by_max = group_by_max(example.original_wc, False)\n        example.number_group_by_max = group_by_max(example.original_nc, True)\n        (example.word_column_exact_match, wcol_matched_indices) = exact_column_match(example.string_question, example.original_wc_names, number=False)\n        (example.number_column_exact_match, ncol_matched_indices) = exact_column_match(example.string_question, example.original_nc_names, number=False)\n        if ((not (1.0 in example.word_column_exact_match)) and (not (1.0 in example.number_column_exact_match))):\n            example.word_column_exact_match = partial_column_match(example.string_question, example.original_wc_names, number=False)\n            example.number_column_exact_match = partial_column_match(example.string_question, example.original_nc_names, number=False)\n        if ((len(word_match) > 0) or (len(number_match) > 0)):\n            example.question.append(utility.entry_match_token)\n        if ((1.0 in example.word_column_exact_match) or (1.0 in example.number_column_exact_match)):\n            example.question.append(utility.column_match_token)\n        example.string_question = example.question[:]\n        example.number_lookup_matrix = np.transpose(example.number_lookup_matrix)[:]\n        example.word_lookup_matrix = np.transpose(example.word_lookup_matrix)[:]\n        example.columns = example.number_columns[:]\n        example.word_columns = example.word_columns[:]\n        example.len_total_cols = (len(example.word_column_names) + len(example.number_column_names))\n        example.column_names = example.number_column_names[:]\n        example.word_column_names = example.word_column_names[:]\n        example.string_column_names = example.number_column_names[:]\n        example.string_word_column_names = example.word_column_names[:]\n        example.sorted_number_index = []\n        example.sorted_word_index = []\n        example.column_mask = []\n        example.word_column_mask = []\n        example.processed_column_mask = []\n        example.processed_word_column_mask = []\n        example.word_column_entry_mask = []\n        example.question_attention_mask = []\n        example.question_number = example.question_number_1 = (- 1)\n        example.question_attention_mask = []\n        example.ordinal_question = []\n        example.ordinal_question_one = []\n        new_question = []\n        if (len(example.number_columns) > 0):\n            example.len_col = len(example.number_columns[0])\n        else:\n            example.len_col = len(example.word_columns[0])\n        for (start, length) in matched_indices:\n            for j in range(length):\n                example.question[(start + j)] = utility.unk_token\n        for word in example.question:\n            if (isinstance(word, numbers.Number) or wiki_data.is_date(word)):\n                if ((not isinstance(word, numbers.Number)) and wiki_data.is_date(word)):\n                    word = word.replace('X', '').replace('-', '')\n                number_found += 1\n                if (number_found == 1):\n                    example.question_number = word\n                    if (len(example.ordinal_question) > 0):\n                        example.ordinal_question[(len(example.ordinal_question) - 1)] = 1.0\n                    else:\n                        example.ordinal_question.append(1.0)\n                elif (number_found == 2):\n                    example.question_number_1 = word\n                    if (len(example.ordinal_question_one) > 0):\n                        example.ordinal_question_one[(len(example.ordinal_question_one) - 1)] = 1.0\n                    else:\n                        example.ordinal_question_one.append(1.0)\n            else:\n                new_question.append(word)\n                example.ordinal_question.append(0.0)\n                example.ordinal_question_one.append(0.0)\n        example.question = [utility.word_ids[word_lookup(w, utility)] for w in new_question]\n        example.question_attention_mask = ([0.0] * len(example.question))\n        example.ordinal_question = example.ordinal_question[0:len(example.question)]\n        example.ordinal_question_one = example.ordinal_question_one[0:len(example.question)]\n        example.question = (([utility.word_ids[utility.dummy_token]] * (utility.FLAGS.question_length - len(example.question))) + example.question)\n        example.question_attention_mask = (([(- 10000.0)] * (utility.FLAGS.question_length - len(example.question_attention_mask))) + example.question_attention_mask)\n        example.ordinal_question = (([0.0] * (utility.FLAGS.question_length - len(example.ordinal_question))) + example.ordinal_question)\n        example.ordinal_question_one = (([0.0] * (utility.FLAGS.question_length - len(example.ordinal_question_one))) + example.ordinal_question_one)\n        if True:\n            num_cols = len(example.columns)\n            start = 0\n            for column in example.number_columns:\n                if check_processed_cols(example.processed_number_columns[start], utility):\n                    example.processed_column_mask.append(0.0)\n                sorted_index = sorted(range(len(example.processed_number_columns[start])), key=(lambda k: example.processed_number_columns[start][k]), reverse=True)\n                sorted_index = (sorted_index + ([utility.FLAGS.pad_int] * (utility.FLAGS.max_elements - len(sorted_index))))\n                example.sorted_number_index.append(sorted_index)\n                example.columns[start] = (column + ([utility.FLAGS.pad_int] * (utility.FLAGS.max_elements - len(column))))\n                example.processed_number_columns[start] += ([utility.FLAGS.pad_int] * (utility.FLAGS.max_elements - len(example.processed_number_columns[start])))\n                start += 1\n                example.column_mask.append(0.0)\n            for remaining in range(num_cols, utility.FLAGS.max_number_cols):\n                example.sorted_number_index.append(([utility.FLAGS.pad_int] * utility.FLAGS.max_elements))\n                example.columns.append(([utility.FLAGS.pad_int] * utility.FLAGS.max_elements))\n                example.processed_number_columns.append(([utility.FLAGS.pad_int] * utility.FLAGS.max_elements))\n                example.number_exact_match.append(([0.0] * utility.FLAGS.max_elements))\n                example.number_group_by_max.append(([0.0] * utility.FLAGS.max_elements))\n                example.column_mask.append((- 100000000.0))\n                example.processed_column_mask.append((- 100000000.0))\n                example.number_column_exact_match.append(0.0)\n                example.column_names.append([utility.dummy_token])\n            start = 0\n            word_num_cols = len(example.word_columns)\n            for column in example.word_columns:\n                if check_processed_cols(example.processed_word_columns[start], utility):\n                    example.processed_word_column_mask.append(0.0)\n                sorted_index = sorted(range(len(example.processed_word_columns[start])), key=(lambda k: example.processed_word_columns[start][k]), reverse=True)\n                sorted_index = (sorted_index + ([utility.FLAGS.pad_int] * (utility.FLAGS.max_elements - len(sorted_index))))\n                example.sorted_word_index.append(sorted_index)\n                column = convert_to_int_2d_and_pad(column, utility)\n                example.word_columns[start] = (column + ([([utility.word_ids[utility.dummy_token]] * utility.FLAGS.max_entry_length)] * (utility.FLAGS.max_elements - len(column))))\n                example.processed_word_columns[start] += ([utility.FLAGS.pad_int] * (utility.FLAGS.max_elements - len(example.processed_word_columns[start])))\n                example.word_column_entry_mask.append((([0] * len(column)) + ([utility.word_ids[utility.dummy_token]] * (utility.FLAGS.max_elements - len(column)))))\n                start += 1\n                example.word_column_mask.append(0.0)\n            for remaining in range(word_num_cols, utility.FLAGS.max_word_cols):\n                example.sorted_word_index.append(([utility.FLAGS.pad_int] * utility.FLAGS.max_elements))\n                example.word_columns.append(([([utility.word_ids[utility.dummy_token]] * utility.FLAGS.max_entry_length)] * utility.FLAGS.max_elements))\n                example.word_column_entry_mask.append(([utility.word_ids[utility.dummy_token]] * utility.FLAGS.max_elements))\n                example.word_exact_match.append(([0.0] * utility.FLAGS.max_elements))\n                example.word_group_by_max.append(([0.0] * utility.FLAGS.max_elements))\n                example.processed_word_columns.append(([utility.FLAGS.pad_int] * utility.FLAGS.max_elements))\n                example.word_column_mask.append((- 100000000.0))\n                example.processed_word_column_mask.append((- 100000000.0))\n                example.word_column_exact_match.append(0.0)\n                example.word_column_names.append(([utility.dummy_token] * utility.FLAGS.max_entry_length))\n            seen_tables[example.table_key] = 1\n        example.column_ids = convert_to_int_2d_and_pad(example.column_names, utility)\n        example.word_column_ids = convert_to_int_2d_and_pad(example.word_column_names, utility)\n        for i_em in range(len(example.number_exact_match)):\n            example.number_exact_match[i_em] = (example.number_exact_match[i_em] + ([0.0] * (utility.FLAGS.max_elements - len(example.number_exact_match[i_em]))))\n            example.number_group_by_max[i_em] = (example.number_group_by_max[i_em] + ([0.0] * (utility.FLAGS.max_elements - len(example.number_group_by_max[i_em]))))\n        for i_em in range(len(example.word_exact_match)):\n            example.word_exact_match[i_em] = (example.word_exact_match[i_em] + ([0.0] * (utility.FLAGS.max_elements - len(example.word_exact_match[i_em]))))\n            example.word_group_by_max[i_em] = (example.word_group_by_max[i_em] + ([0.0] * (utility.FLAGS.max_elements - len(example.word_group_by_max[i_em]))))\n        example.exact_match = (example.number_exact_match + example.word_exact_match)\n        example.group_by_max = (example.number_group_by_max + example.word_group_by_max)\n        example.exact_column_match = (example.number_column_exact_match + example.word_column_exact_match)\n        if example.is_lookup:\n            example.answer = example.calc_answer\n            example.number_print_answer = example.number_lookup_matrix.tolist()\n            example.word_print_answer = example.word_lookup_matrix.tolist()\n            for i_answer in range(len(example.number_print_answer)):\n                example.number_print_answer[i_answer] = (example.number_print_answer[i_answer] + ([0.0] * (utility.FLAGS.max_elements - len(example.number_print_answer[i_answer]))))\n            for i_answer in range(len(example.word_print_answer)):\n                example.word_print_answer[i_answer] = (example.word_print_answer[i_answer] + ([0.0] * (utility.FLAGS.max_elements - len(example.word_print_answer[i_answer]))))\n            example.number_lookup_matrix = convert_to_bool_and_pad(example.number_lookup_matrix, utility)\n            example.word_lookup_matrix = convert_to_bool_and_pad(example.word_lookup_matrix, utility)\n            for remaining in range(num_cols, utility.FLAGS.max_number_cols):\n                example.number_lookup_matrix.append(([False] * utility.FLAGS.max_elements))\n                example.number_print_answer.append(([0.0] * utility.FLAGS.max_elements))\n            for remaining in range(word_num_cols, utility.FLAGS.max_word_cols):\n                example.word_lookup_matrix.append(([False] * utility.FLAGS.max_elements))\n                example.word_print_answer.append(([0.0] * utility.FLAGS.max_elements))\n            example.print_answer = (example.number_print_answer + example.word_print_answer)\n        else:\n            example.answer = example.calc_answer\n            example.print_answer = ([([0.0] * utility.FLAGS.max_elements)] * (utility.FLAGS.max_number_cols + utility.FLAGS.max_word_cols))\n        if (example.question_number == (- 1)):\n            example.question_number_mask = np.zeros([utility.FLAGS.max_elements])\n        else:\n            example.question_number_mask = np.ones([utility.FLAGS.max_elements])\n        if (example.question_number_1 == (- 1)):\n            example.question_number_one_mask = (- 10000.0)\n        else:\n            example.question_number_one_mask = np.float64(0.0)\n        if (example.len_col > utility.FLAGS.max_elements):\n            continue\n        processed_data.append(example)\nreturn processed_data\n"}
{"label_name":"process","label":2,"method_name":"_process_caption","method":"\n'Processes a caption string into a list of tonenized words.\\n\\n  Args:\\n    caption: A string caption.\\n\\n  Returns:\\n    A list of strings; the tokenized caption.\\n  '\ntokenized_caption = [FLAGS.start_word]\ntokenized_caption.extend(nltk.tokenize.word_tokenize(caption.lower()))\ntokenized_caption.append(FLAGS.end_word)\nreturn tokenized_caption\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n'Preprocess a single image of layout [height, width, depth].'\nif is_training:\n    image = tf.image.resize_image_with_crop_or_pad(image, (_HEIGHT + 8), (_WIDTH + 8))\n    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _NUM_CHANNELS])\n    image = tf.image.random_flip_left_right(image)\nimage = tf.image.per_image_standardization(image)\nreturn image\n"}
{"label_name":"train","label":0,"method_name":"get_pretraining_model","method":"\n(cfg, tokenizer, _, _) = get_pretrained_bert(model_name, load_backbone=False, load_mlm=False)\ncfg = BertModel.get_cfg().clone_merge(cfg)\nmodel = BertForPretrain(cfg)\nreturn (cfg, tokenizer, model)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, (EPOCH * 0.2), 0.9, staircase=True)\n    tf.summary.scalar('total_loss', total_loss)\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    return optimizer.minimize(total_loss, global_step=global_step)\n"}
{"label_name":"process","label":2,"method_name":"process_ud_treebank","method":"\n'\\n    Process a normal UD treebank with train\/dev\/test splits\\n\\n    SL-SSJ and Vietnamese both use this code path as well.\\n    '\nprepare_ud_dataset(treebank, udbase_dir, tokenizer_dir, short_name, short_language, 'train', augment, prepare_labels)\nprepare_ud_dataset(treebank, udbase_dir, tokenizer_dir, short_name, short_language, 'dev', augment, prepare_labels)\nprepare_ud_dataset(treebank, udbase_dir, tokenizer_dir, short_name, short_language, 'test', augment, prepare_labels)\n"}
{"label_name":"process","label":2,"method_name":"process_stats","method":"\n'Update info and check for overflow.'\ninfo['avg_step_time'] = (stats['step_time'] \/ steps_per_stats)\ninfo['avg_grad_norm'] = (stats['grad_norm'] \/ steps_per_stats)\ninfo['train_ppl'] = utils.safe_exp((stats['loss'] \/ stats['predict_count']))\ninfo['speed'] = (stats['total_count'] \/ (1000 * stats['step_time']))\nis_overflow = False\ntrain_ppl = info['train_ppl']\nif (math.isnan(train_ppl) or math.isinf(train_ppl) or (train_ppl > 1e+20)):\n    utils.print_out(('  step %d overflow, stop early' % global_step), log_f)\n    is_overflow = True\nreturn is_overflow\n"}
{"label_name":"train","label":0,"method_name":"example_simple_training_setting","method":"\nprint('Example: Simple Training Setting')\nimport numpy as np\nimport imgaug.augmenters as iaa\n\ndef load_batch(batch_idx):\n    return (np.zeros((128, 32, 32, 3), dtype=np.uint8) + (batch_idx % 255))\n\ndef train_on_images(images):\n    pass\nseq = iaa.Sequential([iaa.Crop(px=(1, 16), keep_size=False), iaa.Fliplr(0.5), iaa.GaussianBlur(sigma=(0, 3.0))])\nfor batch_idx in range(100):\n    images = load_batch(batch_idx)\n    images_aug = seq(images=images)\n    train_on_images(images_aug)\n    if (batch_idx == 0):\n        assert (not np.array_equal(images, images_aug))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\ndataset = TextDataSet(FLAGS.dest)\nepisodes = (dataset.dataset_size \/\/ FLAGS.batch_size)\nimage_placeholder = tf.placeholder(tf.float32, [FLAGS.examples_num, network.IMAGE_SIZE, network.IMAGE_SIZE, network.NUM_CHANNELS], name='image_placeholder')\n(prelogits, _) = network.inference(image_placeholder, is_training=False)\nembeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')\n(anchor, positive, negative) = tf.unstack(tf.reshape(embeddings, [(- 1), 3, FLAGS.embeddings_size]), 3, 1)\nglobal_step = tf.Variable(0, trainable=False)\nvariable_averages = tf.train.ExponentialMovingAverage(FLAGS.moving_average_decay, global_step)\nvariable_averages_op = variable_averages.apply(tf.trainable_variables())\nlearning_rate = tf.train.exponential_decay(FLAGS.learning_rate_base, global_step, episodes, FLAGS.learning_rate_decay)\ntriplet_loss = network.triplet_loss(anchor, positive, negative, FLAGS.margin)\nregularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\ntotal_loss = tf.add_n(([triplet_loss] + regularization_losses), name='total_loss')\ntf.summary.scalar('total_loss', total_loss)\nopt = tf.train.AdagradOptimizer(learning_rate)\ngrads = opt.compute_gradients(total_loss, tf.global_variables())\napply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\nwith tf.control_dependencies([apply_gradient_op, variable_averages_op]):\n    train_op = tf.no_op(name='train')\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    summary_writer = tf.summary.FileWriter('.\/tmp\/views\/', sess.graph)\n    sess.run(tf.global_variables_initializer())\n    model_path = os.path.join(FLAGS.model_save_path, 'checkpoint')\n    if os.path.exists(model_path):\n        ckpt = tf.train.get_checkpoint_state(FLAGS.model_save_path)\n        if (ckpt and ckpt.model_checkpoint_path):\n            saver.restore(sess, os.path.expanduser(ckpt.model_checkpoint_path))\n            image_placeholder = tf.get_default_graph().get_tensor_by_name('image_placeholder:0')\n            embeddings = tf.get_default_graph().get_tensor_by_name('embeddings:0')\n            from data.dataset import load_embs_data\n            (files, labels) = load_embs_data(FLAGS.dest)\n            emb_array = np.zeros((len(files), FLAGS.embeddings_size))\n            for idx in range(0, len(files), 3):\n                batch = load_data(files[idx:(idx + 3)])\n                data = np.reshape(batch, (FLAGS.examples_num, network.IMAGE_SIZE, network.IMAGE_SIZE, network.NUM_CHANNELS))\n                embs = sess.run(embeddings, feed_dict={image_placeholder: data})\n                (emb_array[idx, :], emb_array[(idx + 1), :], emb_array[(idx + 2), :]) = (embs[0, :], embs[1, :], embs[2, :])\n            batch = load_data(['.\/data\/\u5b89\/2.png', '.\/data\/\u5b89\/0.png', '.\/data\/\u5b89\/0.png'])\n            data = np.reshape(batch, (FLAGS.examples_num, network.IMAGE_SIZE, network.IMAGE_SIZE, network.NUM_CHANNELS))\n            embs = sess.run(embeddings, feed_dict={image_placeholder: data})\n            goal = np.array([embs[0, :] for i in range(emb_array.shape[0])])\n            dists = np.sqrt(np.sum(np.square(np.subtract(goal, emb_array)), axis=1))\n            indexes = np.where((dists == np.min(dists)))\n            print('Word IN Image: {}'.format([labels[i] for i in indexes[0]]))\n            quit()\n    for step in range(episodes):\n        (imgs, paths, labels) = dataset.next_batch(FLAGS.batch_size)\n        train_operation(sess, summary_writer, global_step, imgs, paths, labels, image_placeholder, train_op, embeddings, total_loss)\n    saver.save(sess, model_path, global_step=global_step)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n(sentences, word2idx) = get_wiki()\nvocab_size = len(word2idx)\nwindow_size = 5\nlearning_rate = (0.025 * 128)\nfinal_learning_rate = (0.0001 * 128)\nnum_negatives = 5\nsamples_per_epoch = int(100000.0)\nepochs = 1\nD = 50\nlearning_rate_delta = ((learning_rate - final_learning_rate) \/ epochs)\nW = (np.random.randn(vocab_size, D) \/ np.sqrt((D + vocab_size)))\nV = (np.random.randn(D, vocab_size) \/ np.sqrt((D + vocab_size)))\nthW = theano.shared(W)\nthV = theano.shared(V)\nth_pos_word = T.ivector('pos_word')\nth_neg_word = T.ivector('neg_word')\nth_context = T.ivector('context')\nth_lr = T.scalar('learning_rate')\ninput_words = T.concatenate([th_pos_word, th_neg_word])\nW_subset = thW[input_words]\ndbl_context = T.concatenate([th_context, th_context])\nV_subset = thV[:, dbl_context]\nlogits = W_subset.dot(V_subset)\nout = T.nnet.sigmoid(logits)\nn = th_pos_word.shape[0]\nth_cost = ((- T.log(out[:n]).mean()) - T.log((1 - out[n:])).mean())\ngW = T.grad(th_cost, W_subset)\ngV = T.grad(th_cost, V_subset)\nW_update = T.inc_subtensor(W_subset, ((- th_lr) * gW))\nV_update = T.inc_subtensor(V_subset, ((- th_lr) * gV))\nupdates = [(thW, W_update), (thV, V_update)]\ncost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\ncost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\np_neg = get_negative_sampling_distribution(sentences, vocab_size)\ncosts = []\ntotal_words = sum((len(sentence) for sentence in sentences))\nprint('total number of words in corpus:', total_words)\nthreshold = 1e-05\np_drop = (1 - np.sqrt((threshold \/ p_neg)))\nfor epoch in range(epochs):\n    np.random.shuffle(sentences)\n    cost = 0\n    counter = 0\n    inputs = []\n    targets = []\n    negwords = []\n    t0 = datetime.now()\n    for sentence in sentences:\n        sentence = [w for w in sentence if (np.random.random() < (1 - p_drop[w]))]\n        if (len(sentence) < 2):\n            continue\n        randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n        for pos in randomly_ordered_positions:\n            word = sentence[pos]\n            context_words = get_context(pos, sentence, window_size)\n            neg_word = np.random.choice(vocab_size, p=p_neg)\n            n = len(context_words)\n            inputs += ([word] * n)\n            negwords += ([neg_word] * n)\n            targets += context_words\n            if (len(inputs) >= 128):\n                c = cost_train_op(inputs, negwords, targets, learning_rate)\n                cost += c\n                if np.isnan(c):\n                    print('c is nan:', c)\n                    exit()\n                inputs = []\n                targets = []\n                negwords = []\n        counter += 1\n        if ((counter % 100) == 0):\n            sys.stdout.write(('processed %s \/ %s, cost: %s\\r' % (counter, len(sentences), c)))\n            sys.stdout.flush()\n    dt = (datetime.now() - t0)\n    print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n    costs.append(cost)\n    learning_rate -= learning_rate_delta\nplt.plot(costs)\nplt.show()\nif (not os.path.exists(savedir)):\n    os.mkdir(savedir)\nwith open(('%s\/word2idx.json' % savedir), 'w') as f:\n    json.dump(word2idx, f)\n(W, V) = (thW.get_value(), thV.get_value())\nnp.savez(('%s\/weights.npz' % savedir), W, V)\nreturn (word2idx, W, V)\n"}
{"label_name":"process","label":2,"method_name":"_process_date_conversion","method":"\n\ndef _isindex(colspec):\n    return ((isinstance(index_col, list) and (colspec in index_col)) or (isinstance(index_names, list) and (colspec in index_names)))\nnew_cols = []\nnew_data = {}\norig_names = columns\ncolumns = list(columns)\ndate_cols = set()\nif ((parse_spec is None) or isinstance(parse_spec, bool)):\n    return (data_dict, columns)\nif isinstance(parse_spec, list):\n    for colspec in parse_spec:\n        if is_scalar(colspec):\n            if (isinstance(colspec, int) and (colspec not in data_dict)):\n                colspec = orig_names[colspec]\n            if _isindex(colspec):\n                continue\n            data_dict[colspec] = converter(data_dict[colspec])\n        else:\n            (new_name, col, old_names) = _try_convert_dates(converter, colspec, data_dict, orig_names)\n            if (new_name in data_dict):\n                raise ValueError(('New date column already in dict %s' % new_name))\n            new_data[new_name] = col\n            new_cols.append(new_name)\n            date_cols.update(old_names)\nelif isinstance(parse_spec, dict):\n    for (new_name, colspec) in compat.iteritems(parse_spec):\n        if (new_name in data_dict):\n            raise ValueError(('Date column %s already in dict' % new_name))\n        (_, col, old_names) = _try_convert_dates(converter, colspec, data_dict, orig_names)\n        new_data[new_name] = col\n        new_cols.append(new_name)\n        date_cols.update(old_names)\ndata_dict.update(new_data)\nnew_cols.extend(columns)\nif (not keep_date_col):\n    for c in list(date_cols):\n        data_dict.pop(c)\n        new_cols.remove(c)\nreturn (data_dict, new_cols)\n"}
{"label_name":"train","label":0,"method_name":"subsubtrain_split_entry_point","method":"\nreturn _get_value_from_file('subsubtrain_split_entry_point')\n"}
{"label_name":"train","label":0,"method_name":"_get_training_dirs","method":"\nreturn [x[0] for x in os.walk(training_dir_path)][1:]\n"}
{"label_name":"save","label":1,"method_name":"save_pkg_resources_state","method":"\nsaved = pkg_resources.__getstate__()\ntry:\n    (yield saved)\nfinally:\n    pkg_resources.__setstate__(saved)\n"}
{"label_name":"save","label":1,"method_name":"save_weights","method":"\ntorch.save({'name': name, 'epoch': epoch, 'state_dict': model.state_dict()}, fpath)\n"}
{"label_name":"train","label":0,"method_name":"load_train","method":"\nimages = []\nclasses = []\npath = train_path\nfile_names = os.listdir(os.path.join(os.getcwd(), train_path))\ncounter = 1\nprint('Creating Classes, reading images and breaking things ...\\n')\nfor file in file_names:\n    drawProgressBar((counter \/ len(file_names)))\n    classes.append(file.split('_')[0])\n    image = cv2.imread(os.path.join(os.getcwd(), train_path, file))\n    image = image.astype(np.float32)\n    image = np.multiply(image, (1.0 \/ 255.0))\n    images.append(image)\n    counter += 1\nprint('\\nDone!')\nimages = np.array(images)\nd = {ni: indi for (indi, ni) in enumerate(set(classes))}\nclasses = [d[ni] for ni in classes]\nclasses = np.array(classes)\nn_values = (np.max(classes) + 1)\nclasses = np.eye(n_values)[classes]\nreturn (images, classes)\n"}
{"label_name":"train","label":0,"method_name":"get_trainable_weight_num","method":"\ntotal_parameters = 0\nfor variable in var_list:\n    shape = variable.get_shape()\n    variable_parameters = 1\n    for dim in shape:\n        variable_parameters *= dim.value\n    total_parameters += variable_parameters\nreturn total_parameters\n"}
{"label_name":"train","label":0,"method_name":"get_pretrained_xlmr","method":"\n'Get the pretrained XLM-R weights\\n\\n    Parameters\\n    ----------\\n    model_name\\n        The name of the xlmr model.\\n    root\\n        The downloading root\\n    load_backbone\\n        Whether to load the weights of the backbone network\\n    load_mlm\\n        Whether to load the weights of MLM\\n\\n    Returns\\n    -------\\n    cfg\\n        Network configuration\\n    tokenizer\\n        The SentencepieceTokenizer\\n    params_path\\n        Path to the parameters\\n    mlm_params_path\\n        Path to the parameter that includes both the backbone and the MLM\\n    '\nassert (model_name in PRETRAINED_URL), '{} is not found. All available are {}'.format(model_name, list_pretrained_xlmr())\ncfg_path = PRETRAINED_URL[model_name]['cfg']\nif isinstance(cfg_path, CN):\n    cfg = cfg_path\nelse:\n    cfg = None\nsp_model_path = PRETRAINED_URL[model_name]['sentencepiece.model']\nparams_path = PRETRAINED_URL[model_name]['params']\nmlm_params_path = PRETRAINED_URL[model_name]['mlm_params']\nlocal_paths = dict()\ndownload_jobs = [('sentencepiece.model', sp_model_path)]\nif (cfg is None):\n    download_jobs.append(('cfg', cfg_path))\nfor (k, path) in download_jobs:\n    local_paths[k] = download(url=(get_repo_model_zoo_url() + path), path=os.path.join(root, path), sha1_hash=FILE_STATS[path])\nif load_backbone:\n    local_params_path = download(url=(get_repo_model_zoo_url() + params_path), path=os.path.join(root, params_path), sha1_hash=FILE_STATS[params_path])\nelse:\n    local_params_path = None\nif (load_mlm and (mlm_params_path is not None)):\n    local_mlm_params_path = download(url=(get_repo_model_zoo_url() + mlm_params_path), path=os.path.join(root, mlm_params_path), sha1_hash=FILE_STATS[mlm_params_path])\nelse:\n    local_mlm_params_path = None\ndo_lower = (True if (('lowercase' in PRETRAINED_URL[model_name]) and PRETRAINED_URL[model_name]['lowercase']) else False)\ntokenizer = SentencepieceTokenizer(model_path=local_paths['sentencepiece.model'], lowercase=do_lower)\nif (cfg is None):\n    cfg = XLMRModel.get_cfg().clone_merge(local_paths['cfg'])\nreturn (cfg, tokenizer, local_params_path, local_mlm_params_path)\n"}
{"label_name":"forward","label":3,"method_name":"forward_and_visual","method":"\nbninp = mod.layers[0].input\nbnout = mod.layers[0].output\nbnfun = k.function([bninp], [bnout])\nbnout = bnfun([img])[0]\nbl1cv1inp = mod.layers[1].layers[1].input\nbl1cv1out = mod.layers[1].layers[1].output\nbl1cv1fun = k.function([bl1cv1inp], [bl1cv1out])\nbl1cv1out = bl1cv1fun([bnout])[0]\nbl1cv2inp = mod.layers[1].layers[2].input\nbl1cv2out = mod.layers[1].layers[2].output\nbl1cv2fun = k.function([bl1cv2inp], [bl1cv2out])\nbl1cv2out = bl1cv2fun([bl1cv1out])[0]\nbl1mxpinp = mod.layers[1].layers[3].input\nbl1mxpout = mod.layers[1].layers[3].output\nbl1mxpfun = k.function([bl1mxpinp], [bl1mxpout])\nbl1mxpout = bl1mxpfun([bl1cv2out])[0]\nbl2cv1inp = mod.layers[1].layers[4].input\nbl2cv1out = mod.layers[1].layers[4].output\nbl2cv1fun = k.function([bl2cv1inp], [bl2cv1out])\nbl2cv1out = bl2cv1fun([bl1mxpout])[0]\nbl2cv2inp = mod.layers[1].layers[5].input\nbl2cv2out = mod.layers[1].layers[5].output\nbl2cv2fun = k.function([bl2cv2inp], [bl2cv2out])\nbl2cv2out = bl2cv2fun([bl2cv1out])[0]\nbl2mxpinp = mod.layers[1].layers[6].input\nbl2mxpout = mod.layers[1].layers[6].output\nbl2mxpfun = k.function([bl2mxpinp], [bl2mxpout])\nbl2mxpout = bl2mxpfun([bl2cv2out])[0]\nbl3cv1inp = mod.layers[1].layers[7].input\nbl3cv1out = mod.layers[1].layers[7].output\nbl3cv1fun = k.function([bl3cv1inp], [bl3cv1out])\nbl3cv1out = bl3cv1fun([bl2mxpout])[0]\nbl3cv2inp = mod.layers[1].layers[8].input\nbl3cv2out = mod.layers[1].layers[8].output\nbl3cv2fun = k.function([bl3cv2inp], [bl3cv2out])\nbl3cv2out = bl3cv2fun([bl3cv1out])[0]\nbl3cv3inp = mod.layers[1].layers[9].input\nbl3cv3out = mod.layers[1].layers[9].output\nbl3cv3fun = k.function([bl3cv3inp], [bl3cv3out])\nbl3cv3out = bl3cv3fun([bl3cv2out])[0]\nbl3cv4inp = mod.layers[1].layers[10].input\nbl3cv4out = mod.layers[1].layers[10].output\nbl3cv4fun = k.function([bl3cv4inp], [bl3cv4out])\nbl3cv4out = bl3cv4fun([bl3cv3out])[0]\nbl3mxpinp = mod.layers[1].layers[11].input\nbl3mxpout = mod.layers[1].layers[11].output\nbl3mxpfun = k.function([bl3mxpinp], [bl3mxpout])\nbl3mxpout = bl3mxpfun([bl3cv1out])[0]\nbl4cv1inp = mod.layers[1].layers[12].input\nbl4cv1out = mod.layers[1].layers[12].output\nbl4cv1fun = k.function([bl4cv1inp], [bl4cv1out])\nbl4cv1out = bl4cv1fun([bl3mxpout])[0]\nbl4cv2inp = mod.layers[1].layers[13].input\nbl4cv2out = mod.layers[1].layers[13].output\nbl4cv2fun = k.function([bl4cv2inp], [bl4cv2out])\nbl4cv2out = bl4cv2fun([bl4cv1out])[0]\nbl4cv3inp = mod.layers[1].layers[14].input\nbl4cv3out = mod.layers[1].layers[14].output\nbl4cv3fun = k.function([bl4cv3inp], [bl4cv3out])\nbl4cv3out = bl4cv3fun([bl4cv2out])[0]\nbl4cv4inp = mod.layers[1].layers[15].input\nbl4cv4out = mod.layers[1].layers[15].output\nbl4cv4fun = k.function([bl4cv4inp], [bl4cv4out])\nbl4cv4out = bl4cv4fun([bl4cv3out])[0]\nbl4mxpinp = mod.layers[1].layers[16].input\nbl4mxpout = mod.layers[1].layers[16].output\nbl4mxpfun = k.function([bl4mxpinp], [bl4mxpout])\nbl4mxpout = bl4mxpfun([bl4cv4out])[0]\nbl5cv1inp = mod.layers[1].layers[17].input\nbl5cv1out = mod.layers[1].layers[17].output\nbl5cv1fun = k.function([bl5cv1inp], [bl5cv1out])\nbl5cv1out = bl5cv1fun([bl4mxpout])[0]\nbl5cv2inp = mod.layers[1].layers[18].input\nbl5cv2out = mod.layers[1].layers[18].output\nbl5cv2fun = k.function([bl5cv2inp], [bl5cv2out])\nbl5cv2out = bl5cv2fun([bl5cv1out])[0]\nbl5cv3inp = mod.layers[1].layers[19].input\nbl5cv3out = mod.layers[1].layers[19].output\nbl5cv3fun = k.function([bl5cv3inp], [bl5cv3out])\nbl5cv3out = bl5cv3fun([bl5cv2out])[0]\nbl5cv4inp = mod.layers[1].layers[20].input\nbl5cv4out = mod.layers[1].layers[20].output\nbl5cv4fun = k.function([bl5cv4inp], [bl5cv4out])\nbl5cv4out = bl5cv4fun([bl5cv3out])[0]\nbl5mxpinp = mod.layers[1].layers[21].input\nbl5mxpout = mod.layers[1].layers[21].output\nbl5mxpfun = k.function([bl5mxpinp], [bl5mxpout])\nbl5mxpout = bl5mxpfun([bl5cv4out])[0]\ndrop1inp = mod.layers[2].input\ndrop1out = mod.layers[2].output\ndrop1fun = k.function([drop1inp], [drop1out])\ndrop1out = drop1fun([bl5mxpout])[0]\nflat1inp = mod.layers[3].input\nflat1out = mod.layers[3].output\nflat1fun = k.function([flat1inp], [flat1out])\nflat1out = flat1fun([drop1out])[0]\nfcinp = mod.layers[4].input\nfcout = mod.layers[4].output\nfcfun = k.function([fcinp], [fcout])\nfcout = fcfun([flat1out])[0]\ndrop2inp = mod.layers[5].input\ndrop2out = mod.layers[5].output\ndrop2fun = k.function([drop2inp], [drop2out])\ndrop2out = drop2fun([fcout])[0]\nsiginp = mod.layers[6].input\nsigout = mod.layers[6].output\nsigfun = k.function([siginp], [sigout])\nsigout = sigfun([drop2out])[0]\nbl1cv1out_avg = mean(bl1cv1out)\nbl1cv2out_avg = mean(bl1cv2out)\nbl1cv2out_swi = get_switches(bl1cv2out_avg)\nbl2cv1out_avg = mean(bl2cv1out)\nbl2cv2out_avg = mean(bl2cv2out)\nbl2cv2out_swi = get_switches(bl2cv2out_avg)\nbl3cv1out_avg = mean(bl3cv1out)\nbl3cv2out_avg = mean(bl3cv2out)\nbl3cv3out_avg = mean(bl3cv3out)\nbl3cv4out_avg = mean(bl3cv4out)\nbl3cv4out_swi = get_switches(bl3cv4out_avg)\nbl4cv1out_avg = mean(bl4cv1out)\nbl4cv2out_avg = mean(bl4cv2out)\nbl4cv3out_avg = mean(bl4cv3out)\nbl4cv4out_avg = mean(bl4cv4out)\nbl4cv4out_swi = get_switches(bl4cv4out_avg)\nbl5cv1out_avg = mean(bl5cv1out)\nbl5cv2out_avg = mean(bl5cv2out)\nbl5cv3out_avg = mean(bl5cv3out)\nbl5cv4out_avg = mean(bl5cv4out)\nbl5cv4out_swi = get_switches(bl5cv4out_avg)\nbl5mxpout_avg = mean(bl5mxpout)\nbl5mxpout_up = unpool(bl5mxpout_avg, bl5cv4out_swi)\nbl5mxpout_up = shave_padding(bl5mxpout_up, 2)\nbl5dec4 = deconv(bl5mxpout_up)\nbl5dec4 = (bl5dec4 * bl5cv4out_avg)\nbl5dec4 = shave_padding(bl5dec4, 2)\nbl5dec3 = deconv(bl5dec4)\nbl5dec3 = (bl5dec3 * bl5cv3out_avg)\nbl5dec3 = shave_padding(bl5dec3, 2)\nbl5dec2 = deconv(bl5dec3)\nbl5dec2 = (bl5dec2 * bl5cv2out_avg)\nbl5dec2 = shave_padding(bl5dec2, 2)\nbl5dec1 = deconv(bl5dec2)\nbl5dec1 = (bl5dec1 * bl5cv1out_avg)\nbl4mxpout_up = unpool(bl5dec1, bl4cv4out_swi)\nbl4mxpout_up = shave_padding(bl4mxpout_up, 2)\nbl4dec4 = deconv(bl4mxpout_up)\nbl4dec4 = (bl4dec4 * bl4cv4out_avg)\nbl4dec4 = shave_padding(bl4dec4, 2)\nbl4dec3 = deconv(bl4dec4)\nbl4dec3 = (bl4dec3 * bl4cv3out_avg)\nbl4dec3 = shave_padding(bl4dec3, 2)\nbl4dec2 = deconv(bl4dec3)\nbl4dec2 = (bl4dec2 * bl4cv2out_avg)\nbl4dec2 = shave_padding(bl4dec2, 2)\nbl4dec1 = deconv(bl4dec2)\nbl4dec1 = (bl4dec1 * bl4cv1out_avg)\nbl3mxpout_up = unpool(bl4dec1, bl3cv4out_swi)\nbl3mxpout_up = shave_padding(bl3mxpout_up, 2)\nbl3dec4 = deconv(bl3mxpout_up)\nbl3dec4 = (bl3dec4 * bl3cv4out_avg)\nbl3dec4 = shave_padding(bl3dec4, 2)\nbl3dec3 = deconv(bl3dec4)\nbl3dec3 = (bl3dec3 * bl3cv3out_avg)\nbl3dec3 = shave_padding(bl3dec3, 2)\nbl3dec2 = deconv(bl3dec3)\nbl3dec2 = (bl3dec2 * bl3cv2out_avg)\nbl3dec2 = shave_padding(bl3dec2, 2)\nbl3dec1 = deconv(bl3dec2)\nbl3dec1 = (bl3dec1 * bl3cv1out_avg)\nbl2mxpout_up = unpool(bl3dec1, bl2cv2out_swi)\nbl2mxpout_up = shave_padding(bl2mxpout_up, 2)\nbl2dec2 = deconv(bl2mxpout_up)\nbl2dec2 = (bl2dec2 * bl2cv2out_avg)\nbl2dec2 = shave_padding(bl2dec2, 2)\nbl2dec1 = deconv(bl2dec2)\nbl2dec1 = (bl2dec1 * bl2cv1out_avg)\nbl1mxpout_up = unpool(bl2dec1, bl1cv2out_swi)\nbl1mxpout_up = shave_padding(bl1mxpout_up, 2)\nbl1dec2 = deconv(bl1mxpout_up)\nbl1dec2 = (bl1dec2 * bl1cv2out_avg)\nbl1dec2 = shave_padding(bl1dec2, 2)\nbl1dec1 = deconv(bl1dec2)\nbl1dec1 = (bl1dec1 * bl1cv1out_avg)\nprint(bl1dec1.shape)\ncv2.imshow('', normalize(bl1dec1[0, :, :, 0]))\ncv2.waitKey(0)\n"}
{"label_name":"train","label":0,"method_name":"train_optimizer","method":"\n\"Trains the meta-parameters of this optimizer.\\n\\n  Args:\\n    logdir: a directory filepath for storing model checkpoints (must exist)\\n    optimizer_spec: specification for an Optimizer (see utils.Spec)\\n    problems_and_data: a list of tuples containing three elements: a problem\\n      specification (see utils.Spec), a dataset (see datasets.Dataset), and\\n      a batch_size (int) for generating a problem and corresponding dataset. If\\n      the problem doesn't have data, set dataset to None.\\n    num_problems: the number of problems to sample during meta-training\\n    num_meta_iterations: the number of iterations (steps) to run the\\n      meta-optimizer for on each subproblem.\\n    num_unroll_func: called once per meta iteration and returns the number of\\n      unrolls to do for that meta iteration.\\n    num_partial_unroll_itrs_func: called once per unroll and returns the number\\n      of iterations to do for that unroll.\\n    learning_rate: learning rate of the RMSProp meta-optimizer (Default: 1e-4)\\n    gradient_clip: value to clip gradients at (Default: 5.0)\\n    is_chief: whether this is the chief task (Default: False)\\n    select_random_problems: whether to select training problems randomly\\n        (Default: True)\\n    callbacks: a list of callback functions that is run after every random\\n        problem draw\\n    obj_train_max_multiplier: the maximum increase in the objective value over\\n        a single training run. Ignored if < 0.\\n    out: where to write output to, e.g. a file handle (Default: sys.stdout)\\n\\n  Raises:\\n    ValueError: If one of the subproblems has a negative objective value.\\n  \"\nif select_random_problems:\n    sampler = (random.choice(problems_and_data) for _ in range(num_problems))\nelse:\n    num_repeats = ((num_problems \/ len(problems_and_data)) + 1)\n    random.shuffle(problems_and_data)\n    sampler = (problems_and_data * num_repeats)[:num_problems]\nfor (problem_itr, (problem_spec, dataset, batch_size)) in enumerate(sampler):\n    problem_start_time = time.time()\n    if (dataset is None):\n        dataset = datasets.EMPTY_DATASET\n        batch_size = dataset.size\n    graph = tf.Graph()\n    real_device_setter = tf.train.replica_device_setter(FLAGS.ps_tasks)\n\n    def custom_device_setter(op):\n        if trainable_optimizer.is_local_state_variable(op):\n            return '\/job:worker'\n        else:\n            return real_device_setter(op)\n    if real_device_setter:\n        device_setter = custom_device_setter\n    else:\n        device_setter = None\n    with graph.as_default(), graph.device(device_setter):\n        problem = problem_spec.build()\n        opt = optimizer_spec.build()\n        train_output = opt.train(problem, dataset)\n        state_keys = opt.state_keys\n        for (key, val) in zip(state_keys, train_output.output_state[0]):\n            finite_val = utils.make_finite(val, replacement=tf.zeros_like(val))\n            tf.summary.histogram('State\/{}'.format(key), finite_val, collections=[OPT_SUM_COLLECTION])\n        tf.summary.scalar('MetaObjective', train_output.metaobj, collections=[OPT_SUM_COLLECTION])\n        tf.summary.scalar((problem_spec.callable.__name__ + '_MetaObjective'), train_output.metaobj, collections=[OPT_SUM_COLLECTION])\n        global_step = tf.Variable(0, name='global_step', trainable=False)\n        meta_parameters = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=OPTIMIZER_SCOPE)\n        reg_l2 = (FLAGS.l2_reg * sum([tf.reduce_sum((param ** 2)) for param in meta_parameters]))\n        meta_opt = tf.train.RMSPropOptimizer(learning_rate, decay=FLAGS.rms_decay, use_locking=True, epsilon=FLAGS.rms_epsilon)\n        grads_and_vars = meta_opt.compute_gradients((train_output.metaobj + reg_l2), meta_parameters)\n        clipped_grads_and_vars = []\n        for (grad, var) in grads_and_vars:\n            clipped_grad = tf.clip_by_value(utils.make_finite(grad, replacement=tf.zeros_like(var)), (- gradient_clip), gradient_clip)\n            clipped_grads_and_vars.append((clipped_grad, var))\n        for (grad, var) in grads_and_vars:\n            tf.summary.histogram((var.name + '_rawgrad'), utils.make_finite(grad, replacement=tf.zeros_like(grad)), collections=[OPT_SUM_COLLECTION])\n        for (grad, var) in clipped_grads_and_vars:\n            tf.summary.histogram((var.name + '_var'), var, collections=[OPT_SUM_COLLECTION])\n            tf.summary.histogram((var.name + '_grad'), grad, collections=[OPT_SUM_COLLECTION])\n        train_op = meta_opt.apply_gradients(clipped_grads_and_vars, global_step=global_step)\n        summary_op = tf.summary.merge_all(key=OPT_SUM_COLLECTION)\n        with tf.control_dependencies([train_op, summary_op]):\n            propagate_loop_state_ops = []\n            for (dest, src) in zip(train_output.init_loop_vars, train_output.output_loop_vars):\n                propagate_loop_state_ops.append(dest.assign(src))\n            propagate_loop_state_op = tf.group(*propagate_loop_state_ops)\n        sv = tf.train.Supervisor(graph=graph, is_chief=is_chief, logdir=logdir, summary_op=None, save_model_secs=0, global_step=global_step)\n        with sv.managed_session() as sess:\n            init_time = (time.time() - problem_start_time)\n            out.write('--------- Problem #{} ---------\\n'.format(problem_itr))\n            out.write('{callable.__name__}{args}{kwargs}\\n'.format(**problem_spec.__dict__))\n            out.write('Took {} seconds to initialize.\\n'.format(init_time))\n            out.flush()\n            if FLAGS.set_profiling:\n                summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)\n            metadata = defaultdict(list)\n            for k in range(num_meta_iterations):\n                if sv.should_stop():\n                    break\n                problem.init_fn(sess)\n                full_trace_opt = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n                run_options = (full_trace_opt if FLAGS.set_profiling else None)\n                run_metadata = (tf.RunMetadata() if FLAGS.set_profiling else None)\n                num_unrolls = num_unroll_func()\n                partial_unroll_iters = [num_partial_unroll_itrs_func() for _ in xrange(num_unrolls)]\n                total_num_iter = sum(partial_unroll_iters)\n                objective_weights = [(np.ones(num) \/ float(num)) for num in partial_unroll_iters]\n                db = dataset.batch_indices(total_num_iter, batch_size)\n                dataset_batches = []\n                last_index = 0\n                for num in partial_unroll_iters:\n                    dataset_batches.append(db[last_index:(last_index + num)])\n                    last_index += num\n                train_start_time = time.time()\n                unroll_itr = 0\n                additional_log_info = ''\n                for unroll_itr in range(num_unrolls):\n                    first_unroll = (unroll_itr == 0)\n                    if FLAGS.reset_rnn_params:\n                        reset_state = (first_unroll and (k == 0))\n                    else:\n                        reset_state = first_unroll\n                    feed = {train_output.obj_weights: objective_weights[unroll_itr], train_output.batches: dataset_batches[unroll_itr], train_output.first_unroll: first_unroll, train_output.reset_state: reset_state}\n                    fetches_list = [train_output.metaobj, train_output.problem_objectives, train_output.initial_obj, summary_op, clipped_grads_and_vars, train_op]\n                    if ((unroll_itr + 1) < num_unrolls):\n                        fetches_list += [propagate_loop_state_op]\n                    fetched = sess.run(fetches_list, feed_dict=feed, options=run_options, run_metadata=run_metadata)\n                    meta_obj = fetched[0]\n                    sub_obj = fetched[1]\n                    init_obj = fetched[2]\n                    summ = fetched[3]\n                    meta_grads_and_params = fetched[4]\n                    if np.any((sub_obj < 0)):\n                        raise ValueError('Training problem objectives must be nonnegative.')\n                    if ((obj_train_max_multiplier > 0) and (sub_obj[(- 1)] > (init_obj + (abs(init_obj) * (obj_train_max_multiplier - 1))))):\n                        msg = ' Broke early at {} out of {} unrolls. '.format((unroll_itr + 1), num_unrolls)\n                        additional_log_info += msg\n                        break\n                    if is_chief:\n                        sv.summary_computed(sess, summ)\n                    metadata['subproblem_objs'].append(sub_obj)\n                    metadata['meta_objs'].append(meta_obj)\n                    metadata['meta_grads_and_params'].append(meta_grads_and_params)\n                optimization_time = (time.time() - train_start_time)\n                if FLAGS.set_profiling:\n                    summary_name = ('%02d_iter%04d_%02d' % (FLAGS.task, problem_itr, k))\n                    summary_writer.add_run_metadata(run_metadata, summary_name)\n                metadata['global_step'].append(sess.run(global_step))\n                metadata['runtimes'].append(optimization_time)\n                args = (k, meta_obj, optimization_time, sum(partial_unroll_iters[:(unroll_itr + 1)]))\n                out.write('  [{:02}] {}, {} seconds, {} iters '.format(*args))\n                out.write('(unrolled {} steps)'.format(', '.join([str(s) for s in partial_unroll_iters[:(unroll_itr + 1)]])))\n                out.write('{}\\n'.format(additional_log_info))\n                out.flush()\n            if FLAGS.set_profiling:\n                summary_writer.close()\n            if is_chief:\n                sv.saver.save(sess, sv.save_path, global_step=global_step)\n    if (is_chief and (callbacks is not None)):\n        for callback in callbacks:\n            if hasattr(callback, '__call__'):\n                problem_name = problem_spec.callable.__name__\n                callback(problem_name, problem_itr, logdir, metadata)\n"}
{"label_name":"process","label":2,"method_name":"dataset_preprocess","method":"\npass\n"}
{"label_name":"save","label":1,"method_name":"save_model_instance","method":"\n' Save a whole Model instance and the corresponding model with weights to two files (model.p and model.hdf5)\\n\\n    :param mod: model instance\\n    :return: saved model files in the checkpoint dir\\n    '\ntmp = mod.model\ntmp.save((mod.checkpointdir + 'model.hdf5'))\nmod.model = None\npickle.dump(mod, open((mod.checkpointdir + 'model.p'), 'wb'))\nmod.model = tmp\n"}
{"label_name":"process","label":2,"method_name":"clarify_processor","method":"\nreturn SageMakerClarifyProcessor(role='AmazonSageMaker-ExecutionRole', instance_count=1, instance_type='ml.c5.xlarge', sagemaker_session=sagemaker_session)\n"}
{"label_name":"process","label":2,"method_name":"process_entry","method":"\nvobj.vtype = enums.IP\nparser = PARSERS[year]\nentries = {k: line[v] for (k, v) in parser.iteritems()}\nvobj.key = entries['KEY_NRD']\npdx = entries['DX1']\nif pdx.strip():\n    vobj.primary_diagnosis = 'D{}'.format(pdx)\nelse:\n    vobj.primary_diagnosis = ''\nvobj.patient_key = entries['NRD_VisitLink']\nvobj.state = 'NRD'\nvobj.day = int(entries['NRD_DaysToEvent'])\nvobj.age = int(entries['AGE'])\nif entries['HCUP_ED'].strip():\n    vobj.source = enums.S_ED\nelse:\n    vobj.source = enums.S_OTHER\nvobj.race = enums.R_UNKNOWN\nvobj.sex = LMAP[('sex_dict', int(entries['FEMALE']))][2]\nvobj.payer = LMAP[('payer_dict', int(entries['PAY1']))][2]\nvobj.disposition = LMAP[('disposition_dict', int(entries['DISPUNIFORM']))][2]\nvobj.death = LMAP[('died_dict', int(entries['DIED']))][2]\nvobj.year = int(year)\nmonth = entries['DMONTH']\nif month:\n    vobj.month = int(month)\nelse:\n    vobj.month = (- 1)\nvobj.quarter = int(entries['DQTR'])\ncharges = float(entries['TOTCHG'])\nvobj.zip = LMAP[('pzip_dict', int(entries['ZIPINC_QRTL']))][2]\nif charges:\n    vobj.charge = float(charges)\nelse:\n    vobj.charge = (- 1)\nvobj.dataset = ('NRD_' + str(year))\nvobj.drg = 'DG{}'.format(entries['DRG'])\nlos = entries['LOS']\nif (los.strip() and (los >= 0)):\n    vobj.los = int(los)\nelse:\n    vobj.los = (- 1)\nvobj.facility = entries['HOSP_NRD']\ndxlist = [entries['DX{}'.format((k + 1))] for k in range(25)]\nprlist = [entries['PR{}'.format((k + 1))] for k in range(15)]\nvobj.dnr = enums.DNR_UNAVAILABLE\nfor k in dxlist:\n    if k.strip():\n        vobj.dxs.append(('D' + k))\nfor k in range(1, 5):\n    if entries['ECODE{}'.format(k)].strip():\n        vobj.exs.append(('E' + entries['ECODE{}'.format(k)]))\nprimary_procedure = entries['PR1']\nif (len(primary_procedure.strip()) > 1):\n    vobj.primary_procedure.pcode = ('P' + primary_procedure)\n    vobj.primary_procedure.ctype = enums.ICD\n    vobj.primary_procedure.pday = 0\nfor pr in prlist:\n    if pr.strip():\n        temp = vobj.prs.add()\n        temp.pcode = ('P' + pr)\n        temp.pday = (- 1)\n        temp.ctype = enums.ICD\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n'Runs the model on the given data.'\nwith open(FLAGS.p_path, 'w') as f:\n    q_id = 1\n    f.write('id,answer\\n')\n    for q in questions:\n        left = q.left\n        if (len(left) < 30):\n            for i in range((30 - len(left))):\n                left += [0]\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        state = session.run(model.initial_state)\n        fetches = {'logits': model.logits, 'final_state': model.final_state}\n        feed_dict = {}\n        for (i, (c, h)) in enumerate(model.initial_state):\n            feed_dict[c] = state[i].c\n            feed_dict[h] = state[i].h\n        feed_dict[model.question] = np.array(left).reshape((1, (- 1)))\n        vals = session.run(fetches, feed_dict)\n        p_opt = []\n        for o in q.options:\n            p_opt.append(np.log(vals['logits'][((q.pos - 1), o)]))\n        f.write('{},{}\\n'.format(q_id, ['a', 'b', 'c', 'd', 'e'][np.argmax(p_opt)]))\n        q_id += 1\nreturn None\n"}
{"label_name":"process","label":2,"method_name":"syncnet_process","method":"\nccore = ccore_library.get()\nccore.syncnet_process.restype = POINTER(c_void_p)\nreturn ccore.syncnet_process(network_pointer, c_double(order), c_uint(solution), c_bool(collect_dynamic))\n"}
{"label_name":"train","label":0,"method_name":"lms_train","method":"\n\ndef error(p, y, args):\n    l = len(p)\n    f = p[(l - 1)]\n    for i in range(len(args)):\n        f += (p[i] * args[i])\n    return (f - y)\nPara = leastsq(error, p0, args=(Zi, Data))\nreturn Para[0]\n"}
{"label_name":"train","label":0,"method_name":"Ten_train","method":"\nInput_Dimen = len(xdata[0])\nUnit_Layers = (([Input_Dimen] + ([hiddennodes] * hiddenlayers)) + [len(ydata[0])])\nx_data = tf.placeholder(shape=[None, Input_Dimen], dtype=tf.float32, name='x_data')\ny_target = tf.placeholder(shape=[None, len(ydata[0])], dtype=tf.float32)\nVAR_NAME = locals()\nfor jj in range((hiddenlayers + 1)):\n    VAR_NAME[('weight%s' % jj)] = tf.Variable((np.random.rand(Unit_Layers[jj], Unit_Layers[(jj + 1)]) \/ np.sqrt(Unit_Layers[jj])), dtype=tf.float32, name=('Weight%s' % jj))\n    VAR_NAME[('bias%s' % jj)] = tf.Variable(tf.random_normal([Unit_Layers[(jj + 1)]], stddev=10), dtype=tf.float32, name=('Bias%s' % jj))\n    if (jj == 0):\n        VAR_NAME[('ooutda%s' % jj)] = activate(x_data, eval(('weight%s' % jj)), eval(('bias%s' % jj)), actfunc=activate_func)\n    elif (jj == hiddenlayers):\n        VAR_NAME[('ooutda%s' % jj)] = activate(eval(('ooutda%s' % (jj - 1))), eval(('weight%s' % jj)), eval(('bias%s' % jj)), actfunc='linear')\n    else:\n        VAR_NAME[('ooutda%s' % jj)] = activate(eval(('ooutda%s' % (jj - 1))), eval(('weight%s' % jj)), eval(('bias%s' % jj)), actfunc=activate_func)\nuuu = tf.nn.softmax(eval(('ooutda%s' % hiddenlayers)))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_target, logits=eval(('ooutda%s' % hiddenlayers))))\naccu = eval(('ooutda%s' % hiddenlayers))\nmy_opt = tf.train.AdamOptimizer(learn_rate)\ntrain_step = my_opt.minimize(loss)\ninit = tf.global_variables_initializer()\nloss_vec = []\nloss_vec_add = []\nacc_vec = []\nacc_vec_add = []\ngraph = tf.get_default_graph()\nsaver = tf.train.Saver(max_to_keep=1)\nsess = tf.Session()\naccudict = {}\naccunum = 0\nsess.run(init)\nfor i in range(itertimes):\n    for jj in range(int((len(xdata) \/ batch_size))):\n        rand_index = np.random.choice(len(xdata), size=batch_size, replace=False)\n        rand_x = xdata[rand_index]\n        rand_y = ydata[rand_index]\n        sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss = sess.run(loss, feed_dict={x_data: xdata, y_target: ydata})\n    loss_vec.append(temp_loss)\n    temp_loss_add = sess.run(loss, feed_dict={x_data: addxdata, y_target: addydata})\n    loss_vec_add.append(temp_loss_add)\n    acc_ru = sess.run(accu, feed_dict={x_data: xdata})\n    acc_rughy_train = outvsreal(judge(acc_ru), ydata)\n    acc_vec.append(acc_rughy_train)\n    acu = sess.run(accu, feed_dict={x_data: addxdata})\n    acc_rughy = outvsreal(judge(acu), addydata)\n    acc_vec_add.append(acc_rughy)\n    print(('%s\u4ee3\u8bef\u5dee\uff1a [\u8bad\u7ec3\uff1a%.4f, \u9a8c\u8bc1\uff1a%.4f], \u6b63\u786e\u7387\uff1a [\u8bad\u7ec3\uff1a%.4f, \u9a8c\u8bc1\uff1a%.4f]' % (i, temp_loss, temp_loss_add, acc_rughy_train, acc_rughy)))\n    accudict[i] = [acc_rughy_train, acc_rughy]\n    if ((i > 30) and ((max(acc_vec) - min(acc_vec)) < 0.3)):\n        tf.reset_default_graph()\n        sess.close()\n        return False\n    zongheaccu = ((0.1 * acc_rughy_train) + (0.9 * acc_rughy))\n    if (zongheaccu > accunum):\n        accunum = zongheaccu\n        saver.save(sess, ('.\/gu\/%smodel' % kcount), global_step=i)\nsign = max(accudict.items(), key=(lambda d: ((0.1 * d[1][0]) + (0.9 * d[1][1]))))[0]\nprint(('%s \u6298\u8fd0\u884c\u5b8c\u6bd5\uff0c\u6a21\u578b\u5df2\u7ecf\u4fdd\u5b58\uff0c\u6700\u4f18\u7684\u662f%s\u4ee3' % (kcount, sign)))\nreturn (loss_vec[:(sign + 1)], loss_vec_add[:(sign + 1)], acc_vec[:(sign + 1)], acc_vec_add[:(sign + 1)], sign, hiddenlayers)\n"}
{"label_name":"save","label":1,"method_name":"save_params","method":"\n'\\n    Save parameters to file\\n    '\npickle.dump(params, open('params.p', 'wb'))\n"}
{"label_name":"predict","label":4,"method_name":"predict1","method":"\ndistances = np.zeros((test_data.shape[0], len(classes)))\nclasses = np.array(classes)\nfor (i, clazz) in enumerate(classes):\n    tmp = (np.sum(((clazz[0] - test_data) ** 2), axis=1) ** 0.5)\n    distances[:, i] = np.array(list(map((lambda x: max(0, x)), (tmp - clazz[1]))))\nreturn distances.argmin(axis=1)\n"}
{"label_name":"save","label":1,"method_name":"_download_higgs_data_and_save_npz","method":"\n'Download higgs data and store as a numpy compressed file.'\ninput_url = os.path.join(URL_ROOT, INPUT_FILE)\nnp_filename = os.path.join(data_dir, NPZ_FILE)\nif tf.gfile.Exists(np_filename):\n    raise ValueError('data_dir already has the processed data file: {}'.format(np_filename))\nif (not tf.gfile.Exists(data_dir)):\n    tf.gfile.MkDir(data_dir)\ntry:\n    tf.logging.info('Data downloading...')\n    (temp_filename, _) = urllib.request.urlretrieve(input_url)\n    tf.logging.info('Data processing... taking multiple minutes...')\n    with gzip.open(temp_filename, 'rb') as csv_file:\n        data = pd.read_csv(csv_file, dtype=np.float32, names=[('c%02d' % i) for i in range(29)]).as_matrix()\nfinally:\n    tf.gfile.Remove(temp_filename)\nf = tempfile.NamedTemporaryFile()\nnp.savez_compressed(f, data=data)\ntf.gfile.Copy(f.name, np_filename)\ntf.logging.info('Data saved to: {}'.format(np_filename))\n"}
{"label_name":"train","label":0,"method_name":"add_final_training_ops","method":"\n\"Adds a new softmax and fully-connected layer for training.\\n\\n  We need to retrain the top layer to identify our new classes, so this function\\n  adds the right operations to the graph, along with some variables to hold the\\n  weights, and then sets up all the gradients for the backward pass.\\n\\n  The set up for the softmax and fully-connected layers is based on:\\n  https:\/\/tensorflow.org\/versions\/master\/tutorials\/mnist\/beginners\/index.html\\n\\n  Args:\\n    class_count: Integer of how many categories of things we're trying to\\n    recognize.\\n    final_tensor_name: Name string for the new final node that produces results.\\n    bottleneck_tensor: The output of the main CNN graph.\\n\\n  Returns:\\n    The tensors for the training and cross entropy results, and tensors for the\\n    bottleneck input and ground truth input.\\n  \"\nwith tf.name_scope('input'):\n    bottleneck_input = tf.placeholder_with_default(bottleneck_tensor, shape=[None, BOTTLENECK_TENSOR_SIZE], name='BottleneckInputPlaceholder')\n    ground_truth_input = tf.placeholder(tf.float32, [None, class_count], name='GroundTruthInput')\nlayer_name = 'final_training_ops'\nwith tf.name_scope(layer_name):\n    with tf.name_scope('weights'):\n        layer_weights = tf.Variable(tf.truncated_normal([BOTTLENECK_TENSOR_SIZE, class_count], stddev=0.001), name='final_weights')\n        variable_summaries(layer_weights)\n    with tf.name_scope('biases'):\n        layer_biases = tf.Variable(tf.zeros([class_count]), name='final_biases')\n        variable_summaries(layer_biases)\n    with tf.name_scope('Wx_plus_b'):\n        logits = (tf.matmul(bottleneck_input, layer_weights) + layer_biases)\n        tf.summary.histogram('pre_activations', logits)\nfinal_tensor = tf.nn.softmax(logits, name=final_tensor_name)\ntf.summary.histogram('activations', final_tensor)\nwith tf.name_scope('cross_entropy'):\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_input, logits=logits)\n    with tf.name_scope('total'):\n        cross_entropy_mean = tf.reduce_mean(cross_entropy)\ntf.summary.scalar('cross_entropy', cross_entropy_mean)\nwith tf.name_scope('train'):\n    train_step = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(cross_entropy_mean)\nreturn (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input, final_tensor)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nhparams = sorted(sbn.hparams.values().items())\nhparams = (map(str, x) for x in hparams)\nhparams = ('_'.join(x) for x in hparams)\nhparams_str = '.'.join(hparams)\nlogger = L.Logger()\nexperiment_name = ([str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)])\nif sbn.hparams.nonlinear:\n    experiment_name = '~'.join(experiment_name)\nelse:\n    experiment_name = '-'.join(experiment_name)\nexperiment_name = ('SBN_%s' % experiment_name)\nrowkey = {'experiment': experiment_name, 'model': hparams_str}\nsumm_dir = os.path.join(FLAGS.working_dir, hparams_str)\nsummary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\nsv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\nwith sv.managed_session() as sess:\n    with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n        json.dump(sbn.hparams.values(), out)\n    sbn.initialize(sess)\n    batch_size = sbn.hparams.batch_size\n    scores = []\n    n = train_xs.shape[0]\n    index = range(n)\n    while (not sv.should_stop()):\n        lHats = []\n        grad_variances = []\n        temperatures = []\n        random.shuffle(index)\n        i = 0\n        while (i < n):\n            batch_index = index[i:min((i + batch_size), n)]\n            batch_xs = train_xs[batch_index, :]\n            if sbn.hparams.dynamic_b:\n                batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n            (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n            if debug:\n                print(i, lHat)\n                if (i > 100):\n                    return\n            lHats.append(lHat)\n            grad_variances.append(grad_variance)\n            temperatures.append(temperature)\n            i += batch_size\n        grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n        summary_strings = []\n        if isinstance(grad_variances, list):\n            grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n            rowkey['step'] = step\n            logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n            grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n        else:\n            rowkey['step'] = step\n            logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n            summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n        print(('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances))))\n        epoch = int((step \/ (train_xs.shape[0] \/ sbn.hparams.batch_size)))\n        if ((epoch % FLAGS.eval_freq) == 0):\n            valid_res = eval(sbn, valid_xs)\n            test_res = eval(sbn, test_xs)\n            print(('\\nValid %d: %s' % (step, str(valid_res))))\n            print(('Test %d: %s\\n' % (step, str(test_res))))\n            logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n            logger.flush()\n        summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n        for summ_str in summary_strings:\n            summary_writer.add_summary(summ_str, global_step=step)\n        summary_writer.flush()\n        sys.stdout.flush()\n        scores.append(np.mean(lHats, axis=0))\n        if (step > training_steps):\n            break\n    return scores\n"}
{"label_name":"process","label":2,"method_name":"_preprocess_padding","method":"\n\"Convert keras' padding to tensorflow's padding.\\n\\n  Arguments:\\n      padding: string, one of 'same' , 'valid'\\n\\n  Returns:\\n      a string, one of 'SAME', 'VALID'.\\n\\n  Raises:\\n      ValueError: if invalid `padding'`\\n  \"\nif (padding == 'same'):\n    padding = 'SAME'\nelif (padding == 'valid'):\n    padding = 'VALID'\nelse:\n    raise ValueError('Invalid padding:', padding)\nreturn padding\n"}
{"label_name":"train","label":0,"method_name":"train_core","method":"\nfrom rasa.train import train_core\nimport asyncio\nloop = asyncio.get_event_loop()\noutput = (train_path or args.out)\nargs.domain = get_validated_path(args.domain, 'domain', DEFAULT_DOMAIN_PATH)\nstories = get_validated_path(args.stories, 'stories', DEFAULT_DATA_PATH)\n_train_path = (train_path or tempfile.mkdtemp())\nif ((not isinstance(args.config, list)) or (len(args.config) == 1)):\n    if isinstance(args.config, list):\n        args.config = args.config[0]\n    config = get_validated_path(args.config, 'config', DEFAULT_CONFIG_PATH)\n    return train_core(args.domain, config, stories, output, train_path)\nelse:\n    from rasa.core.train import do_compare_training\n    loop.run_until_complete(do_compare_training(args, stories, None))\n    return None\n"}
{"label_name":"train","label":0,"method_name":"linear_train","method":"\ndata_f = pandas.read_csv(features_train, header=None, sep=';')\nfeatures = data_f.iloc[:, 1:]\nfeatures = scale(features)\ndata_t = pandas.read_csv(target_train, header=None, sep=';')\ntarget = data_t.iloc[:, 1]\nperc = Perceptron(random_state=242)\nperc.fit(features, target)\nreturn perc\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n'\\n    :param custom_metrics_logger: Optional custom metrics logging function. If supplied, takes care of metrics produced\\n                                  during training in a custom way. It should accept a list or a dictionary of\\n                                  (metric name, metric value) pairs, and an optional global_step\/checkpoint parameter.\\n    :param checkpoint_callback: An optional callback function (int -> None). The function will be called\\n+                                each time a checkpoint has been reached\\n    '\nif args.dry_run:\n    temp_dir = tempfile.TemporaryDirectory()\n    args.output = temp_dir.name\n    args.max_updates = 0\nusing_amp = False\nif args.amp:\n    using_amp = True\n    amp.init()\nconsole_level = None\nif args.horovod:\n    if ((horovod_mpi.hvd is None) or (horovod_mpi.MPI is None)):\n        raise RuntimeError('Horovod training requires the following packages to be installed: horovod mpi4py')\n    if (C.HOROVOD_HIERARCHICAL_ALLREDUCE not in os.environ):\n        os.environ[C.HOROVOD_HIERARCHICAL_ALLREDUCE] = '1'\n    if (C.HOROVOD_HIERARCHICAL_ALLGATHER not in os.environ):\n        os.environ[C.HOROVOD_HIERARCHICAL_ALLGATHER] = '1'\n    horovod_mpi.hvd.init()\n    if (horovod_mpi.hvd.rank() > 0):\n        args.output = os.path.join(args.output, C.HOROVOD_SECONDARY_WORKERS_DIRNAME, str(horovod_mpi.hvd.rank()))\n        args.keep_last_params = 1\n        if args.quiet_secondary_workers:\n            args.quiet = True\n        console_level = args.loglevel_secondary_workers\ncheck_arg_compatibility(args)\noutput_folder = os.path.abspath(args.output)\nresume_training = check_resume(args, output_folder)\nsetup_main_logger(file_logging=(not args.no_logfile), console=(not args.quiet), path=os.path.join(output_folder, C.LOG_NAME), level=args.loglevel, console_level=console_level)\nutils.log_basic_info(args)\narguments.save_args(args, os.path.join(output_folder, C.ARGS_STATE_NAME))\n(max_seq_len_source, max_seq_len_target) = args.max_seq_len\nmax_seq_len_source = (max_seq_len_source + C.SPACE_FOR_XOS)\nmax_seq_len_target = (max_seq_len_target + C.SPACE_FOR_XOS)\nlogger.info('Adjusting maximum length to reserve space for a BOS\/EOS marker. New maximum length: (%d, %d)', max_seq_len_source, max_seq_len_target)\nwith ExitStack() as exit_stack:\n    context = utils.determine_context(device_ids=args.device_ids, use_cpu=args.use_cpu, disable_device_locking=args.disable_device_locking, lock_dir=args.lock_dir, exit_stack=exit_stack)\n    if (args.batch_type == C.BATCH_TYPE_SENTENCE):\n        check_condition(((args.batch_size % len(context)) == 0), ('When using multiple devices the batch size must be divisible by the number of devices. Choose a batch size that is a multiple of %d.' % len(context)))\n    logger.info('Training Device(s): %s', ', '.join((str(c) for c in context)))\n    utils.seed_rngs(args.seed, ctx=context)\n    (train_iter, eval_iter, config_data, source_vocabs, target_vocabs) = create_data_iters_and_vocabs(args=args, max_seq_len_source=max_seq_len_source, max_seq_len_target=max_seq_len_target, shared_vocab=use_shared_vocab(args), resume_training=resume_training, output_folder=output_folder)\n    if (max_seq_len_source != config_data.max_seq_len_source):\n        logger.info('Maximum source length determined by prepared data. Using %d instead of %d', config_data.max_seq_len_source, max_seq_len_source)\n        max_seq_len_source = config_data.max_seq_len_source\n    if (max_seq_len_target != config_data.max_seq_len_target):\n        logger.info('Maximum target length determined by prepared data. Using %d instead of %d', config_data.max_seq_len_target, max_seq_len_target)\n        max_seq_len_target = config_data.max_seq_len_target\n    if (not resume_training):\n        vocab.save_source_vocabs(source_vocabs, output_folder)\n        vocab.save_target_vocabs(target_vocabs, output_folder)\n    source_vocab_sizes = [len(v) for v in source_vocabs]\n    target_vocab_sizes = [len(v) for v in target_vocabs]\n    logger.info('Vocabulary sizes: source=[%s] target=[%s]', '|'.join([str(size) for size in source_vocab_sizes]), '|'.join([str(size) for size in target_vocab_sizes]))\n    model_config = create_model_config(args=args, source_vocab_sizes=source_vocab_sizes, target_vocab_sizes=target_vocab_sizes, max_seq_len_source=max_seq_len_source, max_seq_len_target=max_seq_len_target, config_data=config_data)\n    training_model = model.SockeyeModel(model_config)\n    trainer_config = training.TrainerConfig(output_dir=args.output, early_stopping_metric=args.optimized_metric, max_params_files_to_keep=args.keep_last_params, keep_initializations=args.keep_initializations, max_params_files_to_cache=args.cache_last_best_params, cache_strategy=args.cache_strategy, cache_metric=args.cache_metric, checkpoint_interval=args.checkpoint_interval, max_num_checkpoint_not_improved=args.max_num_checkpoint_not_improved, checkpoint_improvement_threshold=args.checkpoint_improvement_threshold, max_checkpoints=args.max_checkpoints, min_samples=args.min_samples, max_samples=args.max_samples, min_updates=args.min_updates, max_updates=args.max_updates, min_epochs=args.min_num_epochs, max_epochs=args.max_num_epochs, max_seconds=args.max_seconds, update_interval=args.update_interval, stop_training_on_decoder_failure=args.stop_training_on_decoder_failure)\n    if ((trainer_config.min_epochs is not None) and (trainer_config.max_epochs is not None)):\n        check_condition((trainer_config.min_epochs <= trainer_config.max_epochs), 'Minimum number of epochs must be smaller than maximum number of epochs')\n    optimizer_config = create_optimizer_config(args)\n    training_model.initialize(optimizer_config.initializer, ctx=context)\n    if (args.params is not None):\n        training_model.load_parameters(filename=args.params, ctx=context, allow_missing=(args.allow_missing_params or model_config.lhuc), ignore_extra=args.ignore_extra_params, cast_dtype=True, dtype_source='current')\n    params = training_model.collect_params()\n    params = set_grad_req_for_fixed_params(config=model_config, params=params, fixed_param_names=args.fixed_param_names, fixed_param_strategy=args.fixed_param_strategy)\n    if (horovod_mpi.using_horovod() and (not resume_training)):\n        for ctx in context:\n            with mx.Context(ctx):\n                horovod_mpi.hvd.broadcast_parameters(params, root_rank=0)\n    if (args.dtype == C.DTYPE_FP16):\n        training_model.cast(C.DTYPE_FP16)\n    utils.log_parameters(params)\n    if (args.update_interval > 1):\n        for (name, param) in params.items():\n            if (param.grad_req != 'null'):\n                param.grad_req = 'add'\n    kvstore = mx.kvstore.create(args.kvstore)\n    if horovod_mpi.using_horovod():\n        gluon_trainer = horovod_mpi.hvd.DistributedTrainer(params, optimizer_config.name, optimizer_config.params)\n    else:\n        gluon_trainer = gluon.Trainer(params, optimizer_config.name, optimizer_config.params, kvstore=kvstore, update_on_kvstore=(False if using_amp else None))\n    if using_amp:\n        amp.init_trainer(gluon_trainer)\n        gluon_trainer._amp_loss_scaler._scale_seq_len = args.amp_scale_interval\n    losses = create_losses(args, all_num_classes=target_vocab_sizes)\n    hybridize = (not args.no_hybridization)\n    if hybridize:\n        training_model.hybridize(static_alloc=True)\n        if (not using_amp):\n            for lf in losses:\n                lf.hybridize(static_alloc=True)\n    trainer = training.GluonEarlyStoppingTrainer(config=trainer_config, optimizer_config=optimizer_config, sockeye_model=training_model, trainer=gluon_trainer, loss_functions=losses, context=context, dtype=args.dtype, using_amp=using_amp, custom_metrics_logger=custom_metrics_logger, checkpoint_callback=checkpoint_callback)\n    cp_decoder = create_checkpoint_decoder(args, exit_stack, context, training_model, source_vocabs, target_vocabs, hybridize=hybridize)\n    training_state = trainer.fit(train_iter=train_iter, validation_iter=eval_iter, checkpoint_decoder=cp_decoder)\n    return training_state\n"}
{"label_name":"process","label":2,"method_name":"_process_list_value","method":"\n'Update results_dictionary from a list of values.\\n\\n  Used to update results_dictionary to be returned by parse_values when\\n  encountering a clause with a list RHS (e.g.  \"arr=[1,2,3]\".)\\n\\n  Mutates results_dictionary.\\n\\n  Args:\\n    name: Name of variable in assignment (\"arr\").\\n    parse_fn: Function for parsing individual values.\\n    var_type: Type of named variable.\\n    m_dict: Dictionary constructed from regex parsing.\\n      m_dict[\\'val\\']: RHS value (scalar)\\n    values: Full expression being parsed\\n    results_dictionary: The dictionary being updated for return by the parsing\\n      function.\\n\\n  Raises:\\n    ValueError: If the name has an index or the values cannot be parsed.\\n  '\nif (m_dict['index'] is not None):\n    raise ValueError('Assignment of a list to a list index.')\nelements = filter(None, re.split('[ ,]', m_dict['vals']))\nif (name in results_dictionary):\n    raise _reuse_fail(name, values)\ntry:\n    results_dictionary[name] = [parse_fn(e) for e in elements]\nexcept ValueError:\n    _parse_fail(name, var_type, m_dict['vals'], values)\n"}
{"label_name":"forward","label":3,"method_name":"forwardOnly","method":"\nreturn left.dir().is_forward()\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n' Preprocess an image by subtracting the ImageNet mean.\\n\\n    Args\\n        x: np.array of shape (None, None, 3) or (3, None, None).\\n        mode: One of \"caffe\" or \"tf\".\\n            - caffe: will zero-center each color channel with\\n                respect to the ImageNet dataset, without scaling.\\n            - tf: will scale pixels between -1 and 1, sample-wise.\\n\\n    Returns\\n        The input with the ImageNet mean subtracted.\\n    '\nx = x.astype(np.float32)\nif (mode == 'tf'):\n    x \/= 127.5\n    x -= 1.0\nelif (mode == 'caffe'):\n    x -= [103.939, 116.779, 123.68]\nreturn x\n"}
{"label_name":"save","label":1,"method_name":"save_summary_in_file","method":"\ntext = wikipedia.summary(input)\nfile = open('Movies_Hollywood_data.txt', 'a')\nfile.write(text.encode('utf-8'))\nfile.write('\\n\\n')\nfile.close()\n"}
{"label_name":"train","label":0,"method_name":"train_models","method":"\n\ndef train(cfg_name, project_name):\n    from rasa_nlu.train import create_persistor\n    from rasa_nlu import training_data\n    cfg = config.load(cfg_name)\n    trainer = Trainer(cfg, component_builder)\n    training_data = training_data.load_data(data)\n    trainer.train(training_data)\n    trainer.persist('test_projects', project_name=project_name)\ntrain('sample_configs\/config_spacy.yml', 'test_project_spacy_sklearn')\ntrain('sample_configs\/config_mitie.yml', 'test_project_mitie')\ntrain('sample_configs\/config_mitie_sklearn.yml', 'test_project_mitie_sklearn')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nprint('-\u5f00\u59cb\u8bad\u7ec3')\n(test_data_buckets, train_data_buckets, train_buckets_scale, _) = _get_buckets()\nmodel = ChatBotModel(False, config.BATCH_SIZE)\nmodel.build_graph()\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    _check_restore_parameters(sess, saver)\n    iteration = model.global_step.eval()\n    learning_rate = model.learning_rate.eval()\n    total_loss = 0\n    while True:\n        skip_step = _get_skip_step(iteration)\n        bucket_id = _get_random_bucket(train_buckets_scale)\n        (encoder_inputs, decoder_inputs, decoder_masks) = data.get_batch(train_data_buckets[bucket_id], bucket_id, batch_size=config.BATCH_SIZE)\n        start = time.time()\n        (_, step_loss, _) = run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, False)\n        print('Iter {}:  loss{}, learning rate{}, time {}'.format(iteration, step_loss, learning_rate, (time.time() - start)))\n        total_loss += step_loss\n        iteration += 1\n        if ((iteration % skip_step) == 0):\n            print('Saved at iter {}: loss {}, time {}'.format(iteration, (total_loss \/ skip_step), (time.time() - start)))\n            start = time.time()\n            total_loss = 0\n            saver.save(sess, os.path.join(config.CPT_PATH, 'chatbot'))\n            if ((iteration % (10 * skip_step)) == 0):\n                _eval_test_set(sess, model, test_data_buckets)\n                start = time.time()\n            sys.stdout.flush()\n"}
{"label_name":"predict","label":4,"method_name":"create_masked_lm_predictions","method":"\n'Creates the predictions for the masked LM objective.'\ncand_indexes = []\nfor (i, token) in enumerate(tokens):\n    if (token in [_CLS_TOKEN, _SEP_TOKEN]):\n        continue\n    if (whole_word_mask and (len(cand_indexes) >= 1) and (not tokenizer.is_first_subword(token))):\n        cand_indexes[(- 1)].append(i)\n    else:\n        cand_indexes.append([i])\nrandom.shuffle(cand_indexes)\noutput_tokens = list(tokens)\nnum_to_predict = min(max_predictions_per_seq, max(1, int(round((len(tokens) * masked_lm_prob)))))\nmasked_lms = []\ncovered_indexes = set()\nfor index_set in cand_indexes:\n    if (len(masked_lms) >= num_to_predict):\n        break\n    if ((len(masked_lms) + len(index_set)) > num_to_predict):\n        continue\n    is_any_index_covered = False\n    for index in index_set:\n        if (index in covered_indexes):\n            is_any_index_covered = True\n            break\n    if is_any_index_covered:\n        continue\n    for index in index_set:\n        covered_indexes.add(index)\n        masked_token = None\n        if (random.random() < 0.8):\n            masked_token = _MASK_TOKEN\n        elif (random.random() < 0.5):\n            masked_token = tokens[index]\n        else:\n            masked_token = random.randint(0, (len(vocab) - 1))\n        output_tokens[index] = masked_token\n        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\nassert (len(masked_lms) <= num_to_predict)\nmasked_lms = sorted(masked_lms, key=(lambda x: x.index))\nmasked_lm_positions = []\nmasked_lm_labels = []\nfor p in masked_lms:\n    masked_lm_positions.append(p.index)\n    masked_lm_labels.append(p.label)\nreturn (output_tokens, masked_lm_positions, masked_lm_labels)\n"}
{"label_name":"save","label":1,"method_name":"save_gif","method":"\n'Save a GIF of a sequence of image frames.\\n\\n    Arguments:\\n        gif_path(str): The path where the GIF will be saved.\\n        frames(list): A list of image frames (numpy matrices).\\n    '\nprint('Saving GIF')\npatch = plt.imshow(frames[0])\nplt.axis('off')\n\ndef animate(i):\n    patch.set_data(frames[i])\nanim = FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\nanim.save(gif_path, fps=70, writer='imagemagick')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\ntf.summary.scalar('Total Loss', total_loss)\noptimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\nreturn optimizer.minimize(total_loss, global_step=global_step)\n"}
{"label_name":"process","label":2,"method_name":"process_text","method":"\nret = []\nfor (i, row) in enumerate(rows):\n    body = row['Body']\n    soup = bs4.BeautifulSoup(body, 'lxml')\n    for code in soup.find_all('code'):\n        code.clear()\n    text = soup.text\n    if ((i % 5000) == 0):\n        print(i, ((i * 100) \/ len(rows)))\n    length = len(text.split())\n    if (length < 100):\n        continue\n    row['Body'] = text\n    ret.append(row)\nreturn ret\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nreturn model(x, w_best)\n"}
{"label_name":"train","label":0,"method_name":"train_main","method":"\nif (use_cuda and (not fluid.core.is_compiled_with_cuda())):\n    return\nplace = (fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace())\ncontext = encoder(is_sparse)\nrnn_out = decoder_train(context, is_sparse)\nlabel = pd.data(name='target_language_next_word', shape=[1], dtype='int64', lod_level=1)\ncost = pd.cross_entropy(input=rnn_out, label=label)\navg_cost = pd.mean(cost)\noptimizer = fluid.optimizer.Adagrad(learning_rate=0.0001, regularization=fluid.regularizer.L2DecayRegularizer(regularization_coeff=0.1))\noptimizer.minimize(avg_cost)\ntrain_data = paddle.batch(paddle.reader.shuffle(paddle.dataset.wmt14.train(dict_size), buf_size=1000), batch_size=batch_size)\nfeed_order = ['src_word_id', 'target_language_word', 'target_language_next_word']\nexe = Executor(place)\n\ndef train_loop(main_program):\n    exe.run(framework.default_startup_program())\n    feed_list = [main_program.global_block().var(var_name) for var_name in feed_order]\n    feeder = fluid.DataFeeder(feed_list, place)\n    batch_id = 0\n    for pass_id in range(1):\n        for data in train_data():\n            outs = exe.run(main_program, feed=feeder.feed(data), fetch_list=[avg_cost])\n            avg_cost_val = np.array(outs[0])\n            print(((((('pass_id=' + str(pass_id)) + ' batch=') + str(batch_id)) + ' avg_cost=') + str(avg_cost_val)))\n            if (batch_id > 3):\n                break\n            batch_id += 1\nif is_local:\n    train_loop(framework.default_main_program())\nelse:\n    port = os.getenv('PADDLE_PSERVER_PORT', '6174')\n    pserver_ips = os.getenv('PADDLE_PSERVER_IPS')\n    eplist = []\n    for ip in pserver_ips.split(','):\n        eplist.append(':'.join([ip, port]))\n    pserver_endpoints = ','.join(eplist)\n    trainers = int(os.getenv('PADDLE_TRAINERS'))\n    current_endpoint = ((os.getenv('POD_IP') + ':') + port)\n    trainer_id = int(os.getenv('PADDLE_TRAINER_ID'))\n    training_role = os.getenv('PADDLE_TRAINING_ROLE', 'TRAINER')\n    t = fluid.DistributeTranspiler()\n    t.transpile(trainer_id, pservers=pserver_endpoints, trainers=trainers)\n    if (training_role == 'PSERVER'):\n        pserver_prog = t.get_pserver_program(current_endpoint)\n        pserver_startup = t.get_startup_program(current_endpoint, pserver_prog)\n        exe.run(pserver_startup)\n        exe.run(pserver_prog)\n    elif (training_role == 'TRAINER'):\n        train_loop(t.get_trainer_program())\n"}
{"label_name":"save","label":1,"method_name":"saveBatchResults","method":"\nsaveBatchResultByField(results, 'ce')\nsaveBatchResultByField(results, 'val_ce')\nsaveBatchResultByField(results, 'accs')\nsaveBatchResultByField(results, 'val_accs')\n"}
{"label_name":"train","label":0,"method_name":"train_until_thresh","method":"\nfor j in xrange(MAX_TRAINING_LOOPS):\n    for _ in xrange(ITERS_PER_ACTOR):\n        s.run(ac.bob_optimizer)\n    for _ in xrange((ITERS_PER_ACTOR * EVE_MULTIPLIER)):\n        s.run(ac.eve_optimizer)\n    if ((j % PRINT_EVERY) == 0):\n        (bob_avg_loss, eve_avg_loss) = doeval(s, ac, EVAL_BATCHES, j)\n        if ((bob_avg_loss < BOB_LOSS_THRESH) and (eve_avg_loss > EVE_LOSS_THRESH)):\n            print('Target losses achieved.')\n            return True\nreturn False\n"}
{"label_name":"predict","label":4,"method_name":"_create_prediction_request","method":"\nfrom kryptoflow.managers.dashboard import DashBoardManager\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = DashBoardManager.TF_MODEL_NAME\nrequest.model_spec.signature_name = DashBoardManager.TF_MODEL_SIGNATURE_NAME\nrequest.inputs[DashBoardManager.TF_MODEL_INPUTS_KEY].CopyFrom(tf.contrib.util.make_tensor_proto(image, dtype=tf.float32))\nreturn request\n"}
{"label_name":"process","label":2,"method_name":"process_fermi_hubbard_hopping","method":"\nfor c in constituents:\n    if (c in ['down', 'up']):\n        spin_type = c\n    elif (c[0] == 'd'):\n        num_sites = int(c[1:])\n    else:\n        sites = [int(s) for s in c.split('h')]\ni_idx = ((2 * (sites[0] - 1)) - 2)\nj_idx = ((2 * (sites[1] - 1)) - 2)\nif (spin_type == 'down'):\n    i_idx = (2 * (sites[0] - 1))\n    j_idx = (2 * (sites[1] - 1))\nelif (spin_type == 'up'):\n    i_idx = ((2 * sites[0]) - 1)\n    j_idx = ((2 * sites[1]) - 1)\ndimensional_description = '{}'.format(((2 * num_sites) - 1))\ndimensional_fermion_op = FermionOperator(dimensional_description)\nhopping_term = FermionOperator(((i_idx, 1), (j_idx, 0)))\nhopping_term += FermionOperator(((j_idx, 1), (i_idx, 0)))\nhopping_term += dimensional_fermion_op\nmtx = (jordan_wigner_mtx(hopping_term) - jordan_wigner_mtx(dimensional_fermion_op))\nreturn np.array(mtx)\n"}
{"label_name":"process","label":2,"method_name":"pre_process_dialouge","method":"\nprint('Extracting file')\nstart = time.time()\ndelete_all_file_dir(pre_processed_dir, type_d)\n(dialogue_file1, dialogue_file2) = dialouge_seperator_cornell_movie(raw_dialogue_file_path, dialogue_file1, dialogue_file2, symbol_seq, max_dialouge_count, max_len)\n(train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2) = train_test_split(dialogue_file1, dialogue_file2, max_dialouge_count_train_test, train_percentile, train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2, max_test_dev_count)\nend = time.time()\ntime_lib.elapsed_time(start, end)\nreturn (train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2)\n"}
{"label_name":"train","label":0,"method_name":"run_train_translate","method":"\n'\\n    Train a model and translate a test set. Returns the updated data dictionary containing paths to translation outputs\\n    and scores.\\n\\n    :param train_params: Command line args for model training.\\n    :param translate_params: First command line args for translation.\\n    :param data: Dictionary containing test data\\n    :param use_prepared_data: Whether to use the prepared data functionality.\\n    :param max_seq_len: The maximum sequence length.\\n    :param seed: The seed used for training.\\n    :return: Data dictionary, updated with translation outputs and scores\\n    '\nwork_dir = os.path.join(data['work_dir'], 'train_translate')\ndata['model'] = os.path.join(work_dir, 'model')\nif use_prepared_data:\n    data['train_prepared'] = os.path.join(work_dir, 'prepared_data')\n    prepare_params = '{} {}'.format(sockeye.prepare_data.__file__, PREPARE_DATA_COMMON.format(train_source=data['train_source'], train_target=data['train_target'], output=data['train_prepared'], max_len=max_seq_len))\n    if ('train_source_factors' in data):\n        prepare_params += TRAIN_WITH_SOURCE_FACTORS_COMMON.format(source_factors=' '.join(data['train_source_factors']))\n    if ('train_target_factors' in data):\n        prepare_params += TRAIN_WITH_TARGET_FACTORS_COMMON.format(target_factors=' '.join(data['train_target_factors']))\n    if ('--weight-tying-type src_trg' in train_params):\n        prepare_params += ' --shared-vocab'\n    logger.info('Preparing data with parameters %s.', prepare_params)\n    with patch.object(sys, 'argv', prepare_params.split()):\n        sockeye.prepare_data.main()\n    params = '{} {} {}'.format(sockeye.train.__file__, TRAIN_PARAMS_PREPARED_DATA_COMMON.format(prepared_data=data['train_prepared'], dev_source=data['dev_source'], dev_target=data['dev_target'], model=data['model'], max_len=max_seq_len), train_params)\n    if ('dev_source_factors' in data):\n        params += DEV_WITH_SOURCE_FACTORS_COMMON.format(dev_source_factors=' '.join(data['dev_source_factors']))\n    if ('dev_target_factors' in data):\n        params += DEV_WITH_TARGET_FACTORS_COMMON.format(dev_target_factors=' '.join(data['dev_target_factors']))\n    logger.info('Starting training with parameters %s.', train_params)\n    with patch.object(sys, 'argv', params.split()):\n        sockeye.train.main()\nelse:\n    params = '{} {} {}'.format(sockeye.train.__file__, TRAIN_PARAMS_COMMON.format(train_source=data['train_source'], train_target=data['train_target'], dev_source=data['dev_source'], dev_target=data['dev_target'], model=data['model'], max_len=max_seq_len, seed=seed), train_params)\n    if ('train_source_factors' in data):\n        params += TRAIN_WITH_SOURCE_FACTORS_COMMON.format(source_factors=' '.join(data['train_source_factors']))\n    if ('train_target_factors' in data):\n        params += TRAIN_WITH_TARGET_FACTORS_COMMON.format(target_factors=' '.join(data['train_target_factors']))\n    if ('dev_source_factors' in data):\n        params += DEV_WITH_SOURCE_FACTORS_COMMON.format(dev_source_factors=' '.join(data['dev_source_factors']))\n    if ('dev_target_factors' in data):\n        params += DEV_WITH_TARGET_FACTORS_COMMON.format(dev_target_factors=' '.join(data['dev_target_factors']))\n    logger.info('Starting training with parameters %s.', train_params)\n    with patch.object(sys, 'argv', params.split()):\n        sockeye.train.main()\nttable_path = os.path.join(data['work_dir'], 'ttable')\ngenerate_fast_align_lex(ttable_path)\nlexicon_path = os.path.join(data['work_dir'], 'lexicon')\nparams = '{} {}'.format(sockeye.lexicon.__file__, LEXICON_CREATE_PARAMS_COMMON.format(input=ttable_path, model=data['model'], topk=20, lexicon=lexicon_path))\nwith patch.object(sys, 'argv', params.split()):\n    sockeye.lexicon.main()\ndata['lexicon'] = lexicon_path\ndata['test_output'] = os.path.join(work_dir, 'test.out')\nparams = '{} {} {}'.format(sockeye.translate.__file__, TRANSLATE_PARAMS_COMMON.format(model=data['model'], input=data['test_source'], output=data['test_output']), translate_params)\nif ('test_source_factors' in data):\n    params += TRANSLATE_WITH_FACTORS_COMMON.format(input_factors=' '.join(data['test_source_factors']))\nlogger.info('Translating with params %s', params)\nwith patch.object(sys, 'argv', params.split()):\n    sockeye.translate.main()\nwith open(data['test_source']) as inputs:\n    data['test_inputs'] = [line.strip() for line in inputs]\nwith open(data['test_target'], 'r') as ref:\n    data['test_targets'] = [line.strip() for line in ref]\ndata['test_outputs'] = collect_translate_output_and_scores(data['test_output'])\nassert (len(data['test_inputs']) == len(data['test_targets']) == len(data['test_outputs']))\nreturn data\n"}
{"label_name":"save","label":1,"method_name":"compute_and_save_video_metrics","method":"\n'Compute and saves the video metrics.'\n(statistics, all_results) = compute_video_metrics_from_png_files(output_dirs, problem_name, video_length, frame_shape)\nfor (results, output_dir) in zip(all_results, output_dirs):\n    save_results(results, output_dir, problem_name)\nparent_dir = os.path.join(output_dirs[0], os.pardir)\nfinal_dir = os.path.join(parent_dir, 'decode')\ntf.gfile.MakeDirs(parent_dir)\nsave_results(statistics, final_dir, problem_name)\n"}
{"label_name":"predict","label":4,"method_name":"predict_future","method":"\ndata = urllib.urlencode({'hyper': hypers, 'loss': loss})\nreq = urllib2.Request(predict_loss_url, data)\nresponse = urllib2.urlopen(req)\nres = json.loads(response.read())\nmore_index = res['msg']\nreturn more_index\n"}
{"label_name":"save","label":1,"method_name":"_save","method":"\n\"\\n    Saves the image as a series of PNG files,\\n    that are then converted to a .icns file\\n    using the macOS command line utility 'iconutil'.\\n\\n    macOS only.\\n    \"\nif hasattr(fp, 'flush'):\n    fp.flush()\niconset = tempfile.mkdtemp('.iconset')\nprovided_images = {im.width: im for im in im.encoderinfo.get('append_images', [])}\nlast_w = None\nfor w in [16, 32, 128, 256, 512]:\n    prefix = 'icon_{}x{}'.format(w, w)\n    first_path = os.path.join(iconset, (prefix + '.png'))\n    if (last_w == w):\n        shutil.copyfile(second_path, first_path)\n    else:\n        im_w = provided_images.get(w, im.resize((w, w), Image.LANCZOS))\n        im_w.save(first_path)\n    second_path = os.path.join(iconset, (prefix + '@2x.png'))\n    im_w2 = provided_images.get((w * 2), im.resize(((w * 2), (w * 2)), Image.LANCZOS))\n    im_w2.save(second_path)\n    last_w = (w * 2)\nfrom subprocess import Popen, PIPE, CalledProcessError\nconvert_cmd = ['iconutil', '-c', 'icns', '-o', filename, iconset]\nwith open(os.devnull, 'wb') as devnull:\n    convert_proc = Popen(convert_cmd, stdout=PIPE, stderr=devnull)\nconvert_proc.stdout.close()\nretcode = convert_proc.wait()\nshutil.rmtree(iconset)\nif retcode:\n    raise CalledProcessError(retcode, convert_cmd)\n"}
{"label_name":"train","label":0,"method_name":"split_train_valid","method":"\n'\\n    k-fold\u4ea4\u53c9\u9a8c\u8bc1,\u9ed8\u8ba4k=10\\n    df_train:\u8bad\u7ec3\u6570\u636e\\n    '\n(X_train, X_vali, y_train, y_vali) = train_test_split(df_train[features], df_train[label], test_size=test_size, random_state=40000)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_vali, label=y_vali)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\nreturn (dtrain, dvalid, watchlist)\n"}
{"label_name":"save","label":1,"method_name":"save_fold","method":"\nfpath = get_labels_fpath(fold['name'])\nreturn utils.files.save_json(fpath, fold)\n"}
{"label_name":"predict","label":4,"method_name":"cross_val_predict","method":"\n\"Generate cross-validated estimates for each input data point\\n\\n    .. deprecated:: 0.18\\n        This module will be removed in 0.20.\\n        Use :func:`sklearn.model_selection.cross_val_predict` instead.\\n\\n    Read more in the :ref:`User Guide <cross_validation>`.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit' and 'predict'\\n        The object to use to fit the data.\\n\\n    X : array-like\\n        The data to fit. Can be, for example a list, or an array at least 2d.\\n\\n    y : array-like, optional, default: None\\n        The target variable to try to predict in the case of\\n        supervised learning.\\n\\n    cv : int, cross-validation generator or an iterable, optional\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 3-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - An object to be used as a cross-validation generator.\\n        - An iterable yielding train\/test splits.\\n\\n        For integer\/None inputs, if the estimator is a classifier and ``y`` is\\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\\n        other cases, :class:`KFold` is used.\\n\\n        Refer :ref:`User Guide <cross_validation>` for the various\\n        cross-validation strategies that can be used here.\\n\\n    n_jobs : integer, optional\\n        The number of CPUs to use to do the computation. -1 means\\n        'all CPUs'.\\n\\n    verbose : integer, optional\\n        The verbosity level.\\n\\n    fit_params : dict, optional\\n        Parameters to pass to the fit method of the estimator.\\n\\n    pre_dispatch : int, or string, optional\\n        Controls the number of jobs that get dispatched during parallel\\n        execution. Reducing this number can be useful to avoid an\\n        explosion of memory consumption when more jobs get dispatched\\n        than CPUs can process. This parameter can be:\\n\\n            - None, in which case all the jobs are immediately\\n              created and spawned. Use this for lightweight and\\n              fast-running jobs, to avoid delays due to on-demand\\n              spawning of the jobs\\n\\n            - An int, giving the exact number of total jobs that are\\n              spawned\\n\\n            - A string, giving an expression as a function of n_jobs,\\n              as in '2*n_jobs'\\n\\n    Returns\\n    -------\\n    preds : ndarray\\n        This is the result of calling 'predict'\\n\\n    Examples\\n    --------\\n    >>> from sklearn import datasets, linear_model\\n    >>> from sklearn.cross_validation import cross_val_predict\\n    >>> diabetes = datasets.load_diabetes()\\n    >>> X = diabetes.data[:150]\\n    >>> y = diabetes.target[:150]\\n    >>> lasso = linear_model.Lasso()\\n    >>> y_pred = cross_val_predict(lasso, X, y)\\n    \"\n(X, y) = indexable(X, y)\ncv = check_cv(cv, X, y, classifier=is_classifier(estimator))\nparallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\npreds_blocks = parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params) for (train, test) in cv))\npreds = [p for (p, _) in preds_blocks]\nlocs = np.concatenate([loc for (_, loc) in preds_blocks])\nif (not _check_is_partition(locs, _num_samples(X))):\n    raise ValueError('cross_val_predict only works for partitions')\ninv_locs = np.empty(len(locs), dtype=int)\ninv_locs[locs] = np.arange(len(locs))\nif sp.issparse(preds[0]):\n    preds = sp.vstack(preds, format=preds[0].format)\nelse:\n    preds = np.concatenate(preds)\nreturn preds[inv_locs]\n"}
{"label_name":"save","label":1,"method_name":"save_all_obj","method":"\nglobal page_doc_map\nglobal doc_page_map\nglobal page_ref_count\nglobal doc_count\nglobal link_queue\nsave_obj(page_doc_map, 'url_doc_map', 'value', 'auto')\nsave_obj(doc_page_map, 'doc_url_map', 'key', 'auto')\nsave_obj_no_sort(doc_count, 'doc_count')\nsave_obj_no_sort_w(link_queue, 'link_queue')\nsave_obj(page_ref_count, 'page_ref_count', 'value', 'reverse')\nprint('saved objects')\n"}
{"label_name":"predict","label":4,"method_name":"find_in_prediction","method":"\ntry:\n    (targets, scores) = zip(*prediction)\n    return targets.index(target)\nexcept ValueError:\n    return (- 1)\n"}
{"label_name":"save","label":1,"method_name":"imgsave","method":"\n'\\n    Write an image to given file path\\n    inputs:\\n        img - numpy img array\\n        img_path - path to write\\n    '\nimsave(img_path, img)\n"}
{"label_name":"predict","label":4,"method_name":"predict_sr","method":"\nstats_vector = np.array([get_vector_gamestats(player, 'us')])\nX = scaler_X.transform(stats_vector)\ny_matrix = model.predict(X)\nsr = np.squeeze(scaler_y.inverse_transform(y_matrix))\nreturn int(sr)\n"}
{"label_name":"predict","label":4,"method_name":"_parallel_predict_regression","method":"\n'Private function used to compute predictions within a job.'\nreturn sum((estimator.predict(X[:, features]) for (estimator, features) in zip(estimators, estimators_features)))\n"}
{"label_name":"train","label":0,"method_name":"run_train_epoch","method":"\ntotal_mmd2 = 0\ntotal_obj = 0\nn_batches = 0\nbatches = itertools.izip(iterate_minibatches(X_train, batchsize=batchsize, shuffle=True), iterate_minibatches(Y_train, batchsize=batchsize, shuffle=True))\nfor ((Xbatch,), (Ybatch,)) in batches:\n    (mmd2, obj) = train_fn(Xbatch, Ybatch)\n    assert np.isfinite(mmd2)\n    assert np.isfinite(obj)\n    total_mmd2 += mmd2\n    total_obj += obj\n    n_batches += 1\nreturn ((total_mmd2 \/ n_batches), (total_obj \/ n_batches))\n"}
{"label_name":"predict","label":4,"method_name":"make_prediction","method":"\n'Make some classification predictions on a toy dataset using a SVC\\n\\n    If binary is True restrict to a binary classification problem instead of a\\n    multiclass classification problem\\n    '\nif (dataset is None):\n    dataset = datasets.load_iris()\nX = dataset.data\ny = dataset.target\nif binary:\n    (X, y) = (X[(y < 2)], y[(y < 2)])\n(n_samples, n_features) = X.shape\np = np.arange(n_samples)\nrng = check_random_state(37)\nrng.shuffle(p)\n(X, y) = (X[p], y[p])\nhalf = int((n_samples \/ 2))\nrng = np.random.RandomState(0)\nX = np.c_[(X, rng.randn(n_samples, (200 * n_features)))]\nclf = svm.SVC(kernel='linear', probability=True, random_state=0)\nprobas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])\nif binary:\n    probas_pred = probas_pred[:, 1]\ny_pred = clf.predict(X[half:])\ny_true = y[half:]\nreturn (y_true, y_pred, probas_pred)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n'\\n    Training a CapsuleNet\\n    :param model: the CapsuleNet model\\n    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\\n    :param args: arguments\\n    :return: The trained model\\n    '\n((x_train, y_train), (x_test, y_test)) = data\nlog = callbacks.CSVLogger((args.save_dir + '\/log.csv'))\ntb = callbacks.TensorBoard(log_dir=(args.save_dir + '\/tensorboard-logs'), batch_size=args.batch_size, histogram_freq=args.debug)\ncheckpoint = callbacks.ModelCheckpoint((args.save_dir + '\/weights-{epoch:02d}.h5'), save_best_only=True, save_weights_only=True, verbose=1)\nlr_decay = callbacks.LearningRateScheduler(schedule=(lambda epoch: (args.lr * (0.9 ** epoch))))\nmodel.compile(optimizer=optimizers.Adam(lr=args.lr), loss=[margin_loss, 'mse'], loss_weights=[1.0, args.lam_recon], metrics={'out_caps': 'accuracy'})\n'\\n    # Training without data augmentation:\\n    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\\n              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\\n    '\n\ndef train_generator(x, y, batch_size, shift_fraction=0.0):\n    train_datagen = ImageDataGenerator(width_shift_range=shift_fraction, height_shift_range=shift_fraction)\n    generator = train_datagen.flow(x, y, batch_size=batch_size)\n    while 1:\n        (x_batch, y_batch) = generator.next()\n        (yield ([x_batch, y_batch], [y_batch, x_batch]))\nmodel.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction), steps_per_epoch=int((y_train.shape[0] \/ args.batch_size)), epochs=args.epochs, validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\nmodel.save_weights((args.save_dir + '\/trained_model.h5'))\nprint((\"Trained model saved to '%s\/trained_model.h5'\" % args.save_dir))\nfrom networks.capsulenet.helper_function import plot_log\nplot_log((args.save_dir + '\/log.csv'), show=True)\nreturn model\n"}
{"label_name":"save","label":1,"method_name":"save_matrix","method":"\nnp.savez_compressed(f, data=m.data, indices=m.indices, indptr=m.indptr, shape=m.shape)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n'Train a translation model.'\nlog_device_placement = hparams.log_device_placement\nout_dir = hparams.out_dir\nnum_train_steps = hparams.num_train_steps\nsteps_per_stats = hparams.steps_per_stats\nsteps_per_external_eval = hparams.steps_per_external_eval\nsteps_per_eval = (10 * steps_per_stats)\navg_ckpts = hparams.avg_ckpts\nif (not steps_per_external_eval):\n    steps_per_external_eval = (5 * steps_per_eval)\nif (not hparams.attention):\n    model_creator = nmt_model.Model\nelif ((hparams.encoder_type == 'gnmt') or (hparams.attention_architecture in ['gnmt', 'gnmt_v2'])):\n    model_creator = gnmt_model.GNMTModel\nelif (hparams.attention_architecture == 'standard'):\n    model_creator = attention_model.AttentionModel\nelse:\n    raise ValueError(('Unknown attention architecture %s' % hparams.attention_architecture))\ntrain_model = model_helper.create_train_model(model_creator, hparams, scope)\neval_model = model_helper.create_eval_model(model_creator, hparams, scope)\ninfer_model = model_helper.create_infer_model(model_creator, hparams, scope)\ndev_src_file = ('%s.%s' % (hparams.dev_prefix, hparams.src))\ndev_tgt_file = ('%s.%s' % (hparams.dev_prefix, hparams.tgt))\nsample_src_data = inference.load_data(dev_src_file)\nsample_tgt_data = inference.load_data(dev_tgt_file)\nsummary_name = 'train_log'\nmodel_dir = hparams.out_dir\nlog_file = os.path.join(out_dir, ('log_%d' % time.time()))\nlog_f = tf.gfile.GFile(log_file, mode='a')\nutils.print_out(('# log_file=%s' % log_file), log_f)\nconfig_proto = utils.get_config_proto(log_device_placement=log_device_placement, num_intra_threads=hparams.num_intra_threads, num_inter_threads=hparams.num_inter_threads)\ntrain_sess = tf.Session(target=target_session, config=config_proto, graph=train_model.graph)\neval_sess = tf.Session(target=target_session, config=config_proto, graph=eval_model.graph)\ninfer_sess = tf.Session(target=target_session, config=config_proto, graph=infer_model.graph)\nwith train_model.graph.as_default():\n    (loaded_train_model, global_step) = model_helper.create_or_load_model(train_model.model, model_dir, train_sess, 'train')\nsummary_writer = tf.summary.FileWriter(os.path.join(out_dir, summary_name), train_model.graph)\nrun_full_eval(model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams, summary_writer, sample_src_data, sample_tgt_data, avg_ckpts)\nlast_stats_step = global_step\nlast_eval_step = global_step\nlast_external_eval_step = global_step\n(stats, info, start_train_time) = before_train(loaded_train_model, train_model, train_sess, global_step, hparams, log_f)\nwhile (global_step < num_train_steps):\n    start_time = time.time()\n    try:\n        step_result = loaded_train_model.train(train_sess)\n        hparams.epoch_step += 1\n    except tf.errors.OutOfRangeError:\n        hparams.epoch_step = 0\n        utils.print_out(('# Finished an epoch, step %d. Perform external evaluation' % global_step))\n        run_sample_decode(infer_model, infer_sess, model_dir, hparams, summary_writer, sample_src_data, sample_tgt_data)\n        run_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer)\n        if avg_ckpts:\n            run_avg_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer, global_step)\n        train_sess.run(train_model.iterator.initializer, feed_dict={train_model.skip_count_placeholder: 0})\n        continue\n    (global_step, info['learning_rate'], step_summary) = update_stats(stats, start_time, step_result)\n    summary_writer.add_summary(step_summary, global_step)\n    if ((global_step - last_stats_step) >= steps_per_stats):\n        last_stats_step = global_step\n        is_overflow = process_stats(stats, info, global_step, steps_per_stats, log_f)\n        print_step_info('  ', global_step, info, _get_best_results(hparams), log_f)\n        if is_overflow:\n            break\n        stats = init_stats()\n    if ((global_step - last_eval_step) >= steps_per_eval):\n        last_eval_step = global_step\n        utils.print_out(('# Save eval, global step %d' % global_step))\n        utils.add_summary(summary_writer, global_step, 'train_ppl', info['train_ppl'])\n        loaded_train_model.saver.save(train_sess, os.path.join(out_dir, 'translate.ckpt'), global_step=global_step)\n        run_sample_decode(infer_model, infer_sess, model_dir, hparams, summary_writer, sample_src_data, sample_tgt_data)\n        run_internal_eval(eval_model, eval_sess, model_dir, hparams, summary_writer)\n    if ((global_step - last_external_eval_step) >= steps_per_external_eval):\n        last_external_eval_step = global_step\n        loaded_train_model.saver.save(train_sess, os.path.join(out_dir, 'translate.ckpt'), global_step=global_step)\n        run_sample_decode(infer_model, infer_sess, model_dir, hparams, summary_writer, sample_src_data, sample_tgt_data)\n        run_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer)\n        if avg_ckpts:\n            run_avg_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer, global_step)\nloaded_train_model.saver.save(train_sess, os.path.join(out_dir, 'translate.ckpt'), global_step=global_step)\n(result_summary, _, final_eval_metrics) = run_full_eval(model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams, summary_writer, sample_src_data, sample_tgt_data, avg_ckpts)\nprint_step_info('# Final, ', global_step, info, result_summary, log_f)\nutils.print_time('# Done training!', start_train_time)\nsummary_writer.close()\nutils.print_out('# Start evaluating saved best models.')\nfor metric in hparams.metrics:\n    best_model_dir = getattr(hparams, (('best_' + metric) + '_dir'))\n    summary_writer = tf.summary.FileWriter(os.path.join(best_model_dir, summary_name), infer_model.graph)\n    (result_summary, best_global_step, _) = run_full_eval(best_model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams, summary_writer, sample_src_data, sample_tgt_data)\n    print_step_info(('# Best %s, ' % metric), best_global_step, info, result_summary, log_f)\n    summary_writer.close()\n    if avg_ckpts:\n        best_model_dir = getattr(hparams, (('avg_best_' + metric) + '_dir'))\n        summary_writer = tf.summary.FileWriter(os.path.join(best_model_dir, summary_name), infer_model.graph)\n        (result_summary, best_global_step, _) = run_full_eval(best_model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams, summary_writer, sample_src_data, sample_tgt_data)\n        print_step_info(('# Averaged Best %s, ' % metric), best_global_step, info, result_summary, log_f)\n        summary_writer.close()\nreturn (final_eval_metrics, global_step)\n"}
{"label_name":"train","label":0,"method_name":"pretrain","method":"\ngen_data_loader = Gen_Data_loader(BATCH_SIZE)\ngen_data_loader.create_batches(positive_samples)\nresults = OrderedDict({'exp_name': PREFIX})\nprint('Start pre-training...')\nfor epoch in range(PRE_EPOCH_NUM):\n    print('pre-train epoch:', epoch)\n    loss = pre_train_epoch(sess, generator, gen_data_loader)\n    if ((epoch == 10) or ((epoch % 40) == 0)):\n        samples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\n        likelihood_data_loader.create_batches(samples)\n        test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n        print('\\t test_loss {}, train_loss {}'.format(test_loss, loss))\n        mm.compute_results(samples, train_samples, ord_dict, results)\nsamples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\nlikelihood_data_loader.create_batches(samples)\ntest_loss = target_loss(sess, target_lstm, likelihood_data_loader)\nsamples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\nlikelihood_data_loader.create_batches(samples)\nprint('Start training discriminator...')\nfor i in range(dis_alter_epoch):\n    print('epoch {}'.format(i))\n    (d_loss, acc) = train_discriminator()\nreturn\n"}
{"label_name":"train","label":0,"method_name":"train_and_eval","method":"\n'Train and eval newsgroup classification.\\n\\n    :param ngram_range: ngram range\\n    :param max_features: the number of maximum features\\n    :param max_df: max document frequency ratio\\n    :param C: Inverse of regularization strength for LogisticRegression\\n    :return: metrics\\n    '\ntrain_data = fetch_20newsgroups(subset='train')\ntest_data = fetch_20newsgroups(subset='test')\npipeline = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LogisticRegression(multi_class='auto'))])\nparams = {'tfidf__ngram_range': ngram_range, 'tfidf__max_features': max_features, 'tfidf__max_df': max_df, 'clf__C': C}\npipeline.set_params(**params)\nprint(pipeline.get_params().keys())\npipeline.fit(train_data.data, train_data.target)\nstart_time = time()\npredictions = pipeline.predict(test_data.data)\ninference_time = (time() - start_time)\navg_inference_time = ((1.0 * inference_time) \/ len(test_data.target))\nprint('Avg. inference time: {}'.format(avg_inference_time))\naccuracy = accuracy_score(test_data.target, predictions)\nrecall = recall_score(test_data.target, predictions, average='weighted')\nf1 = f1_score(test_data.target, predictions, average='weighted')\nmetrics = {'accuracy': accuracy, 'recall': recall, 'f1': f1}\nreturn metrics\n"}
{"label_name":"predict","label":4,"method_name":"create_masked_lm_predictions","method":"\n'Creates the predictions for the masked LM objective.'\ncand_indexes = [i for (i, tok) in enumerate(tokens) if (tok not in (cls_token_id, sep_token_id))]\noutput_tokens = list(tokens)\nrandom.shuffle(cand_indexes)\nnum_to_predict = min(args.max_predictions_per_seq, max(1, int(round((len(tokens) * args.masked_lm_prob)))))\nmlm_positions = []\nmlm_labels = []\ncovered_indexes = set()\nfor index in cand_indexes:\n    if (len(mlm_positions) >= num_to_predict):\n        break\n    if (index in covered_indexes):\n        continue\n    covered_indexes.add(index)\n    masked_token = None\n    if (random.random() < 0.8):\n        masked_token = mask_token_id\n    elif (random.random() < 0.5):\n        masked_token = tokens[index]\n    else:\n        masked_token = random.choice(non_special_ids)\n    output_tokens[index] = masked_token\n    mlm_positions.append(index)\n    mlm_labels.append(tokens[index])\nassert (len(mlm_positions) <= num_to_predict)\nassert (len(mlm_positions) == len(mlm_labels))\nreturn (output_tokens, mlm_positions, mlm_labels)\n"}
{"label_name":"train","label":0,"method_name":"_convert_training_info","method":"\n\"\\n    Convert the training information from the given Keras 'model' into the Core\\n    ML in 'builder'.\\n\\n    :param model: keras.model.Sequential\\n        The source Keras model.\\n    :param builder: NeutralNetworkBuilder\\n        The target model that will gain the loss and optimizer.\\n    :param output_features: list of tuples, (str, datatype)\\n        The set of tensor names that are output from the layers in the Keras\\n        model.\\n    \"\nbuilder.set_epochs(1)\nimport keras\ntry:\n    if ((model.loss == keras.losses.categorical_crossentropy) or (model.loss == 'categorical_crossentropy')):\n        builder.set_categorical_cross_entropy_loss(name='loss_layer', input=output_features[0][0])\n    elif ((model.loss == keras.losses.mean_squared_error) or (model.loss == 'mean_squared_error')):\n        builder.set_mean_squared_error_loss(name='loss_layer', input_feature=output_features[0])\n    else:\n        print(((('Models loss: ' + str(model.loss)) + ', vs Keras loss: ') + str(keras.losses.mean_squared_error)))\n        logging.warning((('Loss ' + str(model.loss)) + ' is not yet supported by Core ML. The loss layer will not be carried over. To train this model, you will need to manually add a supported loss layer.'))\nexcept AttributeError:\n    logging.warning('Core ML conversion was asked to respect trainable parameters from the Keras model, but the input model does not include a loss layer.')\ntry:\n    opt = model.optimizer\nexcept AttributeError:\n    logging.warning('Core ML conversion was asked to respect trainable parameters from the Keras model, but could not read the optimizer from Keras.')\n    return\nif model.optimizer:\n    cfg = model.optimizer.get_config()\n    if (('decay' in cfg) and (cfg['decay'] != 0.0)):\n        logging.warning(\"Keras optimizer has 'decay' set, which is not supported in Core ML. This parameter of the optimizer will be ignored. Clients can change the learning rate from within an MLUpdateTask callback to achieve the same effect.\")\n    if isinstance(model.optimizer, keras.optimizers.SGD):\n        params = SgdParams(lr=cfg['lr'], momentum=cfg['momentum'])\n        if (('nesterov' in cfg) and (cfg['nesterov'] == True)):\n            logging.warning(\"Keras SGD optimizer has 'nesterov' set, but this is not supported by Core ML. The parameter will be ignored.\")\n        params.set_batch(16, [1, 16, 32])\n        builder.set_sgd_optimizer(params)\n    elif isinstance(model.optimizer, keras.optimizers.Adam):\n        params = AdamParams(lr=cfg['lr'], beta1=cfg['beta_1'], beta2=cfg['beta_2'], eps=cfg['epsilon'])\n        if (('amsgrad' in cfg) and (cfg['amsgrad'] == True)):\n            logging.warning(\"Keras Adam optimizer has 'amsgrad' set, but this is not supported by Core ML. The parameter will be ignored.\")\n        params.set_batch(16, [1, 16, 32])\n        builder.set_adam_optimizer(params)\n    else:\n        logging.warning((('Optimizer ' + str(model.optimizer)) + ' is not yet supported by Core ML. The optimizer will not be carried over. To train this model, you will need to manually add a supported optimizer.'))\nelse:\n    logging.warning('Core ML conversion was asked to respect trainable parameters from the Keras model, but the input model does not include an optimizer.')\n"}
{"label_name":"predict","label":4,"method_name":"predict_image_with_CNN","method":"\n'\\n        predicts an image\\n        Returns:\\n            path of the image and its class\\n    '\nimg = image_utils.load_img(path, target_size=(100, 100))\nimg = PIL.ImageOps.invert(img)\nimg = image_utils.img_to_array(img)\nimg = (img \/ 255.0)\nimg = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\ny = model.predict_classes(img, verbose=0, batch_size=1)\nreturn (path, T.classes[y[0]])\n"}
{"label_name":"save","label":1,"method_name":"save_abc","method":"\nfolder = 'epoch_data'\nif (not os.path.exists(folder)):\n    os.makedirs(folder)\nsmi_file = os.path.join(folder, (name + '.abc'))\nwith open(smi_file, 'w') as afile:\n    afile.write('\\n'.join(smiles))\nreturn\n"}
{"label_name":"process","label":2,"method_name":"process_data_short_text","method":"\nprint('Processing data for short text...')\ndf = pd.DataFrame(columns=['label', 'text', 'gender', 'age', 'zodiac'])\nfor filename in os.listdir(folder_path):\n    labels = filename.rstrip('.xml').lstrip('0123456789.').lower().split('.')\n    blog = BeautifulSoup(codecs.open(((folder_path + '\/') + filename), encoding='utf-8', errors='ignore'), 'lxml')\n    for post in blog.find_all('post'):\n        post_text = post.text\n        post_text = re.sub('[^A-Za-z]+', ' ', post_text).strip().lower().split()\n        stop_words = set(stopwords.words('english'))\n        post_text = [word for word in post_text if (word not in stop_words)]\n        post_text = ' '.join(post_text)\n        df = df.append({'label': labels[2], 'text': post_text, 'gender': labels[0], 'age': labels[1], 'zodiac': labels[3]}, ignore_index=True)\ndf.to_csv('blogdata_short_text.csv')\nreturn print('Data processed & saved as {}\/blogdata_short_text.csv'.format(os.getcwd()))\n"}
{"label_name":"train","label":0,"method_name":"train2","method":"\n'\\n\\tthis function is used to process train.yzbx.txt format\\n\\t'\ntrain_data_file = os.path.join(base_path, num, 'train.yzbx.txt')\nb_data = defaultdict(list)\nfi = open(train_data_file, 'r')\nsize = 0\nmaxb = 0\nfor line in fi:\n    s = line.strip().split()\n    b = int(s[2])\n    maxb = max(b, maxb)\n    o = (b > int(s[1]))\n    o = int(o)\n    b_data[b].append(o)\n    size += 1\nfi.close()\nb_data = sorted(b_data.items(), key=(lambda e: e[0]), reverse=False)\nb_data = dict(b_data)\nbdns = []\nwins = 0\nfor z in b_data:\n    wins = sum(b_data[z])\n    b = z\n    d = wins\n    n = size\n    bdn = [b, d, n]\n    bdns.append(bdn)\n    size -= len(b_data[z])\nzw_dict = {}\nmin_p_w = 0\nbdns_length = len(bdns)\ncount = 0\np_l_tmp = 1.0\nfor bdn in bdns:\n    count += 1\n    b = float(bdn[0])\n    d = float(bdn[1])\n    n = float(bdn[2])\n    if (count < bdns_length):\n        p_l_tmp *= ((n - d) \/ n)\n    p_l = p_l_tmp\n    p_w = max((1.0 - p_l), min_p_w)\n    zw_dict[int(b)] = p_w\nreturn (zw_dict, maxb)\n"}
{"label_name":"train","label":0,"method_name":"train_gluon","method":"\n\ndef evaluate(epoch):\n    if (not args.use_rec):\n        return\n    val_data.reset()\n    acc_top1 = mx.metric.Accuracy()\n    acc_top5 = mx.metric.TopKAccuracy(5)\n    for (_, batch) in enumerate(val_data):\n        (data, label) = get_data_label(batch, context)\n        output = net(data.astype(args.dtype, copy=False))\n        acc_top1.update([label], [output])\n        acc_top5.update([label], [output])\n    (top1_name, top1_acc) = acc_top1.get()\n    (top5_name, top5_acc) = acc_top5.get()\n    logging.info('Epoch[%d] Rank[%d]\\tValidation-%s=%f\\tValidation-%s=%f', epoch, rank, top1_name, top1_acc, top5_name, top5_acc)\nnet.hybridize()\nnet.initialize(initializer, ctx=context)\nparams = net.collect_params()\nif (params is not None):\n    hvd.broadcast_parameters(params, root_rank=0)\noptimizer_params = {'wd': args.wd, 'momentum': args.momentum, 'lr_scheduler': lr_sched}\nif (args.dtype == 'float16'):\n    optimizer_params['multi_precision'] = True\nopt = mx.optimizer.create('sgd', **optimizer_params)\ntrainer = hvd.DistributedTrainer(params, opt, gradient_predivide_factor=args.gradient_predivide_factor)\nloss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\nmetric = mx.metric.Accuracy()\nfor epoch in range(args.num_epochs):\n    tic = time.time()\n    if args.use_rec:\n        train_data.reset()\n    metric.reset()\n    btic = time.time()\n    for (nbatch, batch) in enumerate(train_data, start=1):\n        (data, label) = get_data_label(batch, context)\n        with autograd.record():\n            output = net(data.astype(args.dtype, copy=False))\n            loss = loss_fn(output, label)\n        loss.backward()\n        trainer.step(batch_size)\n        metric.update([label], [output])\n        if (args.log_interval and ((nbatch % args.log_interval) == 0)):\n            (name, acc) = metric.get()\n            logging.info('Epoch[%d] Rank[%d] Batch[%d]\\t%s=%f\\tlr=%f', epoch, rank, nbatch, name, acc, trainer.learning_rate)\n            if (rank == 0):\n                batch_speed = (((num_workers * batch_size) * args.log_interval) \/ (time.time() - btic))\n                logging.info('Epoch[%d] Batch[%d]\\tSpeed: %.2f samples\/sec', epoch, nbatch, batch_speed)\n            btic = time.time()\n    elapsed = (time.time() - tic)\n    (_, acc) = metric.get()\n    logging.info('Epoch[%d] Rank[%d] Batch[%d]\\tTime cost=%.2f\\tTrain-accuracy=%f', epoch, rank, nbatch, elapsed, acc)\n    if (rank == 0):\n        epoch_speed = (((num_workers * batch_size) * nbatch) \/ elapsed)\n        logging.info('Epoch[%d]\\tSpeed: %.2f samples\/sec', epoch, epoch_speed)\n    if (args.eval_frequency and (((epoch + 1) % args.eval_frequency) == 0)):\n        evaluate(epoch)\n    if (args.save_frequency and (((epoch + 1) % args.save_frequency) == 0)):\n        net.export(('%s-%d' % (args.model, rank)), epoch=epoch)\nevaluate(epoch)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nreturn np.array([classify(phrase) for phrase in tweets])\n"}
{"label_name":"predict","label":4,"method_name":"plot_predictions","method":"\nindices = np.array(list(args))\nn = len(indices)\n(fig, axes) = plt.subplots(ncols=n, figsize=(n, 1))\nfor (i, idx) in enumerate(indices):\n    ax = axes.flatten()[i]\n    img = np.expand_dims(x_test[idx], 0)\n    pred = sess.run(model.preds, feed_dict={x: img})\n    ax.set_title(classes[np.argmax(pred)])\n    ax.imshow(img[0])\n    ax.axis('off')\nplt.show()\n"}
{"label_name":"train","label":0,"method_name":"train_breast_cancer","method":"\n(data, target) = sklearn.datasets.load_breast_cancer(return_X_y=True)\n(train_x, test_x, train_y, test_y) = train_test_split(data, target, test_size=0.25)\ntrain_set = lgb.Dataset(train_x, label=train_y)\ntest_set = lgb.Dataset(test_x, label=test_y)\ngbm = lgb.train(config, train_set, valid_sets=[test_set], verbose_eval=False, callbacks=[LightGBMCallback])\npreds = gbm.predict(test_x)\npred_labels = np.rint(preds)\ntune.report(mean_accuracy=sklearn.metrics.accuracy_score(test_y, pred_labels), done=True)\n"}
{"label_name":"train","label":0,"method_name":"lms_train","method":"\n\ndef error(p, y, args):\n    l = len(p)\n    f = p[(l - 1)]\n    for i in range(len(args)):\n        f += (p[i] * args[i])\n    return (f - y)\nPara = leastsq(error, p0, args=(Zi, Data))\nreturn Para[0]\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\"Obtain a model's predictions.\\n\\n    Receives either `config_files` or `checkpoint` in order to load the correct\\n    model. Afterwards, runs the model through the inputs specified by\\n    `path-or-dir`, returning predictions according to the format specified by\\n    `output`.\\n\\n    Additional model behavior may be modified with `min-prob`, `only-class` and\\n    `ignore-class`.\\n    \"\nif debug:\n    tf.logging.set_verbosity(tf.logging.DEBUG)\nelse:\n    tf.logging.set_verbosity(tf.logging.ERROR)\nif (only_class and ignore_class):\n    click.echo('Only one of `only-class` or `ignore-class` may be specified.')\n    return\nfiles = resolve_files(path_or_dir)\nif (not files):\n    error = 'No files to predict found. Accepted formats are: {}.'.format(', '.join((IMAGE_FORMATS + VIDEO_FORMATS)))\n    click.echo(error)\n    return\nelse:\n    click.echo('Found {} files to predict.'.format(len(files)))\nif (output_path == '-'):\n    output = sys.stdout\nelse:\n    output = open(output_path, 'w')\nif save_media_to:\n    tf.gfile.MakeDirs(save_media_to)\nif checkpoint:\n    config = get_checkpoint_config(checkpoint)\nelif config_files:\n    config = get_config(config_files)\nelse:\n    click.echo('Neither checkpoint not config specified, assuming `accurate`.')\n    config = get_checkpoint_config('accurate')\nif override_params:\n    config = override_config_params(config, override_params)\nif (config.model.type == 'fasterrcnn'):\n    if config.model.network.with_rcnn:\n        config.model.rcnn.proposals.total_max_detections = max_detections\n    else:\n        config.model.rpn.proposals.post_nms_top_n = max_detections\n    config.model.rcnn.proposals.min_prob_threshold = min_prob\nelif (config.model.type == 'ssd'):\n    config.model.proposals.total_max_detections = max_detections\n    config.model.proposals.min_prob_threshold = min_prob\nelse:\n    raise ValueError(\"Model type '{}' not supported\".format(config.model.type))\nnetwork = PredictorNetwork(config)\nfor file in files:\n    save_path = (os.path.join(save_media_to, 'pred_{}'.format(os.path.basename(file))) if save_media_to else None)\n    file_type = get_file_type(file)\n    predictor = (predict_image if (file_type == 'image') else predict_video)\n    objects = predictor(network, file, only_classes=only_class, ignore_classes=ignore_class, save_path=save_path)\n    if ((objects is not None) and (file_type == 'image')):\n        output.write((json.dumps({'file': file, 'objects': objects}) + '\\n'))\noutput.close()\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nmodel.train()\nfor (batch_idx, (data, target)) in enumerate(train_loader):\n    if config_dict.use_cuda:\n        (data, target) = (data.cuda(), target.cuda())\n    (data, target) = (Variable(data), Variable(target))\n    optimizer.zero_grad()\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    Viz.show_value(loss.item(), name='Training Loss')\n    loss.backward()\n    optimizer.step()\n    if ((batch_idx % config_dict.log_interval) == 0):\n        print('Train Epoch: {} [{}\/{} samples ({:.0f}%)]\\t Batch Loss: {:.6f}'.format(epoch, (batch_idx * len(data)), len(train_loader.dataset), ((100.0 * batch_idx) \/ len(train_loader)), loss.item()))\n        Exp.save_model(model, name='MNIST_ConvNet', n_iter=batch_idx)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nreturn [('1' if (like >= th) else '0') for like in X]\n"}
{"label_name":"train","label":0,"method_name":"get_training_mode","method":"\n' get_training_mode.\\n\\n    Returns variable in-use to set training mode.\\n\\n    Returns:\\n        A `Variable`, the training mode holder.\\n\\n    '\ninit_training_mode()\ncoll = tf.get_collection('is_training')\nreturn coll[0]\n"}
{"label_name":"train","label":0,"method_name":"trainNetwork","method":"\n'Train the artificial agent using Q-learning to play the pong game.\\n\\n    Args:\\n        s: the current state formed by 4 frames of the playground.\\n        readout: the Q value for each passible action in the current state.\\n        sess: session\\n    '\na = tf.placeholder('float', [None, ACTIONS])\ny = tf.placeholder('float', [None])\ncost = compute_cost(y, a, readout)\ntrain_step = tf.train.AdamOptimizer(Lr).minimize(cost)\ngame_state = game.GameState()\nD = deque()\ndo_nothing = np.zeros(ACTIONS)\ndo_nothing[0] = 1\n(x_t, r_0, terminal) = game_state.frame_step(do_nothing)\nx_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n(ret, x_t) = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)\ns_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\nsaver = tf.train.Saver()\nsess.run(tf.initialize_all_variables())\ncheckpoint = tf.train.get_checkpoint_state('.\/saved_networks_q_learning\/')\nif (checkpoint and checkpoint.model_checkpoint_path):\n    saver.restore(sess, checkpoint.model_checkpoint_path)\n    print('Successfully loaded:', checkpoint.model_checkpoint_path)\nelse:\n    print('Could not find old network weights')\nepsilon = INITIAL_EPSILON\nt = 0\nwhile True:\n    readout_t = readout.eval(feed_dict={s: [s_t]})[0]\n    action_index = get_action_index(readout_t, epsilon, t)\n    a_t = np.zeros([ACTIONS])\n    a_t[action_index] = 1\n    epsilon = scale_down_epsilon(epsilon, t)\n    for i in range(0, K):\n        (s_t1, r_t, terminal) = run_selected_action(a_t, s_t, game_state)\n        D.append((s_t, a_t, r_t, s_t1, terminal))\n        if (len(D) > REPLAY_MEMORY):\n            D.popleft()\n    if (t > OBSERVE):\n        minibatch = random.sample(list(D), BATCH)\n        s_j_batch = [d[0] for d in minibatch]\n        a_batch = [d[1] for d in minibatch]\n        r_batch = [d[2] for d in minibatch]\n        s_j1_batch = [d[3] for d in minibatch]\n        terminal_batch = [d[4] for d in minibatch]\n        readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})\n        target_q_batch = compute_target_q(r_batch, readout_j1_batch, terminal_batch)\n        train_step.run(feed_dict={y: target_q_batch, a: a_batch, s: s_j_batch})\n    s_t = s_t1\n    t += 1\n    if ((t % 10000) == 0):\n        saver.save(sess, (('saved_networks_q_learning\/' + GAME) + '-dqn'), global_step=t)\n    state = ''\n    if (t <= OBSERVE):\n        state = 'observe'\n    elif ((t > OBSERVE) and (t <= (OBSERVE + EXPLORE))):\n        state = 'explore'\n    else:\n        state = 'train'\n    print('TIMESTEP', t, '\/ STATE', state, '\/ EPSILON', epsilon, '\/ ACTION', action_index, '\/ REWARD', r_t, ('\/ Q_MAX %e' % np.max(readout_t)))\n"}
{"label_name":"save","label":1,"method_name":"save_img","method":"\nimg = array_to_img(x, data_format=data_format, scale=scale)\nif ((img.mode == 'RGBA') and ((file_format == 'jpg') or (file_format == 'jpeg'))):\n    warnings.warn('The JPG format does not support RGBA images, converting to RGB.')\n    img = img.convert('RGB')\nimg.save(path, format=file_format, **kwargs)\n"}
{"label_name":"process","label":2,"method_name":"process_file","method":"\nlogging.debug('processing file %s', filename)\ntry:\n    ast = ast_generator.parse_file(filename, process_file.options.get('normalize', False))\n    item = ProcessedFileItem(filename, ast, process_file.options)\n    process_file.queue.put(item)\nexcept Exception as e:\n    logging.debug('failed to parse %s: %s', filename, str(e))\n    process_file.queue.put(FailedFileItem(filename, e))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nprint('Starting training...')\nparams = {}\nparams.update(paramdict)\nprint('Passed params: ', params)\nprint(platform.uname())\nsuffix = ((('grid_' + ''.join([((str(x) + '_') if ((pair[0] is not 'nbsteps') and (pair[0] is not 'rngseed') and (pair[0] is not 'save_every') and (pair[0] is not 'test_every')) else '') for pair in sorted(zip(params.keys(), params.values()), key=(lambda x: x[0])) for x in pair])[:(- 1)]) + '_rngseed_') + str(params['rngseed']))\nparams['rngseed'] = 3\nprint('Setting random seeds')\nnp.random.seed(params['rngseed'])\nrandom.seed(params['rngseed'])\ntorch.manual_seed(params['rngseed'])\nnet = Network(params)\nnet.load_state_dict(torch.load((('.\/tmpWorked\/torchmodel_' + suffix) + '.txt')))\nprint('Shape of all optimized parameters:', [x.size() for x in net.parameters()])\nallsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]\nprint('Size (numel) of all optimized elements:', allsizes)\nprint('Total size (numel) of all optimized elements:', sum(allsizes))\nprint('Initializing optimizer')\noptimizer = torch.optim.Adam(net.parameters(), lr=(1.0 * params['lr']), eps=0.0001)\nLABSIZE = params['labsize']\nlab = np.ones((LABSIZE, LABSIZE))\nCTR = (LABSIZE \/\/ 2)\nlab[1:(LABSIZE - 1), 1:(LABSIZE - 1)].fill(0)\nfor row in range(1, (LABSIZE - 1)):\n    for col in range(1, (LABSIZE - 1)):\n        if (((row % 2) == 0) and ((col % 2) == 0)):\n            lab[(row, col)] = 1\nlab[(CTR, CTR)] = 0\nall_losses = []\nall_losses_objective = []\nall_losses_eval = []\nall_losses_v = []\nlossbetweensaves = 0\nnowtime = time.time()\nprint('Starting episodes...')\nsys.stdout.flush()\npos = 0\nhidden = net.initialZeroState()\nhebb = net.initialZeroHebb()\nparams['nbiter'] = 1\nfor numiter in range(params['nbiter']):\n    PRINTTRACE = 0\n    if (((numiter + 1) % (1 + params['print_every'])) == 0):\n        PRINTTRACE = 1\n    rposr = 0\n    rposc = 0\n    if (params['rp'] == 0):\n        while (lab[(rposr, rposc)] == 1):\n            rposr = np.random.randint(1, (LABSIZE - 1))\n            rposc = np.random.randint(1, (LABSIZE - 1))\n    elif (params['rp'] == 1):\n        while ((lab[(rposr, rposc)] == 1) or ((rposr != 1) and (rposr != (LABSIZE - 2)) and (rposc != 1) and (rposc != (LABSIZE - 2)))):\n            rposr = np.random.randint(1, (LABSIZE - 1))\n            rposc = np.random.randint(1, (LABSIZE - 1))\n    posc = CTR\n    posr = CTR\n    optimizer.zero_grad()\n    loss = 0\n    lossv = 0\n    hidden = net.initialZeroState()\n    hebb = net.initialZeroHebb()\n    reward = 0.0\n    rewards = []\n    vs = []\n    logprobs = []\n    sumreward = 0.0\n    dist = 0\n    print('==========')\n    print('==========')\n    ax_imgs = []\n    for numstep in range(params['eplen']):\n        inputsN = np.zeros((1, TOTALNBINPUTS), dtype='float32')\n        inputsN[0, 0:(RFSIZE * RFSIZE)] = lab[(posr - (RFSIZE \/\/ 2)):((posr + (RFSIZE \/\/ 2)) + 1), (posc - (RFSIZE \/\/ 2)):((posc + (RFSIZE \/\/ 2)) + 1)].flatten()\n        inputs = torch.from_numpy(inputsN).cuda()\n        inputs[0][(- 1)] = 1\n        inputs[0][(- 2)] = numstep\n        inputs[0][(- 3)] = reward\n        (y, v, hidden, hebb) = net(Variable(inputs, requires_grad=False), hidden, hebb)\n        distrib = torch.distributions.Categorical(y)\n        actionchosen = distrib.sample()\n        numactionchosen = actionchosen.data[0]\n        tgtposc = posc\n        tgtposr = posr\n        if (numactionchosen == 0):\n            tgtposr -= 1\n        elif (numactionchosen == 1):\n            tgtposr += 1\n        elif (numactionchosen == 2):\n            tgtposc -= 1\n        elif (numactionchosen == 3):\n            tgtposc += 1\n        else:\n            raise ValueError('Wrong Action')\n        reward = 0.0\n        if (lab[tgtposr][tgtposc] == 1):\n            reward = (- 0.1)\n        else:\n            dist += 1\n            posc = tgtposc\n            posr = tgtposr\n        for numr in range(LABSIZE):\n            s = ''\n            for numc in range(LABSIZE):\n                if ((posr == numr) and (posc == numc)):\n                    s += 'o'\n                elif ((rposr == numr) and (rposc == numc)):\n                    s += 'X'\n                elif (lab[(numr, numc)] == 1):\n                    s += '#'\n                else:\n                    s += ' '\n            print(s)\n        print('')\n        print('')\n        labg = lab.copy()\n        labg[(rposr, rposc)] = 2\n        labg[(posr, posc)] = 3\n        fullimg = plt.imshow(labg, animated=True)\n        ax_imgs.append([fullimg])\n        if ((rposr == posr) and (rposc == posc)):\n            reward += 10\n            if (params['randstart'] == 1):\n                posr = np.random.randint(1, (LABSIZE - 1))\n                posc = np.random.randint(1, (LABSIZE - 1))\n                while (lab[(posr, posc)] == 1):\n                    posr = np.random.randint(1, (LABSIZE - 1))\n                    posc = np.random.randint(1, (LABSIZE - 1))\n            else:\n                posr = CTR\n                posc = CTR\n        rewards.append(reward)\n        vs.append(v)\n        sumreward += reward\n        logprobs.append(distrib.log_prob(actionchosen))\n        loss += (params['bentropy'] * y.pow(2).sum())\n    R = 0\n    gammaR = params['gr']\n    for numstepb in reversed(range(params['eplen'])):\n        R = ((gammaR * R) + rewards[numstepb])\n        lossv += (vs[numstepb][0] - R).pow(2)\n        loss -= (logprobs[numstepb] * (R - vs[numstepb].data[0][0]))\n    if True:\n        print('lossv: ', lossv.data.cpu().numpy()[0])\n        print('Total reward for this episode:', sumreward, 'Dist:', dist)\n    if (params['squash'] == 1):\n        if (sumreward < 0):\n            sumreward = (- np.sqrt((- sumreward)))\n        else:\n            sumreward = np.sqrt(sumreward)\n    elif (params['squash'] == 0):\n        pass\n    else:\n        raise ValueError('Incorrect value for squash parameter')\n    loss += (params['blossv'] * lossv)\n    loss \/= params['eplen']\n    lossnum = loss.data[0]\n    lossbetweensaves += lossnum\n    if (((numiter + 1) % 10) == 0):\n        all_losses_objective.append(lossnum)\n        all_losses_eval.append(sumreward)\n        all_losses_v.append(lossv.data[0])\n    anim = animation.ArtistAnimation(fig, ax_imgs, interval=200)\n    anim.save('anim.gif', writer='imagemagick', fps=10)\n    if (((numiter + 1) % params['print_every']) == 0):\n        print(numiter, '====')\n        print('Mean loss: ', (lossbetweensaves \/ params['print_every']))\n        lossbetweensaves = 0\n        previoustime = nowtime\n        nowtime = time.time()\n        print('Time spent on last', params['print_every'], 'iters: ', (nowtime - previoustime))\n        if ((params['type'] == 'plastic') or (params['type'] == 'lstmplastic')):\n            print('ETA: ', net.eta.data.cpu().numpy(), 'alpha[0,1]: ', net.alpha.data.cpu().numpy()[(0, 1)], 'w[0,1]: ', net.w.data.cpu().numpy()[(0, 1)])\n        elif (params['type'] == 'rnn'):\n            print('w[0,1]: ', net.w.data.cpu().numpy()[(0, 1)])\n    if (((numiter + 1) % params['save_every']) == 0):\n        print('Saving files...')\n        losslast100 = np.mean(all_losses_objective[(- 100):])\n        print('Average loss over the last 100 episodes:', losslast100)\n        print('Saving local files...')\n"}
{"label_name":"process","label":2,"method_name":"process_content","method":"\ntry:\n    for i in tokenized[5:]:\n        words = nltk.word_tokenize(i)\n        tagged = nltk.pos_tag(words)\n        namedEnt = nltk.ne_chunk(tagged, binary=True)\n        namedEnt.draw()\nexcept Exception as e:\n    print(str(e))\n"}
{"label_name":"save","label":1,"method_name":"save_num","method":"\nwith open(filepath, 'w') as fd:\n    fd.write(str(int_num))\n"}
{"label_name":"save","label":1,"method_name":"plot_learner_and_save","method":"\n(fig, ax) = plt.subplots()\ntri = learner.interpolator(scaled=True).tri\ntriang = mtri.Triangulation(*tri.points.T, triangles=tri.vertices)\nax.triplot(triang, c='k', lw=0.8)\nax.imshow(learner.plot().Image.I.data, extent=((- 0.5), 0.5, (- 0.5), 0.5))\nax.set_xticks([])\nax.set_yticks([])\nplt.savefig(fname, bbox_inches='tight', transparent=True, dpi=300, pad_inches=(- 0.1))\n"}
{"label_name":"train","label":0,"method_name":"train_example","method":"\nconfig = {'test_mode': test_mode, 'batch_size': (16 if test_mode else (512 \/\/ num_workers)), 'classification_model_path': MODEL_PATH}\ntrainer = TorchTrainer(training_operator_cls=GANOperator, num_workers=num_workers, config=config, use_gpu=use_gpu, use_tqdm=True)\nfrom tabulate import tabulate\npbar = trange(5, unit='epoch')\nfor itr in pbar:\n    stats = trainer.train(info=dict(epoch_idx=itr, num_epochs=5))\n    pbar.set_postfix(dict(loss_g=stats['loss_g'], loss_d=stats['loss_d']))\n    formatted = tabulate([stats], headers='keys')\n    if (itr > 0):\n        formatted = formatted.split('\\n')[(- 1)]\n    pbar.write(formatted)\nreturn trainer\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nerrorCount = 0\nfor j in range(testCount):\n    try:\n        (pP, pN, smsType) = classify_without_D(pWordsPosicity, pWordsNegy, prior_Pos, test_word_arrayMarkedArray[j], D, DS_temp)\n        if (smsType != test_word_arrayLabel[j]):\n            errorCount += 1\n    except Exception as e:\n        traceback.print_exc(e)\nprint((errorCount \/ testCount))\n"}
{"label_name":"train","label":0,"method_name":"build_training_array","method":"\n'\\n    Create array of all training images using the sliced landmark\\n    shapes. Returns array of images, for which each row is an image.\\n    '\nimages = np.zeros((TRAIN_AMOUNT, TRAIN_DIM))\nindex = 0\nfor i in os.listdir(ROOT):\n    image = cv2.cvtColor(cv2.imread((ROOT + i)), cv2.COLOR_BGR2GRAY)\n    image = cv2.medianBlur(image, 5)\n    images[index, :] = np.hstack(image)\n    index += 1\nreturn images\n"}
{"label_name":"train","label":0,"method_name":"train_extra_trees","method":"\n'\\n    Train an extra tree\\n    Note:     This function is simply a wrapper to the sklearn functionality\\n              for model training.\\n              See function extract_features_and_train() to use a wrapper on both\\n              the feature extraction and the model training (and parameter\\n              tuning) processes.\\n    ARGUMENTS:\\n        - features:         a list ([numOfClasses x 1]) whose elements\\n                            containt np matrices of features\\n                            each matrix features[i] of class i is\\n                            [n_samples x numOfDimensions]\\n        - n_estimators:     number of trees in the forest\\n    RETURNS:\\n        - et:               the trained model\\n    '\n(feature_matrix, labels) = features_to_matrix(features)\net = sklearn.ensemble.ExtraTreesClassifier(n_estimators=n_estimators)\net.fit(feature_matrix, labels)\nreturn et\n"}
{"label_name":"process","label":2,"method_name":"preprocess_data","method":"\n'Process the dataset into vector representation.\\n\\n    When converting the BldgType to a vector, use one-hot encoding, the order\\n    has been provided in the one_hot_bldg_type helper function. Otherwise,\\n    the values in the column can be directly used.\\n\\n    If squared_features is true, then the feature values should be\\n    element-wise squared.\\n\\n    Args:\\n        dataset(dict): Dataset extracted from io_tools.read_dataset\\n        feature_columns(list): List of feature names.\\n        squred_features(bool): Whether to square the features.\\n\\n    Returns:\\n        processed_datas(list): List of numpy arrays x, y.\\n            x is a numpy array, of dimension (N,K), N is the number of example\\n            in the dataset, and K is the length of the feature vector.\\n            Note: BldgType when converted to one hot vector is of length 5.\\n            Each row of x contains an example.\\n            y is a numpy array, of dimension (N,1) containing the SalePrice.\\n    '\ncolumns_to_id = {'Id': 0, 'BldgType': 1, 'OverallQual': 6, 'GrLivArea': 7, 'GarageArea': 8, 'SalePrice': 9}\ncat_col = []\nfor key in feature_columns:\n    cat_col.append(columns_to_id[key])\nif ('BldgType' in feature_columns):\n    cat_col.extend([2, 3, 4, 5])\nx = []\ny = []\nfor keys in dataset:\n    temp = list(dataset[keys])\n    vector = one_hot_bldg_type(temp[1])\n    temp = (([temp[0]] + vector) + temp[2:])\n    temp = list(map(int, temp))\n    x.append(temp[0:(- 1)])\n    y.append(temp[(- 1)])\nif squared_features:\n    x = np.square(x)\nx = np.array(x)[:, sorted(cat_col)]\ny = np.reshape(y, (x.shape[0], 1))\nprocessed_dataset = [x, y]\nreturn processed_dataset\n"}
{"label_name":"save","label":1,"method_name":"get_saved_model_tag_sets","method":"\n'Retrieves all the tag-sets available in the SavedModel.\\n\\n  Args:\\n    saved_model_dir: Directory containing the SavedModel.\\n\\n  Returns:\\n    String representation of all tag-sets in the SavedModel.\\n  '\nsaved_model = read_saved_model(saved_model_dir)\nall_tags = []\nfor meta_graph_def in saved_model.meta_graphs:\n    all_tags.append(list(meta_graph_def.meta_info_def.tags))\nreturn all_tags\n"}
{"label_name":"process","label":2,"method_name":"process_data","method":"\nfile_path = download(FILE_NAME, EXPECTED_BYTES)\nwords = read_data(file_path)\n(dictionary, _) = build_vocab(words, vocab_size)\nindex_words = convert_words_to_index(words, dictionary)\ndel words\nsingle_gen = generate_sample(index_words, skip_window)\nreturn get_batch(single_gen, batch_size)\n"}
{"label_name":"save","label":1,"method_name":"maybe_saved_model_directory","method":"\n\"Checks whether the provided export directory could contain a SavedModel.\\n\\n  Note that the method does not load any data by itself. If the method returns\\n  `false`, the export directory definitely does not contain a SavedModel. If the\\n  method returns `true`, the export directory may contain a SavedModel but\\n  provides no guarantee that it can be loaded.\\n\\n  Args:\\n    export_dir: Absolute string path to possible export location. For example,\\n                '\/my\/foo\/model'.\\n\\n  Returns:\\n    True if the export directory contains SavedModel files, False otherwise.\\n  \"\ntxt_path = os.path.join(export_dir, constants.SAVED_MODEL_FILENAME_PBTXT)\npb_path = os.path.join(export_dir, constants.SAVED_MODEL_FILENAME_PB)\nreturn (file_io.file_exists(txt_path) or file_io.file_exists(pb_path))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nmodel.train()\nfor (batch_idx, (data, target)) in enumerate(train_loader):\n    if ((batch_idx * len(data)) > 1024):\n        return\n    (data, target) = (data.to(device), target.to(device))\n    optimizer.zero_grad()\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n"}
{"label_name":"save","label":1,"method_name":"save_blob","method":"\nf = open(path, 'wb')\npickle.dump(content, f)\nf.close()\n"}
{"label_name":"predict","label":4,"method_name":"read_predictions","method":"\nwith open(file_path, 'r') as f:\n    csv = f.read()\n    csv = csv.replace(' Q', 'Q')\ndf = pd.read_csv(StringIO(unicode(csv)), sep=',', index_col=0, parse_dates=True, infer_datetime_format=True)\ndf.index = df.index.to_period()\nreturn df\n"}
{"label_name":"train","label":0,"method_name":"get_train_val_loaders","method":"\ntrain_ds = get_train_dataset(root_path)\nval_ds = get_val_dataset(root_path)\nif (sbd_path is not None):\n    sbd_train_ds = get_train_noval_sbdataset(sbd_path)\n    train_ds = (train_ds + sbd_train_ds)\nif (len(val_ds) < len(train_ds)):\n    train_eval_indices = np.random.permutation(len(train_ds))[:len(val_ds)]\n    train_eval_ds = Subset(train_ds, train_eval_indices)\nelse:\n    train_eval_ds = train_ds\ntrain_ds = TransformedDataset(train_ds, transform_fn=train_transforms)\nval_ds = TransformedDataset(val_ds, transform_fn=val_transforms)\ntrain_eval_ds = TransformedDataset(train_eval_ds, transform_fn=val_transforms)\nval_batch_size = ((batch_size * 4) if (val_batch_size is None) else val_batch_size)\ntrain_loader = get_dataloader(train_ds, shuffle=True, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, drop_last=True, limit_num_samples=limit_train_num_samples)\nval_loader = get_dataloader(val_ds, shuffle=False, batch_size=val_batch_size, num_workers=num_workers, drop_last=False, limit_num_samples=limit_val_num_samples)\ntrain_eval_loader = get_dataloader(train_eval_ds, shuffle=False, batch_size=val_batch_size, num_workers=num_workers, drop_last=False, limit_num_samples=limit_val_num_samples)\nreturn (train_loader, val_loader, train_eval_loader)\n"}
{"label_name":"train","label":0,"method_name":"get_train_hold_split","method":"\nX_train = tensors_dict['X_train']\nY_train = tensors_dict['Y_train']\ninds = np.random.permutation(X_train.size(0))\nwith open(os.path.join(save_folder, 'th_split_permutation'), 'wb') as f:\n    np.save(f, inds)\ntrain_inds = torch.LongTensor(inds[:int((X_train.size(0) * th_frac))])\nhold_inds = torch.LongTensor(inds[int((X_train.size(0) * th_frac)):])\n(X_train2, X_hold2) = (X_train[train_inds, :], X_train[hold_inds, :])\n(Y_train2, Y_hold2) = (Y_train[train_inds, :], Y_train[hold_inds, :])\ntensors_task = {'X_train': X_train2, 'Y_train': Y_train2, 'X_hold': X_hold2, 'Y_hold': Y_hold2, 'X_test': tensors_dict['X_test'].clone(), 'Y_test': tensors_dict['Y_test'].clone()}\nreturn tensors_task\n"}
{"label_name":"process","label":2,"method_name":"preprocess","method":"\nif (tokenizer is None):\n    tokenizer = Tokenizer(filters=filters, split=split)\n    tokenizer.fit_on_texts((train + test))\n    sequences_train = tokenizer.texts_to_sequences(train)\n    sequences_test = tokenizer.texts_to_sequences(test)\n    tokenizer.word_index['<PAD>'] = 0\n    return (pad_sequences(sequences_train, maxlen=maxlen), pad_sequences(sequences_test, maxlen=maxlen), tokenizer)\nelse:\n    sequences_train = tokenizer.texts_to_sequences(train)\n    sequences_test = tokenizer.texts_to_sequences(test)\n    return (pad_sequences(sequences_train, maxlen=maxlen), pad_sequences(sequences_test, maxlen=maxlen))\n"}
{"label_name":"save","label":1,"method_name":"save_image_array_as_png","method":"\n'Saves an image (represented as a numpy array) to PNG.\\n\\n  Args:\\n    image: a numpy array with shape [height, width, 3].\\n    output_path: path to which image should be written.\\n  '\nimage_pil = Image.fromarray(np.uint8(image)).convert('RGB')\nwith tf.gfile.Open(output_path, 'w') as fid:\n    image_pil.save(fid, 'PNG')\n"}
{"label_name":"forward","label":3,"method_name":"adjust_data_forward","method":"\nBayesianNetworkNode = apps.get_model('bayesian_networks', 'BayesianNetworkNode')\nY = BayesianNetworkNode.objects.get(name='Y', network__name='Clustering (Example)')\nY_col_avg_logged = Y.data_columns.get(ref_column='avg_time_logged')\nY_col_avg_logged.ref_column = 'avg_time_pages'\nY_col_avg_logged.save()\n"}
{"label_name":"train","label":0,"method_name":"make_training_functions","method":"\nX = T.TensorType('float32', ([False] * 5))('X')\ny = T.TensorType('int32', ([False] * 1))('y')\nl_out = model['l_out']\nbatch_index = T.iscalar('batch_index')\nbatch_slice = slice((batch_index * cfg['batch_size']), ((batch_index + 1) * cfg['batch_size']))\ntest_batch_slice = slice((batch_index * 24), ((batch_index + 1) * 24))\ntest_batch_i = batch_index\ny_hat_deterministic = lasagne.layers.get_output(l_out, X, deterministic=True)\nraw_pred = T.sum(y_hat_deterministic, axis=0)\npred = T.argmax(raw_pred)\nclassifier_test_error_rate = T.cast(T.mean(T.neq(pred, T.mean(y, dtype='int32'))), 'float32')\nX_shared = lasagne.utils.shared_empty(5, dtype='float32')\ny_shared = lasagne.utils.shared_empty(1, dtype='float32')\ntest_error_fn = theano.function([batch_index], [classifier_test_error_rate, pred, raw_pred], givens={X: X_shared[test_batch_slice], y: T.cast(y_shared[test_batch_slice], 'int32')})\ntfuncs = {'test_function': test_error_fn}\ntvars = {'X': X, 'y': y, 'X_shared': X_shared, 'y_shared': y_shared, 'batch_slice': batch_slice, 'batch_index': batch_index}\nreturn (tfuncs, tvars, model)\n"}
{"label_name":"save","label":1,"method_name":"save_organized_data_info","method":"\nwith open(_organized_data_info_file_dim(imgs_dim_1d, multi_crop), 'w') as f:\n    dump(info, f)\n"}
{"label_name":"train","label":0,"method_name":"fetch_traingset","method":"\nimage_file = 'data\/train-images-idx3-ubyte'\nlabel_file = 'data\/train-labels-idx1-ubyte'\nimages = read_image_files(image_file, 60000)\nlabels = read_label_files(label_file)\nreturn {'images': images, 'labels': labels}\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\ntorch.save(network, path)\nprint(('Save Model to File: %s' % path))\n"}
{"label_name":"process","label":2,"method_name":"post_process_metrics","method":"\n'Update current dataset metrics and filter out specific keys.\\n\\n    Args:\\n        prefix (str): Prefix string to be appended\\n        workers (WorkerSet): Set of workers\\n        metrics (dict): Current metrics dictionary\\n    '\nres = collect_metrics(remote_workers=workers.remote_workers())\nfor key in METRICS_KEYS:\n    metrics[((prefix + '_') + key)] = res[key]\nreturn metrics\n"}
{"label_name":"process","label":2,"method_name":"process","method":"\n' Noun Chunks processing\\n    '\nmode = settings.get('mode', 'tokenize')\ndrop_deter = settings['drop_determiners']\nmin_freq = int(settings['min_freq'])\nfor doc in content:\n    text = doc['text']\n    try:\n        td = Doc(text)\n        ncs = [x.text for x in noun_chunks(td, drop_determiners=drop_deter, min_freq=min_freq)]\n    except Exception:\n        logger.exception('Error extracting noun chunks %r', doc)\n        continue\n    if (mode == 'tokenize'):\n        for nc in ncs:\n            text = text.replace(nc, tokenize(nc))\n    if (mode == 'append'):\n        text = ' '.join(([text] + [tokenize(nc) for nc in ncs]))\n    if (mode == 'replace'):\n        text = ' '.join([tokenize(nc) for nc in ncs])\n    try:\n        (yield set_text(doc, text))\n    except Exception:\n        logger.exception('Error in converting to Doc %r', text)\n        continue\n"}
{"label_name":"save","label":1,"method_name":"save_to_numpy","method":"\nimage_folders = ['images_background', 'images_evaluation']\nall_np_array = []\nfor image_folder in image_folders:\n    np_array_loc = os.path.join(data_dir, (image_folder + '.npy'))\n    print('Converting folder {} to numpy array...'.format(image_folder))\n    np_array = omniglot_folder_to_NDarray(os.path.join(extracted_images_location, image_folder))\n    np.save(np_array_loc, np_array)\n    all_np_array.append(np_array)\n    print('Done.')\nall_np_array = np.concatenate(all_np_array, axis=0)\nnp.save(os.path.join('data', 'omniglot.npy'), all_np_array)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\"Runs the training loop.\\n\\n  Args:\\n    train_op: A `Tensor` that, when executed, will apply the gradients and\\n      return the loss value.\\n    logdir: The directory where the graph and checkpoints are saved.\\n    master: The URL of the master.\\n    is_chief: Specifies whether or not the training is being run by the primary\\n      replica during replica training.\\n    scaffold: An tf.train.Scaffold instance.\\n    hooks: List of `tf.train.SessionRunHook` callbacks which are run inside the\\n      training loop.\\n    chief_only_hooks: List of `tf.train.SessionRunHook` instances which are run\\n      inside the training loop for the chief trainer only.\\n    save_checkpoint_secs: The frequency, in seconds, that a checkpoint is saved\\n      using a default checkpoint saver. If `save_checkpoint_secs` is set to\\n      `None`, then the default checkpoint saver isn't used.\\n    save_summaries_steps: The frequency, in number of global steps, that the\\n      summaries are written to disk using a default summary saver. If\\n      `save_summaries_steps` is set to `None`, then the default summary saver\\n      isn't used.\\n    config: An instance of `tf.ConfigProto`.\\n\\n  Returns:\\n    the value of the loss function after training.\\n\\n  Raises:\\n    ValueError: if `logdir` is `None` and either `save_checkpoint_secs` or\\n    `save_summaries_steps` are `None.\\n  \"\nif ((logdir is None) and is_chief):\n    if save_summaries_steps:\n        raise ValueError('logdir cannot be None when save_summaries_steps is not None')\n    if save_checkpoint_secs:\n        raise ValueError('logdir cannot be None when save_checkpoint_secs is not None')\nwith monitored_session.MonitoredTrainingSession(master=master, is_chief=is_chief, checkpoint_dir=logdir, scaffold=scaffold, hooks=hooks, chief_only_hooks=chief_only_hooks, save_checkpoint_secs=save_checkpoint_secs, save_summaries_steps=save_summaries_steps, config=config) as session:\n    loss = None\n    while (not session.should_stop()):\n        loss = session.run(train_op)\nreturn loss\n"}
{"label_name":"process","label":2,"method_name":"call_subprocess","method":"\ncmd_parts = []\nfor part in cmd:\n    if (len(part) > 45):\n        part = ((part[:20] + '...') + part[(- 20):])\n    if ((' ' in part) or ('\\n' in part) or ('\"' in part) or (\"'\" in part)):\n        part = ('\"%s\"' % part.replace('\"', '\\\\\"'))\n    if hasattr(part, 'decode'):\n        try:\n            part = part.decode(sys.getdefaultencoding())\n        except UnicodeDecodeError:\n            part = part.decode(sys.getfilesystemencoding())\n    cmd_parts.append(part)\ncmd_desc = ' '.join(cmd_parts)\nif show_stdout:\n    stdout = None\nelse:\n    stdout = subprocess.PIPE\nlogger.debug(('Running command %s' % cmd_desc))\nif (extra_env or remove_from_env):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    if remove_from_env:\n        for varname in remove_from_env:\n            env.pop(varname, None)\nelse:\n    env = None\ntry:\n    proc = subprocess.Popen(cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout, cwd=cwd, env=env)\nexcept Exception:\n    e = sys.exc_info()[1]\n    logger.fatal(('Error %s while executing command %s' % (e, cmd_desc)))\n    raise\nall_output = []\nif (stdout is not None):\n    stdout = proc.stdout\n    encoding = sys.getdefaultencoding()\n    fs_encoding = sys.getfilesystemencoding()\n    while 1:\n        line = stdout.readline()\n        try:\n            line = line.decode(encoding)\n        except UnicodeDecodeError:\n            line = line.decode(fs_encoding)\n        if (not line):\n            break\n        line = line.rstrip()\n        all_output.append(line)\n        if filter_stdout:\n            level = filter_stdout(line)\n            if isinstance(level, tuple):\n                (level, line) = level\n            logger.log(level, line)\n            if (not logger.stdout_level_matches(level)):\n                logger.show_progress()\n        else:\n            logger.info(line)\nelse:\n    proc.communicate()\nproc.wait()\nif proc.returncode:\n    if raise_on_returncode:\n        if all_output:\n            logger.notify(('Complete output from command %s:' % cmd_desc))\n            logger.notify(('\\n'.join(all_output) + '\\n----------------------------------------'))\n        raise OSError(('Command %s failed with error code %s' % (cmd_desc, proc.returncode)))\n    else:\n        logger.warn(('Command %s had error code %s' % (cmd_desc, proc.returncode)))\n"}
{"label_name":"train","label":0,"method_name":"instances_in_subtrain","method":"\nreturn _get_stat_from_file('instances_in_subtrain')\n"}
{"label_name":"train","label":0,"method_name":"training","method":"\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    if (FLAGS.optimizer == 'SGD'):\n        print('Running with SGD optimizer')\n        optimizer = tf.train.GradientDescentOptimizer(0.1)\n    elif (FLAGS.optimizer == 'adam'):\n        print('Running with adam optimizer')\n        optimizer = tf.train.AdamOptimizer(0.001)\n    elif (FLAGS.optimizer == 'adagrad'):\n        print('Running with adagrad optimizer')\n        optimizer = tf.train.AdagradOptimizer(0.01)\n    else:\n        raise ValueError('optimizer was not recognized.')\n    train_op = optimizer.minimize(loss=loss, global_step=global_step)\nreturn (train_op, global_step)\n"}
{"label_name":"forward","label":3,"method_name":"linear_activation_forward","method":"\nif (activation == 'relu'):\n    (Z, linear_cache) = linear_forward(A_prev, W, b)\n    (A, activation_cache) = relu(Z)\nif (activation == 'softmax'):\n    (Z, linear) = linear_forward(A_prev, W, b)\n    (A, activation_cache) = softmax(Z)\nassert (A.shape == (W.shape[0], A_prev.shape[1]))\ncache = (linear_cache, activation_cache)\nreturn (A, cache)\n"}
{"label_name":"process","label":2,"method_name":"process_partition","method":"\noutput_dir = os.path.join(args.output_path, partition)\nif (not os.path.exists(output_dir)):\n    os.mkdir(output_dir)\nxy_pairs = []\npatients = list(filter(str.isdigit, os.listdir(os.path.join(args.root_path, partition))))\nfor patient in tqdm(patients, desc='Iterating over patients in {}'.format(partition)):\n    patient_folder = os.path.join(args.root_path, partition, patient)\n    patient_ts_files = list(filter((lambda x: (x.find('timeseries') != (- 1))), os.listdir(patient_folder)))\n    for ts_filename in patient_ts_files:\n        with open(os.path.join(patient_folder, ts_filename)) as tsfile:\n            lb_filename = ts_filename.replace('_timeseries', '')\n            label_df = pd.read_csv(os.path.join(patient_folder, lb_filename))\n            if (label_df.shape[0] == 0):\n                continue\n            mortality = int(label_df.iloc[0]['Mortality'])\n            los = (24.0 * label_df.iloc[0]['Length of Stay'])\n            if pd.isnull(los):\n                print('\\n\\t(length of stay is missing)', patient, ts_filename)\n                continue\n            if (los < (n_hours - eps)):\n                continue\n            ts_lines = tsfile.readlines()\n            header = ts_lines[0]\n            ts_lines = ts_lines[1:]\n            event_times = [float(line.split(',')[0]) for line in ts_lines]\n            ts_lines = [line for (line, t) in zip(ts_lines, event_times) if ((- eps) < t < (n_hours + eps))]\n            if (len(ts_lines) == 0):\n                print('\\n\\t(no events in ICU) ', patient, ts_filename)\n                continue\n            output_ts_filename = ((patient + '_') + ts_filename)\n            with open(os.path.join(output_dir, output_ts_filename), 'w') as outfile:\n                outfile.write(header)\n                for line in ts_lines:\n                    outfile.write(line)\n            xy_pairs.append((output_ts_filename, mortality))\nprint('Number of created samples:', len(xy_pairs))\nif (partition == 'train'):\n    random.shuffle(xy_pairs)\nif (partition == 'test'):\n    xy_pairs = sorted(xy_pairs)\nwith open(os.path.join(output_dir, 'listfile.csv'), 'w') as listfile:\n    listfile.write('stay,y_true\\n')\n    for (x, y) in xy_pairs:\n        listfile.write('{},{:d}\\n'.format(x, y))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\npaddle.init(use_gpu=False, trainer_count=1)\ndata = paddle.layer.data(name='data', type=paddle.data_type.integer_value_sequence(102100))\nlabel = paddle.layer.data(name='label', type=paddle.data_type.integer_value(2))\noutput = model(data)\nloss = paddle.layer.classification_cost(input=output, label=label)\nparameters = paddle.parameters.create(loss)\nprint(parameters.keys())\noptimizer = paddle.optimizer.Adam(learning_rate=0.001)\ntrainer = paddle.trainer.SGD(parameters=parameters, update_equation=optimizer, cost=loss)\npath = 'data\/imdb.pkl'\ndataset = imdb.Imdb(path)\ntrain_data_reader = dataset.create_reader('train')\ntest_data_reader = dataset.create_reader('test')\nfeeding = {'data': 0, 'label': 1}\n\ndef event_handler(event):\n    if isinstance(event, paddle.event.EndIteration):\n        if ((event.batch_id % 5) == 0):\n            class_error_rate = event.metrics['classification_error_evaluator']\n            print(('\\npass %d, Batch: %d cost: %f error: %s' % (event.pass_id, event.batch_id, event.cost, class_error_rate)))\n        else:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n    if isinstance(event, paddle.event.EndPass):\n        with gzip.open(('output\/params_pass_%d.tar.gz' % event.pass_id), 'w') as f:\n            parameters.to_tar(f)\n        result = trainer.test(reader=paddle.batch(test_data_reader, batch_size=32), feeding=feeding)\n        class_error_rate = result.metrics['classification_error_evaluator']\n        print(('\\nTest with Pass %d, cost: %s error: %f' % (event.pass_id, result.cost, class_error_rate)))\ntrainer.train(reader=paddle.batch(train_data_reader, batch_size=32), event_handler=event_handler, num_passes=10, feeding=feeding)\n"}
{"label_name":"train","label":0,"method_name":"train_tree","method":"\ntrain_data = []\ntrain_labels = []\ncounter = 0\nfor i in range(len(train_set)):\n    person_id = train_set[i][0]\n    product_id = train_set[i][1]\n    if ((product_id not in product_dict) or (person_id not in user_dict)):\n        counter += 1\n        continue\n    train_labels.append(train_set[i][(- 1)])\n    train_data.append((user_dict[person_id][:(- 1)] + product_dict[product_id][:(- 1)]))\nprint('Generate Traing Data')\nclf = tree.DecisionTreeClassifier()\nclf.fit(train_data, train_labels)\njoblib.dump(clf, 'model_param\/tree.model')\n"}
{"label_name":"predict","label":4,"method_name":"predict_author","method":"\nfiles = listdir(PATH)\ntrue = 0\nall = 0\nprint(' ________________________________________ ')\nprint('| real author | predicted author| matched ')\nfor file in files:\n    author = file.split('_')[0]\n    if (author in recognizer.authors):\n        all += 1\n        pred_author = recognizer.predict(path.join(PATH, file))\n        print(f\"| {author}{(' ' * (12 - len(author)))}|  {pred_author}{(' ' * (15 - len(pred_author)))}| {(author == pred_author)}\", end=' ')\n        if (author == pred_author):\n            true += 1\n            print()\n        else:\n            print(file)\nprint('|________________________________________ ')\nif (PATH != '..\/IlfPetrov\/'):\n    print(f'Accuracy {((100 * true) \/ all):.2f}%', end='\\n\\n')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nencoder_optimizer.zero_grad()\nloss = 0\nencoder_outputs = encoder(input_variable)\nloss = criterion(encoder_outputs, target_variable)\nloss.backward()\nencoder_optimizer.step()\nreturn loss.data[0]\n"}
{"label_name":"predict","label":4,"method_name":"decode_predictions","method":"\nglobal CLASS_INDEX\nif (CLASS_INDEX is None):\n    CLASS_INDEX = json.load(open(model_json))\nresults = []\nfor pred in preds:\n    top_indices = pred.argsort()[(- top):][::(- 1)]\n    for i in top_indices:\n        each_result = []\n        each_result.append(CLASS_INDEX[str(i)])\n        each_result.append(pred[i])\n        results.append(each_result)\nreturn results\n"}
{"label_name":"process","label":2,"method_name":"_process_image_files","method":"\n'Process and save list of images as TFRecord of Example protos.\\n\\n  Args:\\n    name: string, unique identifier specifying the data set\\n    filenames: list of strings; each string is a path to an image file\\n    synsets: list of strings; each string is a unique WordNet ID\\n    labels: list of integer; each integer identifies the ground truth\\n    humans: list of strings; each string is a human-readable label\\n    bboxes: list of bounding boxes for each image. Note that each entry in this\\n      list might contain from 0+ entries corresponding to the number of bounding\\n      box annotations for the image.\\n    num_shards: integer number of shards for this data set.\\n  '\nassert (len(filenames) == len(synsets))\nassert (len(filenames) == len(labels))\nassert (len(filenames) == len(humans))\nassert (len(filenames) == len(bboxes))\nspacing = np.linspace(0, len(filenames), (FLAGS.num_threads + 1)).astype(np.int)\nranges = []\nthreads = []\nfor i in range((len(spacing) - 1)):\n    ranges.append([spacing[i], spacing[(i + 1)]])\nprint(('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges)))\nsys.stdout.flush()\ncoord = tf.train.Coordinator()\ncoder = ImageCoder()\nthreads = []\nfor thread_index in range(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames, synsets, labels, humans, bboxes, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\ncoord.join(threads)\nprint(('%s: Finished writing all %d images in data set.' % (datetime.now(), len(filenames))))\nsys.stdout.flush()\n"}
{"label_name":"process","label":2,"method_name":"preprocess_for_eval","method":"\n'Preprocesses the given image for evaluation.\\n\\n  Args:\\n    image: A `Tensor` representing an image of arbitrary size.\\n    output_height: The height of the image after preprocessing.\\n    output_width: The width of the image after preprocessing.\\n\\n  Returns:\\n    A preprocessed image.\\n  '\ntf.image_summary('image', tf.expand_dims(image, 0))\nimage = tf.to_float(image)\nresized_image = tf.image.resize_image_with_crop_or_pad(image, output_width, output_height)\ntf.image_summary('resized_image', tf.expand_dims(resized_image, 0))\nreturn tf.image.per_image_whitening(resized_image)\n"}
{"label_name":"predict","label":4,"method_name":"evaluate_link_prediction_model","method":"\nlink_features_test = link_examples_to_features(link_examples_test, get_embedding, binary_operator)\nscore = evaluate_roc_auc(clf, link_features_test, link_labels_test)\nreturn score\n"}
{"label_name":"train","label":0,"method_name":"train_one_node","method":"\n'\\n    Gets data in the form of sparse matrix from `qc.dataprep.feature_stack` module\\n    which is ready for use in a machine learning model. Using the data trains a ml node\\n    and serialize the trained object to the secondary memory (hard-disk).\\n\\n    :argument:\\n        :param rp: Absolute path of the root directory of the project.\\n        :param cat_type: Type of categorical class `coarse` or any of the 6 main classes.\\n                                        (`abbr` | `desc` | `enty` | `hum` | `loc` | `num`)\\n        :param ml_algo: The type of machine learning models to be used. (svm | lr | linear_svm)\\n    :return:\\n        boolean_flag: True for successful operation.\\n        model: trained SVC model\\n    '\nx_ft = get_ft_obj('training', rp, ml_algo, cat_type)\nlabels = read_file('{0}_classes_training'.format(cat_type), rp)[1]\ny_lb = [remove_endline_char(c).strip() for c in labels]\nmachine = None\nif (ml_algo == 'svm'):\n    machine = svm.SVC()\nelif (ml_algo == 'linear_svm'):\n    machine = svm.LinearSVC()\nelif (ml_algo == 'lr'):\n    machine = linear_model.LogisticRegression(solver='newton-cg')\nelse:\n    print('- Error while training {0} model. {0} is unexpected ML algorithm'.format(ml_algo))\nmodel = machine.fit(x_ft, y_lb)\nmw_flag = write_obj(model, '{0}_model'.format(cat_type), (rp + '\/{0}'.format(ml_algo)))\nif mw_flag:\n    print('- Training done for {0} model of {1}'.format(cat_type, ml_algo))\n    return True\nelse:\n    print('- Error in writing trained {0} model of {1}'.format(cat_type, ml_algo))\n    return False\n"}
{"label_name":"process","label":2,"method_name":"read_process","method":"\n(pipein, pipeout) = os.popen4(('%s %s' % (cmd, args)))\ntry:\n    firstline = pipeout.readline()\n    if re.search('(not recognized|No such file|not found)', firstline, re.IGNORECASE):\n        raise IOError(('%s must be on your system path.' % cmd))\n    output = (firstline + pipeout.read())\nfinally:\n    pipeout.close()\nreturn output\n"}
{"label_name":"process","label":2,"method_name":"_process_dataset","method":"\n'Process a complete data set and save it as a TFRecord.\\n\\n  Args:\\n    name: string, unique identifier specifying the data set.\\n    directory: string, root path to the data set.\\n    num_shards: integer number of shards for this data set.\\n    labels_file: string, path to the labels file.\\n  '\n(filenames, texts, labels) = _find_image_files(directory, labels_file)\n_process_image_files(name, filenames, texts, labels, num_shards)\n"}
{"label_name":"train","label":0,"method_name":"mxnet_training_job","method":"\nwith timeout(minutes=TRAINING_DEFAULT_TIMEOUT_MINUTES):\n    script_path = os.path.join(DATA_DIR, 'mxnet_mnist', 'mnist_neo.py')\n    data_path = os.path.join(DATA_DIR, 'mxnet_mnist')\n    mx = MXNet(entry_point=script_path, role='SageMakerRole', framework_version=mxnet_training_latest_version, py_version=mxnet_training_latest_py_version, instance_count=1, instance_type=cpu_instance_type, sagemaker_session=sagemaker_session)\n    train_input = mx.sagemaker_session.upload_data(path=os.path.join(data_path, 'train'), key_prefix='integ-test-data\/mxnet_mnist\/train')\n    test_input = mx.sagemaker_session.upload_data(path=os.path.join(data_path, 'test'), key_prefix='integ-test-data\/mxnet_mnist\/test')\n    mx.fit({'train': train_input, 'test': test_input})\n    return mx.latest_training_job.name\n"}
{"label_name":"process","label":2,"method_name":"process_str","method":"\ncode = [header]\ncode.extend(parse_string(astr, global_names, 0, 1))\nreturn ''.join(code)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nprobability = calculateClassProbablity(summaries, vector)\nclass_ = None\nbest_prob = 0\nfor (key, value) in probability.items():\n    if (value > best_prob):\n        class_ = key\n        best_prob = value\nreturn class_\n"}
{"label_name":"train","label":0,"method_name":"train_project","method":"\n\" train_project: populates and ECNet project's folder structure with\\n    neural networks, selects best neural networks in each pool\\n\\n    Args:\\n        prj_name (str): name of the project\\n        num_pools (int): number of pools in the project\\n        num_candidates (int): number of candidates per pool\\n        df (DataFrame): currently loaded DataFrame\\n        sets (PackagedData): learn, validation, test sets\\n        vars (dict): learning\/architecture variables\\n        shuffle (str): shuffles None, `train`, `all` sets for each candidate\\n        split (list): if shuffling, [learn%, valid%, test%]\\n        retrain (bool): if True, uses existing project models for additional\\n            training\\n        selection_set (str): best candidates\/pool are selected using this\\n            set; `learn`, `valid`, `train`, `test`, None (all data)\\n        selection_fn (str): candidates are selected based on this error\\n            metric; `rmse`, `mean_abs_error`, `med_abs_error`\\n        num_processes (int): number of concurrent processes used to train\\n    \"\nif (name != 'nt'):\n    set_start_method('spawn', force=True)\nlogger.log('info', 'Training {}x{} models'.format(num_pools, num_candidates), call_loc='TRAIN')\nlogger.log('debug', 'Arguments:\\n\\t| shuffle:\\t\\t{}\\n\\t| split:\\t\\t{}\\n\\t| retrain:\\t\\t{}\\n\\t| validate\\t\\t{}\\n\\t| selection_set:\\t{}\\n\\t| selection_fn:\\t\\t{}'.format(shuffle, split, retrain, validate, selection_set, selection_fn), call_loc='TRAIN')\npool_errors = [[] for _ in range(num_pools)]\nif (num_processes > 1):\n    train_pool = Pool(processes=num_processes)\nfor pool in range(num_pools):\n    for candidate in range(num_candidates):\n        filename = get_candidate_path(prj_name, pool, candidate, model=True)\n        save_df(df, filename.replace('model.h5', 'data.d'))\n        if (num_processes > 1):\n            pool_errors[pool].append(train_pool.apply_async(train_model, [sets, vars, selection_set, selection_fn, retrain, filename, validate]))\n        else:\n            pool_errors[pool].append(train_model(sets, vars, selection_set, selection_fn, retrain, filename, validate)[0])\n        if (shuffle is not None):\n            df.shuffle(sets=shuffle, split=split)\n            sets = df.package_sets()\nif (num_processes > 1):\n    train_pool.close()\n    train_pool.join()\n    for (p_idx, pool) in enumerate(pool_errors):\n        pool_errors[p_idx] = [e.get()[0] for e in pool]\nlogger.log('debug', 'Pool errors: {}'.format(pool_errors), call_loc='TRAIN')\nfor (p_idx, pool) in enumerate(pool_errors):\n    candidate_fp = get_candidate_path(prj_name, p_idx, min(enumerate(pool), key=itemgetter(1))[0], model=True)\n    pool_fp = get_candidate_path(prj_name, p_idx, p_best=True)\n    resave_model(candidate_fp, pool_fp)\n    resave_df(candidate_fp.replace('model.h5', 'data.d'), pool_fp.replace('model.h5', 'data.d'))\n"}
{"label_name":"save","label":1,"method_name":"save_spec","method":"\n\"\\n    Save a protobuf model specification to file.\\n\\n    Parameters\\n    ----------\\n    spec: Model_pb\\n        Protobuf representation of the model\\n\\n    filename: str\\n        File path  where the spec gets saved.\\n\\n    auto_set_specification_version: bool\\n        If true, will always try to set specification version automatically.\\n\\n    Examples\\n    --------\\n    .. sourcecode:: python\\n\\n        >>> coremltools.utils.save_spec(spec, 'HousePricer.mlmodel')\\n\\n    See Also\\n    --------\\n    load_spec\\n    \"\n(name, ext) = _os.path.splitext(filename)\nif (not ext):\n    filename = '{}.mlmodel'.format(filename)\nelif (ext != '.mlmodel'):\n    raise Exception('Extension must be .mlmodel (not {})'.format(ext))\nspec = spec.SerializeToString()\nif auto_set_specification_version:\n    try:\n        from ..libcoremlpython import _MLModelProxy\n        spec = _MLModelProxy.auto_set_specification_version(spec)\n    except Exception as e:\n        print(e)\n        _warnings.warn('Failed to automatic set specification version for this model.', RuntimeWarning)\nwith open(filename, 'wb') as f:\n    f.write(spec)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n'\\n    Train a model on a dataset. Saves model and hyperparams so that training\\n    can be easily resumed\\n    Parameters:\\n        task_name: Name of the task like NER1 etc. More like a unique identifier\\n        model_cls: Model class name. Should inherit from sequence_tagging.Tagger class\\n        dataset_fn: Dataset function. Must return a dict. See data\/conll.py\\n        hparams: Hyperparameters defined for the particular model. Not required when\\n                resuming training\\n        hparams_map: mapping between model classes and hparams. Only used of hparams is\\n                None\\n        num_epochs: Set to 100. With early stopping set number of epochs will usually\\n                be lower. Set PREFS.early_stopping to desired criteria\\n        checkpoint: Saved checkpoint number (train iteration). Used to resume training\\n        use_iob_metrics: If True then adds IOB metrics (precision\/recall\/F1) to Evaluator\\n        early_stopping: Early stopping criteria. Should be of the form\\n                        [lowest\/highest]_[window_Size]_[metric] where metric is one of\\n                        loss, acc, acc-seq, precision, recall, F1. Default is picked\\n                        from PREFS.early_stopping\\n    '\nearly_stopping = (early_stopping or PREFS.early_stopping)\nif (checkpoint is None):\n    if (hparams is None):\n        hparams = hparams_map[model_cls]\n    dataset = dataset_fn(hparams.batch_size)\n    model = model_cls.create(task_name, hparams, vocabs=dataset['vocabs'], overwrite=PREFS.overwrite_model_dir)\nelse:\n    (model, hparams_loaded) = model_cls.load(task_name, checkpoint)\n    if (hparams is None):\n        hparams = hparams_loaded\n    dataset = dataset_fn(hparams.batch_size)\n(train_iter, validation_iter, _) = dataset['iters']\n(_, _, tag_vocab) = dataset['vocabs']\nmetrics = [BasicMetrics(output_vocab=tag_vocab)]\nif use_iob_metrics:\n    metrics += [IOBMetrics(tag_vocab=tag_vocab)]\nevaluator = Evaluator(validation_iter, *metrics)\ntrainer = Trainer(task_name, model, hparams, train_iter, evaluator)\n(best_cp, _) = trainer.train(num_epochs, early_stopping=early_stopping)\nreturn best_cp\n"}
{"label_name":"process","label":2,"method_name":"image_process_retinopathy","method":"\nfullpath_image = image\nif ((not cv2_flag) and (not debug_mode)):\n    if (image_type == 0):\n        image = PIL.Image.open(image).convert('L')\n    else:\n        image = PIL.Image.open(image)\n    if (to_save or to_predict):\n        image_array = np.array(image)\n        image = PIL.Image.fromarray(image_array)\n        (width2, height2) = image.size\n        to_write = ((('BEFORE RESIZE:\\nwidth: ' + str(width2)) + ' || height: ') + str(height2))\n        LOGGER.write_to_logger(to_write)\n        pt('width2', width2)\n        pt('height2', height2)\n        image = np.array(image.resize((width, height)))\n        to_write = (((((('AFTER RESIZE:\\nwidth: ' + str(width)) + ' || height: ') + str(height)) + ' || ') + 'image.shape: ') + str(image.shape))\n        LOGGER.write_to_logger(to_write)\n        pt('image', image.shape)\n        if (image.shape[2] == 4):\n            image = image[:, :, :3]\n            to_write = (((((('AFTER REMOVE ALPHA:\\nwidth: ' + str(width)) + ' || height: ') + str(height)) + ' || ') + 'image.shape: ') + str(image.shape))\n            LOGGER.write_to_logger(to_write)\n        if to_save:\n            path_to_save = os.path.dirname(fullpath_image)\n            filename = os.path.basename(fullpath_image)[:(- 5)]\n            if is_test:\n                folder = '\\\\test\\\\'\n                pass\n            else:\n                folder = '\\\\train\\\\'\n            fullpath_to_save = ((path_to_save + folder) + filename)\n            folders.create_directory_from_fullpath(fullpath=fullpath_to_save)\n            PIL.Image.fromarray(image).save((fullpath_to_save + '.jpeg'))\n    else:\n        image = np.array(image)\n    return (image \/ 255.0)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\nX = np.array([i[0] for i in training_data]).reshape((- 1), len(training_data[0][0]), 1)\ny = [i[1] for i in training_data]\nif (not model):\n    model = neural_network_model(input_size=len(X[0]))\nmodel.fit(X, y, n_epoch=num_epochs, snapshot_step=500, show_metric=True, run_id='openai_learning')\nreturn model\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n\"Preprocesses the given image.\\n\\n  Args:\\n    image: A `Tensor` representing an image of arbitrary size.\\n    output_height: The height of the image after preprocessing.\\n    output_width: The width of the image after preprocessing.\\n    is_training: `True` if we're preprocessing the image for training and\\n      `False` otherwise.\\n\\n  Returns:\\n    A preprocessed image.\\n  \"\nimage = tf.to_float(image)\nimage = tf.image.resize_image_with_crop_or_pad(image, output_width, output_height)\nimage = tf.sub(image, 128.0)\nimage = tf.div(image, 128.0)\nreturn image\n"}
{"label_name":"process","label":2,"method_name":"preprocess_style_transfer","method":"\n\"Preprocesses the image and labels for style transfer purposes.\\n\\n  Args:\\n    image: A `Tensor` of size [height, width, 3].\\n    labels: A dictionary of labels.\\n    augment: Whether to apply data augmentation to inputs\\n    size: The height and width to which images should be resized. If left as\\n      `None`, then no resizing is performed\\n    is_training: Whether or not we're training the model\\n\\n  Returns:\\n    The preprocessed image and labels. Scaled to [-1, 1]\\n  \"\nimage = tf.image.convert_image_dtype(image, tf.float32)\nif (augment and is_training):\n    image = image_augmentation(image)\nif size:\n    image = resize_image(image, size)\nimage -= 0.5\nimage *= 2\nreturn (image, labels)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nstartstep = (0 if (not is_finetune) else int(FLAGS.finetune_dir.split('-')[(- 1)]))\n(image_filenames, label_filenames) = AirNet.get_filename_list(FLAGS.train_dir)\n(val_image_filenames, val_label_filenames) = AirNet.get_filename_list(FLAGS.val_dir)\nwith tf.Graph().as_default():\n    (images, labels, is_training, keep_prob) = AirNet.placeholder_inputs(batch_size=FLAGS.batch_size)\n    (images, labels) = AirNet.dataset_inputs(image_filenames, label_filenames, FLAGS.batch_size)\n    (val_images, val_labels) = AirNet.dataset_inputs(val_image_filenames, val_label_filenames, FLAGS.eval_batch_size, False)\n    if (FLAGS.model == 'basic'):\n        logits = AirNet.inference_basic(images, is_training)\n    elif (FLAGS.model == 'extended'):\n        logits = AirNet.inference_extended(images, is_training)\n    elif (FLAGS.model == 'basic_dropout'):\n        logits = AirNet.inference_basic_dropout(images, is_training, keep_prob)\n    elif (FLAGS.model == 'extended_dropout'):\n        logits = AirNet.inference_extended_dropout(images, is_training, keep_prob)\n    else:\n        raise ValueError('The selected model does not exist')\n    loss = AirNet.loss_calc(logits=logits, labels=labels)\n    (train_op, global_step) = AirNet.training(loss=loss)\n    accuracy = tf.argmax(logits, axis=3)\n    summary = tf.summary.merge_all()\n    saver = tf.train.Saver(max_to_keep=100000)\n    with tf.Session() as sess:\n        if is_finetune:\n            print('\\n =====================================================')\n            print('  Finetuning with model: ', FLAGS.model)\n            print('\\n    Batch size is: ', FLAGS.batch_size)\n            print('    ckpt files are saved to: ', FLAGS.log_dir)\n            print('    Max iterations to train is: ', FLAGS.max_steps)\n            print(' =====================================================')\n            saver.restore(sess, FLAGS.finetune_dir)\n        else:\n            print('\\n =====================================================')\n            print('  Training from scratch with model: ', FLAGS.model)\n            print('\\n    Batch size is: ', FLAGS.batch_size)\n            print('    ckpt files are saved to: ', FLAGS.log_dir)\n            print('    Max iterations to train is: ', FLAGS.max_steps)\n            print(' =====================================================')\n            sess.run(tf.variables_initializer(tf.global_variables()))\n            sess.run(tf.local_variables_initializer())\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n        ' Starting iterations to train the network '\n        for step in range((startstep + 1), ((startstep + FLAGS.max_steps) + 1)):\n            (images_batch, labels_batch) = sess.run(fetches=[images, labels])\n            train_feed_dict = {images: images_batch, labels: labels_batch, is_training: True, keep_prob: 0.5}\n            start_time = time.time()\n            (_, train_loss_value, train_accuracy_value, train_summary_str) = sess.run([train_op, loss, accuracy, summary], feed_dict=train_feed_dict)\n            duration = (time.time() - start_time)\n            if ((step % 10) == 0):\n                examples_per_sec = (FLAGS.batch_size \/ duration)\n                sec_per_batch = float(duration)\n                print('\\n--- Normal training ---')\n                format_str = '%s: step %d, loss = %.2f (%.1f examples\/sec; %.3f sec\/batch)'\n                print((format_str % (datetime.now(), step, train_loss_value, examples_per_sec, sec_per_batch)))\n                pred = sess.run(logits, feed_dict=train_feed_dict)\n                AirNet.per_class_acc(pred, labels_batch)\n                train_writer.add_summary(train_summary_str, step)\n                train_writer.flush()\n            if (((step % 100) == 0) or ((step + 1) == FLAGS.max_steps)):\n                test_iter = (FLAGS.num_examples_epoch_test \/\/ FLAGS.batch_size)\n                ' Validate training by running validation dataset '\n                print('\\n===========================================================')\n                print('--- Running test on VALIDATION dataset ---')\n                total_val_loss = 0.0\n                hist = np.zeros((FLAGS.num_class, FLAGS.num_class))\n                for val_step in range(test_iter):\n                    (val_images_batch, val_labels_batch) = sess.run(fetches=[val_images, val_labels])\n                    val_feed_dict = {images: val_images_batch, labels: val_labels_batch, is_training: True, keep_prob: 1.0}\n                    (_val_loss, _val_pred) = sess.run(fetches=[loss, logits], feed_dict=val_feed_dict)\n                    total_val_loss += _val_loss\n                    hist += AirNet.get_hist(_val_pred, val_labels_batch)\n                print('Validation Loss: ', (total_val_loss \/ test_iter), '. If this value increases the model is likely overfitting.')\n                AirNet.print_hist_summery(hist)\n                print('===========================================================')\n            if (((step % 1000) == 0) or ((step % 500) == 0) or ((step + 1) == FLAGS.max_steps)):\n                print('\\n--- SAVING SESSION ---')\n                checkpoint_path = os.path.join(FLAGS.log_dir, 'model.ckpt')\n                saver.save(sess, checkpoint_path, global_step=step)\n                print('=========================')\n        coord.request_stop()\n        coord.join(threads)\n"}
{"label_name":"predict","label":4,"method_name":"make_prediction","method":"\n'Make some classification predictions on a toy dataset using a SVC\\n\\n    If binary is True restrict to a binary classification problem instead of a\\n    multiclass classification problem\\n    '\nif (dataset is None):\n    dataset = datasets.load_iris()\nX = dataset.data\ny = dataset.target\nif binary:\n    (X, y) = (X[(y < 2)], y[(y < 2)])\n(n_samples, n_features) = X.shape\np = np.arange(n_samples)\nrng = check_random_state(37)\nrng.shuffle(p)\n(X, y) = (X[p], y[p])\nhalf = int((n_samples \/ 2))\nrng = np.random.RandomState(0)\nX = np.c_[(X, rng.randn(n_samples, (200 * n_features)))]\nclf = svm.SVC(kernel='linear', probability=True, random_state=0)\nprobas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])\nif binary:\n    probas_pred = probas_pred[:, 1]\ny_pred = clf.predict(X[half:])\ny_true = y[half:]\nreturn (y_true, y_pred, probas_pred)\n"}
{"label_name":"predict","label":4,"method_name":"make_dnn_prediction","method":"\nreturn_list = True\nif (not isinstance(functions, (tuple, list))):\n    functions = [functions]\n    return_list = False\nn_functions = len(functions)\nresults = [[] for i in range(n_functions)]\nn_samples = len(X)\nprog = Progbar(target=n_samples, print_summary=True, name=('Making prediction: %s' % str(title)))\nif isinstance(X, F.Feeder):\n    y_true = []\n    for (x, y) in X.set_batch(batch_size=batch_size):\n        for (res, fn) in zip(results, functions):\n            res.append(fn(x))\n        prog.add(x.shape[0])\n        y_true.append((np.argmax(y, axis=(- 1)) if (y.ndim == 2) else y))\n    results = [np.concatenate(res, axis=0) for res in results]\n    y_true = np.concatenate(y_true, axis=0)\n    if return_list:\n        return (results, y_true)\n    return (results[0], y_true)\nelse:\n    for (start, end) in minibatch(batch_size=batch_size, n=n_samples):\n        y = X[start:end]\n        for (res, fn) in zip(results, functions):\n            res.append(fn(y))\n        prog.add((end - start))\n    results = [np.concatenate(res, axis=0) for res in results]\n    if return_list:\n        return results\n    return results[0]\n"}
{"label_name":"save","label":1,"method_name":"save_pred","method":"\nprint('Read test images name for submission file')\npath = os.path.join('imgs', 'test', '*.jpg')\nfiles = glob.glob(path)\nX_test_id = []\nfor file in files:\n    X_test_id.append(os.path.basename(file))\npreds = preds.clip(min=0.05, max=0.995)\npreds_df = pd.DataFrame(preds, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\npreds_df['img'] = X_test_id\nprint('Saving predictions')\nnow = datetime.datetime.now()\nif (not os.path.isdir('subm')):\n    os.mkdir('subm')\nsuffix = ((info + '_') + str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file = os.path.join('subm', (('submission_' + suffix) + '.csv'))\npreds_df.to_csv(sub_file, index=False)\n"}
{"label_name":"train","label":0,"method_name":"validate_crp_constrained_input","method":"\nvalidate_dependency_constraints(N, Cd, Ci)\nfor c in Rd:\n    col_dep = Rd[c]\n    row_dep = Ri.get(c, {})\n    validate_dependency_constraints(None, col_dep, row_dep)\nfor block in Cd:\n    for (a, b) in it.combinations(block, 2):\n        if (not check_compatible_customers(Cd, Ci, Ri, Rd, a, b)):\n            raise ValueError('Incompatible row constraints for dep cols.')\nreturn True\n"}
{"label_name":"train","label":0,"method_name":"trainable_parameters","method":"\nreturn [x for x in model.parameters() if x.requires_grad]\n"}
{"label_name":"train","label":0,"method_name":"parse_constraints","method":"\nntags = len(res)\nnwords = len(inpt)\nnpostags = len([x for x in res if is_pos_tag(x)])\nnclose = len([x for x in res if (x[0] == '\/')])\nnopen = ((ntags - nclose) - npostags)\nreturn (abs((npostags - nwords)), abs((nclose - nopen)))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nimport time\nt0 = time.time()\n\ndef dt():\n    return round((time.time() - t0), 2)\nprint('+{}s: Importing libraries'.format(dt()))\nimport pickle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom rpscv import imgproc as imp\nfrom rpscv import utils\nprint('+{}s: Generating image data'.format(dt()))\n(features, labels) = imp.generateGrayFeatures(nbImg=nbImg, verbose=False, rs=rs)\n(unique, count) = np.unique(labels, return_counts=True)\nfor (i, label) in enumerate(unique):\n    print('  {}: {} images'.format(utils.gestureTxt[label], count[i]))\nprint('+{}s: Generating test set'.format(dt()))\nsssplit = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=rs)\nfor (train_index, test_index) in sssplit.split(features, labels):\n    features_train = features[train_index]\n    features_test = features[test_index]\n    labels_train = labels[train_index]\n    labels_test = labels[test_index]\nprint('+{}s: Defining pipeline'.format(dt()))\nsteps = [('pca', PCA()), ('clf', SVC(kernel='rbf'))]\npipe = Pipeline(steps)\nprint('+{}s: Defining cross-validation'.format(dt()))\ncv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=rs)\nprint('+{}s: Defining grid search'.format(dt()))\ngrid_params = dict(pca__n_components=pca__n_components, clf__gamma=clf__gamma, clf__C=clf__C)\ngrid = GridSearchCV(pipe, grid_params, scoring=scoring, n_jobs=n_jobs, refit=True, cv=cv, verbose=1)\nprint('Grid search parameters:')\nprint(grid)\nt0_train = time.time()\nprint('+{}s: Fitting classifier'.format(dt()))\ngrid.fit(features_train, labels_train)\ndt_train = (time.time() - t0_train)\nif cvScore:\n    cvres = grid.cv_results_\n    print('Cross-validation results:')\n    for (score, std, params) in zip(cvres['mean_test_score'], cvres['std_test_score'], cvres['params']):\n        print('  {}, {}, {}'.format(round(score, 4), round(std, 5), params))\nprint('Grid search best score: {}'.format(grid.best_score_))\nprint('Grid search best parameters:')\nfor (key, value) in grid.best_params_.items():\n    print('  {}: {}'.format(key, value))\nprint('+{}s: Validating classifier on test set'.format(dt()))\npred = grid.predict(features_test)\nscore = f1_score(labels_test, pred, average='micro')\nprint('Classifier f1-score on test set: {}'.format(score))\nprint('Confusion matrix:')\nprint(confusion_matrix(labels_test, pred))\nprint('Classification report:')\ntn = [utils.gestureTxt[i] for i in range(3)]\nprint(classification_report(labels_test, pred, target_names=tn))\nprint('+{}s: Writing classifier to {}'.format(dt(), pklFilename))\nwith open(pklFilename, 'wb') as f:\n    f.flush()\n    pickle.dump(grid, f)\nprint('+{}s: Done!'.format(dt()))\nreturn (grid.best_score_, score, dt_train)\n"}
{"label_name":"save","label":1,"method_name":"save_bar_graph","method":"\nplt.clf()\nsns.set_style('whitegrid')\nax = sns.barplot(x=x, y=y)\nfor item in ax.get_xticklabels():\n    item.set_rotation(15)\nplt.savefig(file_name)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\nif fluid.core.is_compiled_with_xpu():\n    selected_devices = os.getenv('FLAGS_selected_xpus')\nelse:\n    selected_devices = os.getenv('FLAGS_selected_gpus')\ntrainer_id = int(os.getenv('PADDLE_TRAINER_ID'))\nworker_endpoints_env = os.getenv('PADDLE_TRAINER_ENDPOINTS')\ncurrent_endpoint = os.getenv('PADDLE_CURRENT_ENDPOINT')\nworker_endpoints = worker_endpoints_env\ntrainers_num = len(worker_endpoints.split(','))\nname = 'selected_devices:{} worker_endpoints:{} trainers_num:{} current_endpoint:{} trainer_id:{}'.format(selected_devices, worker_endpoints, trainers_num, current_endpoint, trainer_id)\nprint(name)\nwith open('{}.check_{}.log'.format(prefix, trainer_id), 'w') as f:\n    f.write(name)\n"}
{"label_name":"save","label":1,"method_name":"save_smi","method":"\nif (not os.path.exists('epoch_data')):\n    os.makedirs('epoch_data')\nsmi_file = os.path.join('epoch_data', '{}.smi'.format(name))\nwith open(smi_file, 'w') as afile:\n    afile.write('\\n'.join(smiles))\nreturn\n"}
{"label_name":"process","label":2,"method_name":"processAbilities","method":"\nfName = open('C:\\\\Program Files (x86)\\\\Steam\\\\steamapps\\\\common\\\\dota 2 beta\\\\game\\\\dota\\\\scripts\\\\npc\\\\npc_abilities.txt', 'r')\ncontent = fName.readlines()\ncontent = [x.strip() for x in content]\nfName.close()\nshiftCount = 0\nabilitiesCount = 0\ncurrName = None\nabilityID = None\nlineCount = 0\nabilities = {}\nfor line in content:\n    lineCount += 1\n    if (line == '{'):\n        shiftCount += 1\n        continue\n    if (line == '}'):\n        shiftCount -= 1\n        if (shiftCount == 1):\n            currName = None\n            abilityID = None\n        continue\n    if (shiftCount == 1):\n        comment = (line[:2] == '\/\/')\n        underscore = line.find('_')\n        if ((not comment) and (underscore > 0)):\n            abilitiesCount += 1\n            currName = line.strip().replace('\"', '')\n    if (shiftCount == 2):\n        comment = (line[:2] == '\/\/')\n        if (not comment):\n            if (line[:4] == '\"ID\"'):\n                res = re.split('\\\\s{1,}', line)\n                res[1] = res[1].replace('\"', '').replace('\/\/', '')\n                abilityID = res[1]\n                abilities[res[1]] = {}\n                if (currName != None):\n                    abilities[res[1]]['Name'] = currName\n                    if ('special_' in currName):\n                        abilities[res[1]]['Talent'] = 1\n                else:\n                    print('ERROR: Name missing!')\n                    break\n            if (line[:17] == '\"AbilityBehavior\"'):\n                if (line.find('DOTA_ABILITY_BEHAVIOR_HIDDEN') >= 0):\n                    if (abilityID != None):\n                        abilities[abilityID]['Hidden'] = 1\n                    else:\n                        print('Error: AbilityID missing!')\n                        break\n                if (line.find('DOTA_ABILITY_BEHAVIOR_PASSIVE') >= 0):\n                    if (abilityID != None):\n                        abilities[abilityID]['Passive'] = 1\n                    else:\n                        print('Error: AbilityID missing!')\n                        break\n                if (line.find('DOTA_ABILITY_BEHAVIOR_AOE') >= 0):\n                    if (abilityID != None):\n                        abilities[abilityID]['AOE'] = 1\n                    else:\n                        print('Error: AbilityID missing!')\n                        break\n                if (line.find('DOTA_ABILITY_BEHAVIOR_AURA') >= 0):\n                    if (abilityID != None):\n                        abilities[abilityID]['Aura'] = 1\n                    else:\n                        print('Error: AbilityID missing!')\n                        break\n                tgt_mask = 0\n                if (line.find('DOTA_ABILITY_BEHAVIOR_POINT') >= 0):\n                    tgt_mask |= 1\n                if (line.find('DOTA_ABILITY_BEHAVIOR_UNIT_TARGET') >= 0):\n                    tgt_mask |= 2\n                if (line.find('DOTA_ABILITY_BEHAVIOR_NO_TARGET') >= 0):\n                    tgt_mask |= 4\n                if (abilityID != None):\n                    abilities[abilityID]['TargetMask'] = tgt_mask\n                else:\n                    print('Error: AbilityID missing!')\n                    break\n            if (line[:23] == '\"AbilityUnitTargetTeam\"'):\n                tgt_team = None\n                if (line.find('DOTA_UNIT_TARGET_TEAM_ENEMY') >= 0):\n                    tgt_team = 1\n                elif (line.find('DOTA_UNIT_TARGET_TEAM_FRIENDLY') >= 0):\n                    tgt_team = 2\n                elif (line.find('DOTA_UNIT_TARGET_TEAM_BOTH') >= 0):\n                    tgt_team = 3\n                elif (line.find('DOTA_UNIT_TARGET_TEAM_CUSTOM') >= 0):\n                    tgt_team = 4\n                if (abilityID != None):\n                    abilities[abilityID]['TargetTeam'] = tgt_team\n                else:\n                    print('Error: AbilityID missing!')\n                    break\n            if (line[:23] == '\"AbilityUnitDamageType\"'):\n                dmg_type = 0\n                if (line.find('DAMAGE_TYPE_PURE') >= 0):\n                    dmg_type = 3\n                elif (line.find('DAMAGE_TYPE_MAGICAL') >= 0):\n                    dmg_type = 2\n                elif (line.find('DAMAGE_TYPE_PHYSICAL') >= 0):\n                    dmg_type = 1\n                if (abilityID != None):\n                    abilities[abilityID]['DamageType'] = dmg_type\n                else:\n                    print('Error: AbilityID missing!')\n                    break\n            if (line[:13] == '\"AbilityType\"'):\n                if (line.find('DOTA_ABILITY_TYPE_ULTIMATE') >= 0):\n                    if (abilityID != None):\n                        abilities[abilityID]['Ultimate'] = 1\n                    else:\n                        print('Error: AbilityID missing!')\n                        break\n            if (line[:18] == '\"AbilityCastPoint\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                multi = re.split('\\\\s{1,}', res[1])\n                if (not multi):\n                    multi = [res[1]]\n                if (abilityID != None):\n                    abilities[abilityID]['Castpoint'] = []\n                    for value in multi:\n                        abilities[abilityID]['Castpoint'].append(float(value))\n                else:\n                    print('Error: AbilityID missing!')\n                    break\n            if (line[:17] == '\"AbilityCooldown\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                multi = re.split('\\\\s{1,}', res[1])\n                if (not multi):\n                    multi = [res[1]]\n                if (abilityID != None):\n                    abilities[abilityID]['Cooldown'] = []\n                    for value in multi:\n                        abilities[abilityID]['Cooldown'].append(float(value))\n                else:\n                    print('Error: AbilityID missing!')\n                    break\n            if (line[:17] == '\"AbilityManaCost\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                multi = re.split('\\\\s{1,}', res[1])\n                if (not multi):\n                    multi = [res[1]]\n                if (abilityID != None):\n                    abilities[abilityID]['Manacost'] = []\n                    for value in multi:\n                        abilities[abilityID]['Manacost'].append(float(value))\n                else:\n                    print('Error: AbilityID missing!')\n                    break\n            if (line[:15] == '\"RequiredLevel\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                if (abilityID != None):\n                    abilities[abilityID]['LevelAvailable'] = int(res[1])\n                else:\n                    print('ERROR: abilityID missing!')\n                    break\n            if (line[:23] == '\"LevelsBetweenUpgrades\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                if (abilityID != None):\n                    abilities[abilityID]['LevelsBetweenUpgrades'] = int(res[1])\n                else:\n                    print('ERROR: abilityID missing!')\n                    break\nprint(('Processed %d abilities' % abilitiesCount))\nwriteData(abilities, 'abilities.json')\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\npaths = Paths(params)\nlog = ModelCloudLog(os.path.join(paths.root_path, 'logs'))\nstart = time.time()\nmodel_variable_scope = paths.var_scope\nlog.log_parameters(params, y_train.shape[0], y_valid.shape[0], y_test.shape[0])\ngraph = tf.Graph()\nwith graph.as_default():\n    tf_x_batch = tf.placeholder(tf.float32, shape=(None, params.image_size[0], params.image_size[1], 1))\n    tf_y_batch = tf.placeholder(tf.float32, shape=(None, params.num_classes))\n    is_training = tf.placeholder(tf.bool)\n    current_epoch = tf.Variable(0, trainable=False)\n    if params.learning_rate_decay:\n        learning_rate = tf.train.exponential_decay(params.learning_rate, current_epoch, decay_steps=params.max_epochs, decay_rate=0.01)\n    else:\n        learning_rate = params.learning_rate\n    with tf.variable_scope(model_variable_scope):\n        logits = model_pass(tf_x_batch, params, is_training)\n        if params.l2_reg_enabled:\n            with tf.variable_scope('fc4', reuse=True):\n                l2_loss = tf.nn.l2_loss(tf.get_variable('weights'))\n        else:\n            l2_loss = 0\n    predictions = tf.nn.softmax(logits)\n    softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_batch)\n    loss = (tf.reduce_mean(softmax_cross_entropy) + (params.l2_lambda * l2_loss))\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\nwith tf.Session(graph=graph) as session:\n    session.run(tf.global_variables_initializer())\n\n    def get_accuracy_and_loss_in_batches(X, y):\n        p = []\n        sce = []\n        batch_iterator = BatchIterator(batch_size=128)\n        for (x_batch, y_batch) in batch_iterator(X, y):\n            [p_batch, sce_batch] = session.run([predictions, softmax_cross_entropy], feed_dict={tf_x_batch: x_batch, tf_y_batch: y_batch, is_training: False})\n            p.extend(p_batch)\n            sce.extend(sce_batch)\n        p = np.array(p)\n        sce = np.array(sce)\n        accuracy = ((100.0 * np.sum((np.argmax(p, 1) == np.argmax(y, 1)))) \/ p.shape[0])\n        loss = np.mean(sce)\n        return (accuracy, loss)\n    if params.resume_training:\n        try:\n            tf.train.Saver().restore(session, paths.model_path)\n        except Exception as e:\n            log('\u041d\u0435\u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438: \u0444\u0430\u0439\u043b \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d.')\n            pass\n    saver = tf.train.Saver()\n    early_stopping = EarlyStopping(tf.train.Saver(), session, patience=params.early_stopping_patience, minimize=True)\n    train_loss_history = np.empty([0], dtype=np.float32)\n    train_accuracy_history = np.empty([0], dtype=np.float32)\n    valid_loss_history = np.empty([0], dtype=np.float32)\n    valid_accuracy_history = np.empty([0], dtype=np.float32)\n    if (params.max_epochs > 0):\n        log('================= \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 ==================')\n    else:\n        log('=============== \u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 ================')\n    log((' Timestamp: ' + get_time_hhmmss()))\n    log.sync()\n    for epoch in range(params.max_epochs):\n        current_epoch = epoch\n        batch_iterator = BatchIterator(batch_size=params.batch_size, shuffle=True)\n        for (x_batch, y_batch) in batch_iterator(X_train, y_train):\n            session.run([optimizer], feed_dict={tf_x_batch: x_batch, tf_y_batch: y_batch, is_training: True})\n        if ((epoch % params.log_epoch) == 0):\n            (valid_accuracy, valid_loss) = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n            (train_accuracy, train_loss) = get_accuracy_and_loss_in_batches(X_train, y_train)\n            if ((epoch % params.print_epoch) == 0):\n                log(('-------------- %4d\/%d \u042d\u043f\u043e\u0445\u0430 --------------' % (epoch, params.max_epochs)))\n                log(('     Train loss: %.8f, accuracy: %.2f%%' % (train_loss, train_accuracy)))\n                log(('Validation loss: %.8f, accuracy: %.2f%%' % (valid_loss, valid_accuracy)))\n                log(('      Best loss: %.8f at epoch %d' % (early_stopping.best_monitored_value, early_stopping.best_monitored_epoch)))\n                log(('   Elapsed time: ' + get_time_hhmmss(start)))\n                log(('      Timestamp: ' + get_time_hhmmss()))\n                log.sync()\n        else:\n            valid_loss = 0.0\n            valid_accuracy = 0.0\n            train_loss = 0.0\n            train_accuracy = 0.0\n        valid_loss_history = np.append(valid_loss_history, [valid_loss])\n        valid_accuracy_history = np.append(valid_accuracy_history, [valid_accuracy])\n        train_loss_history = np.append(train_loss_history, [train_loss])\n        train_accuracy_history = np.append(train_accuracy_history, [train_accuracy])\n        if params.early_stopping_enabled:\n            if (valid_loss == 0):\n                (_, valid_loss) = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n            if early_stopping(valid_loss, epoch):\n                log('Early stopping.\\n\u041d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c: {:.8f} \u043d\u0430 {} \u044d\u043f\u043e\u0445\u0435.'.format(early_stopping.best_monitored_value, early_stopping.best_monitored_epoch))\n                break\n    (test_accuracy, test_loss) = get_accuracy_and_loss_in_batches(X_test, y_test)\n    (valid_accuracy, valid_loss) = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n    log('=============================================')\n    log((' Valid loss: %.8f, accuracy = %.2f%%)' % (valid_loss, valid_accuracy)))\n    log((' Test loss: %.8f, accuracy = %.2f%%)' % (test_loss, test_accuracy)))\n    log((' Total time: ' + get_time_hhmmss(start)))\n    log(('  Timestamp: ' + get_time_hhmmss()))\n    saved_model_path = saver.save(session, paths.model_path)\n    log(('\u0424\u0430\u0439\u043b \u043c\u043e\u0434\u0435\u043b\u0438: ' + saved_model_path))\n    np.savez(paths.train_history_path, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, valid_loss_history=valid_loss_history, valid_accuracy_history=valid_accuracy_history)\n    log(('\u0424\u0430\u0439\u043b \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f: ' + paths.train_history_path))\n    log.sync(notify=True, message=('\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u043a\u043e\u043d\u0447\u0435\u043d\u043e \u0441 *%.2f%%* accuracy \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 (loss = *%.6f*).' % (test_accuracy, test_loss)))\n    plot_learning_curves(params)\n    log.add_plot(notify=True, caption='Learning curves')\n    pyplot.show()\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\nout = inpd\nfor layer in layers:\n    (w, b) = layer\n    out = elu(((out @ w) + b))\nreturn out\n"}
{"label_name":"save","label":1,"method_name":"save_config_file_from_dict","method":"\ns = b'wandb_version: 1'\nif config_dict:\n    s += (b'\\n\\n' + yaml.dump(config_dict, Dumper=yaml.SafeDumper, default_flow_style=False, allow_unicode=True, encoding='utf-8'))\ndata = s.decode('utf-8')\nfilesystem._safe_makedirs(os.path.dirname(config_filename))\nwith open(config_filename, 'w') as conf_file:\n    conf_file.write(data)\n"}
{"label_name":"predict","label":4,"method_name":"predict_extended","method":"\n\"Get prediction results for SQuAD.\\n\\n    Start Logits: (B, N_start)\\n    End Logits: (B, N_start, N_end)\\n\\n    Parameters\\n    ----------\\n    original_feature:\\n        The original SquadFeature before chunked\\n    chunked_features\\n        List of ChunkFeatures\\n    results\\n        List of model predictions for span start and span end.\\n    n_best_size\\n        Best N results written to file\\n    max_answer_length\\n        Maximum length of the answer tokens.\\n    start_top_n\\n        Number of start-position candidates\\n    end_top_n\\n        Number of end-position candidates\\n    Returns\\n    -------\\n    not_answerable_score\\n        Model's estimate that the question is not answerable.\\n    prediction\\n        The final prediction.\\n    nbest_json\\n        n-best predictions with their probabilities.\\n    \"\nnot_answerable_score = 1000000\nall_start_idx = []\nall_end_idx = []\nall_pred_score = []\ncontext_length = len(original_feature.context_token_ids)\ntoken_max_context_score = _np.full((len(chunked_features), context_length), (- _np.inf), dtype=_np.float32)\nfor (i, chunked_feature) in enumerate(chunked_features):\n    chunk_start = chunked_feature.chunk_start\n    chunk_length = chunked_feature.chunk_length\n    for j in range(chunk_start, (chunk_start + chunk_length)):\n        token_max_context_score[(i, j)] = (min((j - chunk_start), (((chunk_start + chunk_length) - 1) - j)) + (0.01 * chunk_length))\ntoken_max_chunk_id = token_max_context_score.argmax(axis=0)\nfor (chunk_id, (result, chunk_feature)) in enumerate(zip(results, chunked_features)):\n    cur_not_answerable_score = float(result.answerable_logits[1])\n    not_answerable_score = min(not_answerable_score, cur_not_answerable_score)\n    context_offset = chunk_feature.context_offset\n    chunk_start = chunk_feature.chunk_start\n    chunk_length = chunk_feature.chunk_length\n    for i in range(start_top_n):\n        for j in range(end_top_n):\n            pred_score = (result.start_top_logits[i] + result.end_top_logits[(i, j)])\n            start_index = result.start_top_index[i]\n            end_index = result.end_top_index[(i, j)]\n            if ((not (context_offset <= start_index < (context_offset + chunk_length))) or (not (context_offset <= end_index < (context_offset + chunk_length))) or (end_index < start_index)):\n                continue\n            pred_answer_length = ((end_index - start_index) + 1)\n            if (pred_answer_length > max_answer_length):\n                continue\n            start_idx = int(((start_index - context_offset) + chunk_start))\n            end_idx = int(((end_index - context_offset) + chunk_start))\n            if (token_max_chunk_id[start_idx] != chunk_id):\n                continue\n            all_start_idx.append(start_idx)\n            all_end_idx.append(end_idx)\n            all_pred_score.append(pred_score)\nsorted_start_end_score = sorted(zip(all_start_idx, all_end_idx, all_pred_score), key=(lambda args: args[(- 1)]), reverse=True)\nnbest = []\ncontext_text = original_feature.context_text\ncontext_token_offsets = original_feature.context_token_offsets\nseen_predictions = set()\nfor (start_idx, end_idx, pred_score) in sorted_start_end_score:\n    if (len(seen_predictions) >= n_best_size):\n        break\n    pred_answer = context_text[context_token_offsets[start_idx][0]:context_token_offsets[end_idx][1]]\n    seen_predictions.add(pred_answer)\n    nbest.append((pred_answer, pred_score))\nif (len(nbest) == 0):\n    nbest.append(('', float('-inf')))\nall_scores = _np.array([ele[1] for ele in nbest], dtype=_np.float32)\nprobs = (_np.exp(all_scores) \/ _np.sum(_np.exp(all_scores)))\nnbest_json = []\nfor (i, (entry, prob)) in enumerate(zip(nbest, probs)):\n    output = collections.OrderedDict()\n    output['text'] = entry[0]\n    output['probability'] = float(prob)\n    nbest_json.append(output)\nassert (len(nbest_json) >= 1)\nreturn (not_answerable_score, nbest[0][0], nbest_json)\n"}
{"label_name":"train","label":0,"method_name":"batch_train","method":"\ntrain_cost_agg = 0\ntrain_nll_agg = 0\n(batch_data_, batch_targets_) = batch.get_vars(batch_sz, X_train_t, Y_train_t)\n(_, batch_targets_int_) = batch.get_vars_scalar_out(batch_sz, X_train_t, Y_train_int_t)\nsize = batch_sz\nfor i in range(0, X_train_t.size(0), batch_sz):\n    if ((i + batch_sz) > X_train_t.size(0)):\n        size = (X_train_t.size(0) - i)\n        (batch_data_, batch_targets_) = batch.get_vars(size, X_train_t, Y_train_t)\n        (_, batch_targets_int_) = batch.get_vars_scalar_out(size, X_train_t, Y_train_int_t)\n    batch_data_.data[:] = X_train_t[i:(i + size)]\n    batch_targets_.data[:] = Y_train_t[i:(i + size)]\n    batch_targets_int_.data[:] = Y_train_int_t[i:(i + size)]\n    opt.zero_grad()\n    preds = model(batch_data_)\n    train_cost = cost_fn_news(preds, batch_targets_)\n    train_nll = nll(preds, batch_targets_int_)\n    (train_cost + (lam * train_nll)).backward()\n    opt.step()\n    train_cost_agg += (((train_cost - train_cost_agg) * batch_sz) \/ (i + batch_sz))\n    train_nll_agg += (((train_nll - train_nll_agg) * batch_sz) \/ (i + batch_sz))\n    print('Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(epoch, (i + batch_sz), X_train_t.size(0), ((float((i + batch_sz)) \/ X_train_t.size(0)) * 100), train_cost.item()))\nreturn (train_cost_agg, train_nll_agg)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\nprobabilities = calculateClassProbabilities(summaries, inputVector)\n(bestLabel, bestProb) = (None, (- 1))\nfor (classValue, probability) in probabilities.iteritems():\n    if ((bestLabel is None) or (probability > bestProb)):\n        bestProb = probability\n        bestLabel = classValue\nreturn bestLabel\n"}
{"label_name":"save","label":1,"method_name":"save_to_disk","method":"\n' Pickle an object to disk '\ndirname = os.path.dirname(path_to_disk)\nif (not os.path.exists(dirname)):\n    raise ValueError((('Path ' + dirname) + ' does not exist'))\nif ((not overwrite) and os.path.exists(path_to_disk)):\n    raise ValueError((('File ' + path_to_disk) + 'already exists'))\npickle.dump(obj, open(path_to_disk, 'wb'))\n"}
{"label_name":"save","label":1,"method_name":"save_distorted","method":"\nfor severity in range(1, 6):\n    print(method.__name__, severity)\n    distorted_dataset = DistortImageFolder(root='\/share\/data\/vision-greg\/ImageNet\/clsloc\/images\/val', method=method, severity=severity, transform=trn.Compose([trn.Resize(256), trn.CenterCrop(224)]))\n    distorted_dataset_loader = torch.utils.data.DataLoader(distorted_dataset, batch_size=100, shuffle=False, num_workers=4)\n    for _ in distorted_dataset_loader:\n        continue\n"}
{"label_name":"process","label":2,"method_name":"_process_tce","method":"\n'Processes the light curve for a Kepler TCE and returns an Example proto.\\n\\n  Args:\\n    tce: Row of the input TCE table.\\n\\n  Returns:\\n    A tensorflow.train.Example proto containing TCE features.\\n\\n  Raises:\\n    IOError: If the light curve files for this Kepler ID cannot be found.\\n  '\n(time, flux) = preprocess.read_and_process_light_curve(tce.kepid, FLAGS.kepler_data_dir)\n(time, flux) = preprocess.phase_fold_and_sort_light_curve(time, flux, tce.tce_period, tce.tce_time0bk)\nglobal_view = preprocess.global_view(time, flux, tce.tce_period)\nlocal_view = preprocess.local_view(time, flux, tce.tce_period, tce.tce_duration)\nex = tf.train.Example()\n_set_float_feature(ex, 'global_view', global_view)\n_set_float_feature(ex, 'local_view', local_view)\nfor (col_name, value) in tce.items():\n    if np.issubdtype(type(value), np.integer):\n        _set_int64_feature(ex, col_name, [value])\n    else:\n        try:\n            _set_float_feature(ex, col_name, [float(value)])\n        except ValueError:\n            _set_bytes_feature(ex, col_name, [value])\nreturn ex\n"}
{"label_name":"train","label":0,"method_name":"train_data_filenames","method":"\nreturn _data_filenames((problem + '-train'), output_dir, num_shards)\n"}
{"label_name":"train","label":0,"method_name":"do_train","method":"\nprint('Training word2vec\/Mol2vec model.')\nfeatures.train_word2vec_model(args.in_file, args.model, vector_size=args.dimensions, window=args.window, min_count=args.threshold, n_jobs=args.n_jobs, method=args.method)\nprint('Done!')\n"}
{"label_name":"train","label":0,"method_name":"is_training_name","method":"\n'\\n    **Guess** if this variable is only used in training.\\n    Only used internally to avoid too many logging. Do not use it.\\n    '\nname = get_op_tensor_name(name)[0]\nif (name.endswith('\/Adam') or name.endswith('\/Adam_1')):\n    return True\nif name.endswith('\/Momentum'):\n    return True\nif (name.endswith('\/Adadelta') or name.endswith('\/Adadelta_1')):\n    return True\nif (name.endswith('\/RMSProp') or name.endswith('\/RMSProp_1')):\n    return True\nif name.endswith('\/Adagrad'):\n    return True\nif (name.startswith('EMA\/') or ('\/EMA\/' in name)):\n    return True\nif (name.startswith('AccumGrad') or name.endswith('\/AccumGrad')):\n    return True\nif name.startswith('apply_gradients'):\n    return True\nreturn False\n"}
{"label_name":"predict","label":4,"method_name":"_problem_for_predictor_multi_classification_label","method":"\n'\\n    Returns *X, y, intial_types, method, node name, X runtime* for a\\n    m-cl classification problem.\\n    It is based on Iris dataset.\\n    '\ndata = load_iris()\nX = data.data\nstate = numpy.random.RandomState(seed=34)\nrnd = (state.randn(*X.shape) \/ 3)\nX += rnd\nX = _modify_dimension(X, n_features)\ny = data.target\ny2 = numpy.zeros((y.shape[0], 3), dtype=numpy.int64)\nfor (i, _) in enumerate(y):\n    y2[(i, _)] = 1\nfor i in range(0, y.shape[0], 5):\n    y2[(i, ((y[i] + 1) % 3))] = 1\nX = X.astype(dtype)\ny2 = y2.astype(numpy.int64)\nreturn (X, y2, [('X', X[:1].astype(dtype))], 'predict_proba', 1, X.astype(dtype))\n"}
{"label_name":"train","label":0,"method_name":"retrieve_training_data","method":"\n'\\n    :param path_model: path of the folder with the model parameters .ckpt\\n    :param path_model_init: if the model is initialized by another, path of its\\n        folder\\n    :return: dictionary {steps, accuracy, loss} describing the evolution over\\n        epochs of the performance of the model. Stacks the initial model if needed\\n    '\npath_model = convert_path(path_model)\nfile = open((path_model \/ 'evolution.pkl'), 'rb')\nevolution = pickle.load(file)\nif path_model_init:\n    path_model_init = convert_path(path_model_init)\n    file_init = open((path_model_init \/ 'evolution.pkl'), 'rb')\n    evolution_init = pickle.load(file_init)\n    last_epoch = evolution_init['steps'][(- 1)]\n    evolution_merged = {}\n    for key in ['steps', 'accuracy', 'loss']:\n        evolution_merged[key] = (evolution_init[key] + evolution[key])\n    evolution = evolution_merged\nreturn evolution\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\nencoder.train()\nclassifier.train()\ncorrect = 0\nfor (idx, batch) in enumerate(train_iter):\n    ((x, x_l), y) = (batch.text, (batch.label - 1))\n    optimizer.zero_grad()\n    encoder_outputs = encoder(x)\n    (output, attn) = classifier(encoder_outputs)\n    loss = F.nll_loss(output, y)\n    loss.backward()\n    optimizer.step()\n    pred = output.data.max(1, keepdim=True)[1]\n    correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n    if ((idx % log_interval) == 0):\n        print('train epoch: {} [{}\/{}], acc:{}, loss:{}'.format(epoch, (idx * len(x)), (len(train_iter) * args.batch_size), (correct \/ float((log_interval * len(x)))), loss.data[0]))\n        correct = 0\n"}
{"label_name":"process","label":2,"method_name":"_process_dbpedia","method":"\n'Process genders_en.ttl downloaded from dbpedia dump.'\nfile_name = 'genders_en.ttl'\nfor (name, gender) in gen_triples_from_file(os.path.join(_RAW_DATA_ROOT, file_name)):\n    proba = _CLASS2PROB[gender]\n    name2proba[name] = proba\nreturn name2proba\n"}
{"label_name":"process","label":2,"method_name":"postprocess_trajectory","method":"\ndel sample_batch[SampleBatch.NEXT_OBS]\nreturn sample_batch\n"}
{"label_name":"predict","label":4,"method_name":"assert_fit_predict_correct","method":"\nmodel2 = copy.deepcopy(model)\npredictions_1 = model.fit(X).predict(X)\npredictions_2 = model2.fit_predict(X)\nassert (adjusted_rand_score(predictions_1, predictions_2) == 1.0)\n"}
{"label_name":"predict","label":4,"method_name":"fit_predict","method":"\nstats = stats_train(train_docs)\n(prop_vect, _) = prop_vectorizer(train_docs, which=dataset, stats=stats, n_most_common_tok=None, n_most_common_dep=2000, return_transf=True)\nlink_vect = link_vectorizer(train_docs, stats, n_most_common=500)\nsec_ord_vect = (second_order_vectorizer(train_docs) if second_order else None)\n(_, _, _, pmi_in, pmi_out) = stats\n\ndef _transform_x_y(docs):\n    X = [_vectorize(doc, pmi_in, pmi_out, prop_vect, link_vect, sec_ord_vect) for doc in docs]\n    Y = [doc.label for doc in docs]\n    return (X, Y)\n(X_tr, Y_tr) = _transform_x_y(train_docs)\n(X_te, Y_te) = _transform_x_y(test_docs)\nmodel = ArgumentGraphCRF(class_weight=class_weight, constraints=constraints, compat_features=compat_features, coparents=coparents, grandparents=grandparents, siblings=siblings)\nclf = FrankWolfeSSVM(model, C=C, random_state=0, verbose=1, check_dual_every=25, show_loss_every=25, max_iter=100, tol=0)\nclf.fit(X_tr, Y_tr)\nif exact_test:\n    clf.model.exact = True\nY_pred = clf.predict(X_te)\nif return_vectorizers:\n    vectorizers = (pmi_in, pmi_out, prop_vect, link_vect, sec_ord_vect)\n    return (clf, Y_te, Y_pred, vectorizers)\nreturn (clf, Y_te, Y_pred)\n"}
{"label_name":"train","label":0,"method_name":"load_train_data","method":"\nimgs_train = np.load('imgs_train.npy')\nimgs_mask_train = np.load('imgs_mask_train.npy')\nreturn (imgs_train, imgs_mask_train)\n"}
{"label_name":"process","label":2,"method_name":"start_processor","method":"\ngraph = GRAPHS[args.graph]()\nwith pf.Processor.from_graph(args.push_address, args.pull_address, graph) as processor:\n    processor.run()\n"}
{"label_name":"train","label":0,"method_name":"load_train_data","method":"\next = filename.split('.')[(- 1)]\nif (ext == 'csv'):\n    return read_smiles_csv(filename)\nif (ext == 'smi'):\n    return read_smi(filename)\nelse:\n    raise ValueError('data is not smi or csv!')\nreturn\n"}
{"label_name":"save","label":1,"method_name":"save_image","method":"\nimage = image[0]\nimage = np.clip(image, 0, 255).astype('uint8')\nscipy.misc.imsave(path, image)\n"}
{"label_name":"save","label":1,"method_name":"_save_as_pickle","method":"\n'Save features as pickle'\nwith open(path, 'wb') as f:\n    pickle.dump(to_pickle, f)\n"}
{"label_name":"train","label":0,"method_name":"list_pretrained_bert","method":"\nreturn sorted(list(PRETRAINED_URL.keys()))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n(src_train, dest_train, src_dev, dest_dev, _, _) = data_util.prepare_headline_data(FLAGS.data_dir, FLAGS.vocab_size)\ndev_config = tf.ConfigProto(device_count={'CPU': 4}, inter_op_parallelism_threads=1, intra_op_parallelism_threads=2)\nwith tf.Session(config=dev_config) as sess:\n    model = create_model(sess, False)\n    dev_set = read_data(src_dev, dest_dev)\n    train_set = read_data(src_train, dest_train, FLAGS.max_train_data_size)\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n    trainbuckets_scale = [(sum(train_bucket_sizes[:(i + 1)]) \/ train_total_size) for i in xrange(len(train_bucket_sizes))]\n    (step_time, loss) = (0.0, 0.0)\n    current_step = 0\n    previous_losses = []\n    while True:\n        random_number_01 = np.random.random_sample()\n        bucket_id = min([i for i in xrange(len(trainbuckets_scale)) if (trainbuckets_scale[i] > random_number_01)])\n        start_time = time.time()\n        (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(train_set, bucket_id)\n        (_, step_loss, _) = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n        step_time += ((time.time() - start_time) \/ FLAGS.steps_per_checkpoint)\n        loss += (step_loss \/ FLAGS.steps_per_checkpoint)\n        current_step += 1\n        if ((current_step % FLAGS.steps_per_checkpoint) == 0):\n            perplexity = (math.exp(float(loss)) if (loss < 300) else float('inf'))\n            print(('global step %d learning rate %.4f step-time %.2f perplexity %.2f' % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity)))\n            if ((len(previous_losses) > 2) and (loss > max(previous_losses[(- 3):]))):\n                sess.run(model.learning_rate_decay_op)\n            previous_losses.append(loss)\n            checkpoint_path = os.path.join(FLAGS.train_dir, 'headline_large.ckpt')\n            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n            (step_time, loss) = (0.0, 0.0)\n            for bucket_id in xrange(len(buckets)):\n                if (len(dev_set[bucket_id]) == 0):\n                    print(('  eval: empty bucket %d' % bucket_id))\n                    continue\n                (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(dev_set, bucket_id)\n                (_, eval_loss, _) = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n                eval_ppx = (math.exp(float(eval_loss)) if (eval_loss < 300) else float('inf'))\n                print(('  eval: bucket %d perplexity %.2f' % (bucket_id, eval_ppx)))\n            sys.stdout.flush()\npass\n"}
{"label_name":"process","label":2,"method_name":"processUnits","method":"\nfName = open('C:\\\\Program Files (x86)\\\\Steam\\\\steamapps\\\\common\\\\dota 2 beta\\\\game\\\\dota\\\\scripts\\\\npc\\\\npc_units.txt', 'r')\ncontent = fName.readlines()\ncontent = [x.strip() for x in content]\nfName.close()\nshiftCount = 0\nunitCount = 0\ncurrName = None\nunitID = None\nlineCount = 0\nunits = {}\nfor line in content:\n    lineCount += 1\n    if (line == '{'):\n        shiftCount += 1\n        continue\n    if (line == '}'):\n        shiftCount -= 1\n        if (shiftCount == 1):\n            currName = None\n            unitID = None\n        continue\n    if (shiftCount == 1):\n        comment = (line[:2] == '\/\/')\n        underscore = line.find('_')\n        if ((not comment) and (underscore > 0)):\n            unitCount += 1\n            currName = line.strip().replace('\"', '')\n    if ((shiftCount == 2) and currName):\n        comment = (line[:2] == '\/\/')\n        if (not comment):\n            if (not (currName in units.keys())):\n                units[currName] = {}\n            if (line[:15] == '\"ArmorPhysical\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                units[currName]['PhysicalResist'] = float(res[1])\n            if (line[:19] == '\"MagicalResistance\"'):\n                res = re.split('\\\\s{2,}', line)\n                res[1] = res[1].replace('\"', '')\n                units[currName]['MagicResist'] = float(res[1])\nprint(('Processed %d units' % unitCount))\nwriteData(units, 'units.json')\n"}
{"label_name":"process","label":2,"method_name":"process_single_audio","method":"\n' Compute double stage HPSS for the given audio file\\n    Args : \\n        audio_file : path to audio file \\n    Return :\\n        mel_D2_total : concatenated melspectrogram of percussive, harmonic components of double stage HPSS. Shape=(2 * n_bins, total_frames) ex. (80, 2004) \\n    '\n(audio_src, _) = librosa.load(audio_file, sr=SR)\naudio_src = librosa.util.normalize(audio_src)\n(D_harmonic, D_percussive) = ono_hpss(audio_src, N_FFT1, N_HOP1)\n(D2_harmonic, D2_percussive) = ono_hpss(D_percussive, N_FFT2, N_HOP2)\nassert (D2_harmonic.shape == D2_percussive.shape)\nprint(D2_harmonic.shape, D2_percussive.shape)\nmel_harmonic = log_melgram(D2_harmonic, SR, N_FFT2, N_HOP2, N_MELS)\nmel_percussive = log_melgram(D2_percussive, SR, N_FFT2, N_HOP2, N_MELS)\nmel_total = np.vstack((mel_harmonic, mel_percussive))\nprint(mel_total.shape)\nreturn mel_total\n"}
{"label_name":"save","label":1,"method_name":"save_combine","method":"\n'\\n    Saves a list of variables into a single file.\\n\\n    Args:\\n        x(list): A list of Tensor\/LoDTensor variables to be saved together in\\n                 a single file.\\n        file_path(str): The file path where variables will be saved.\\n        overwrite(bool): Whether or not cover the given file when it has already\\n            existed. If it\\'s set \\'False\\' and the file is existed, a runtime\\n            error will be thrown.\\n\\n    Returns:\\n        There is no return value.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            import paddle.fluid as fluid\\n            v1 = fluid.layers.data(name=\"data\",\\n                                   shape=(4, 6),\\n                                   dtype=\"float32\")\\n            v2 = fluid.layers.data(name=\"data\",\\n                                   shape=(6, 8, 4),\\n                                   dtype=\"float32\")\\n            normed = fluid.layers.save_combine([v1, v2], file_path=\"output\")\\n    '\nhelper = LayerHelper('save_combine', **locals())\nhelper.append_op(type='save_combine', inputs={'input': x}, outputs={}, args={'file_path': file_path, 'overwrite': overwrite})\n"}
