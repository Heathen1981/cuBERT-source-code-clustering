{"label_name":"train","label":0,"method_name":"transformer_tall_pretrain_lm_tpu_adafactor_large","method":"\n\n@registry.register_hparams\ndef transformer_tall_pretrain_lm_tpu_adafactor_large():\n    'Hparams for transformer on LM pretraining on TPU, large model.'\n    hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n    hparams.hidden_size = 1024\n    hparams.num_heads = 16\n    hparams.filter_size = 32768\n    hparams.batch_size = 4\n    hparams.multiproblem_mixing_schedule = 'constant'\n    hparams.multiproblem_per_task_threshold = '320,80,160,1,80,160,2,20,10,5'\n    return hparams\n"}
{"label_name":"train","label":0,"method_name":"train_poetry","method":"\n\ndef train_poetry():\n    (sentences, word2idx) = get_robert_frost()\n    rnn = SimpleRNN(50, 50, len(word2idx))\n    rnn.fit(sentences, learning_rate=0.0001, show_fig=True, activation=T.nnet.relu, epochs=2000)\n    rnn.save('RRNN_D50_M50_epochs2000_relu.npz')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(trainset, batch_size, epoch, user_feature, product_feature):\n    learning_rate = 0.0005\n    network = NeuralNetwork()\n    training_optimizer = optimizer.Adam(lr=learning_rate, params=network.parameters())\n    loss_criterion = torch.nn.CrossEntropyLoss()\n    count = 0\n    loss_ave = 0\n    for i in range(epoch):\n        iters = 0\n        temp = 0\n        epoch_loss = 0\n        for j in range((len(trainset) \/ batch_size)):\n            temp += 1\n            iters += 1\n            user_self_vector = []\n            product_self_vector = []\n            labels = []\n            for p in range(batch_size):\n                person_id = trainset[(p + j)][0]\n                product_id = trainset[(p + j)][1]\n                user_self_vector.append(user_feature[person_id][:11])\n                product_self_vector.append(product_feature[product_id][:10])\n                labels.append(trainset[(p + j)][(- 1)])\n            user_self_vector = Variable(torch.FloatTensor(user_self_vector))\n            product_self_vector = Variable(torch.FloatTensor(product_self_vector))\n            prob = network(product_vector=product_self_vector, user_vector=user_self_vector)\n            labels = Variable(torch.LongTensor(labels))\n            loss = loss_criterion(prob, labels)\n            loss_ave += loss.data[0]\n            epoch_loss += loss.data[0]\n            if ((iters % 100) == 0):\n                print(('Epoch: %s Sample: %s: Loss: %s' % (i, iters, (loss_ave \/ 100.0))))\n                loss_ave = 0\n            training_optimizer.zero_grad()\n            loss.backward()\n            if (epoch == 15):\n                for param_group in training_optimizer.param_groups:\n                    learning_rate = (learning_rate \/ 5.0)\n                    param_group['lr'] = learning_rate\n            training_optimizer.step()\n        print('loss: %s', (epoch_loss \/ float(temp)))\n        save_model(network, ('.\/model_param\/deep1_neural_network_param_%s.pkl' % i))\n"}
{"label_name":"save","label":1,"method_name":"save_gpu_ids_shutdown_only","method":"\n\n@pytest.fixture\ndef save_gpu_ids_shutdown_only():\n    original_gpu_ids = os.environ.get('CUDA_VISIBLE_DEVICES', None)\n    (yield None)\n    ray.shutdown()\n    if (original_gpu_ids is not None):\n        os.environ['CUDA_VISIBLE_DEVICES'] = original_gpu_ids\n    else:\n        del os.environ['CUDA_VISIBLE_DEVICES']\n"}
{"label_name":"train","label":0,"method_name":"experiment_train","method":"\n\ndef experiment_train(epochs):\n    resnet18 = models.resnet18(pretrained=True)\n    resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n    freeze_weights(resnet18)\n    save_path = os.path.join(save_base_dir, 'cub200_resnet18_experiment')\n    optimizer = optim.SGD(resnet18.fc.parameters(), lr=learning_rate, weight_decay=0.001)\n    exp = Experiment(save_path, resnet18, device=device, optimizer=optimizer, task='classif')\n    exp.train(train_loader, valid_loader, epochs=epochs)\n    exp.test(test_loader)\n"}
{"label_name":"process","label":2,"method_name":"process_notes","method":"\n\ndef process_notes(data, output=None):\n    \"\\n    This is a simple processing function. It either loads the notes from a MIDI\\n    file and or writes the notes to a file.\\n\\n    The behaviour depends on the presence of the `output` argument, if 'None'\\n    is given, the notes are read, otherwise the notes are written to file.\\n\\n    Parameters\\n    ----------\\n    data : str or numpy array\\n        MIDI file to be loaded (if `output` is 'None') \/ notes to be written.\\n    output : str, optional\\n        Output file name. If set, the notes given by `data` are written.\\n\\n    Returns\\n    -------\\n    notes : numpy array\\n        Notes read\/written.\\n\\n    \"\n    if (output is None):\n        return MIDIFile.from_file(data).notes()\n    MIDIFile.from_notes(data).write(output)\n    return data\n"}
{"label_name":"process","label":2,"method_name":"process_data","method":"\n\ndef process_data():\n    print('Preparing data to be model-ready ...')\n    build_vocab('train.enc')\n    build_vocab('train.dec')\n    token2id('train', 'enc')\n    token2id('train', 'dec')\n    token2id('test', 'enc')\n    token2id('test', 'dec')\n"}
{"label_name":"train","label":0,"method_name":"train_wrapper","method":"\n\ndef train_wrapper(config):\n    ray_params = RayParams(elastic_training=False, max_actor_restarts=2, num_actors=4, cpus_per_actor=1, gpus_per_actor=0)\n    train_ray(path='\/data\/classification.parquet', num_workers=4, num_boost_rounds=100, num_files=25, regression=False, use_gpu=False, ray_params=ray_params, xgboost_params=config)\n"}
{"label_name":"process","label":2,"method_name":"create_preprocessed_dataset","method":"\n\ndef create_preprocessed_dataset(path_to_data, outputpath, preprocessing_queue):\n    'Create a preprocessed dataset file by applying `preprocessing_queue`\\n    to `path_to_data`. The result will be stored in `outputpath`.'\n    logger.info('Data soure %s', path_to_data)\n    logger.info('Output will be stored in %s', outputpath)\n    tmp = 'Preprocessing Queue:\\n'\n    for preprocessing_class in preprocessing_queue:\n        tmp += (str(preprocessing_class) + '\\n')\n    logger.info(tmp)\n    if (not os.path.isfile(path_to_data)):\n        logger.info(\"'%s' does not exist. Please either abort this script or update the data location.\", path_to_data)\n        raw_dataset_path = utils.choose_raw_dataset()\n        raw_dataset_path = ('raw-datasets' + raw_dataset_path.split('raw-datasets')[1])\n        print(raw_dataset_path)\n        sys.exit()\n    logger.info('Start loading data...')\n    with open(path_to_data, 'rb') as fp:\n        loaded = pickle.load(fp)\n    raw_datasets = loaded['handwriting_datasets']\n    logger.info('Start applying preprocessing methods')\n    start_time = time.time()\n    for (i, raw_dataset) in enumerate(raw_datasets):\n        if (((i % 10) == 0) and (i > 0)):\n            utils.print_status(len(raw_datasets), i, start_time)\n        raw_dataset['handwriting'].preprocessing(preprocessing_queue)\n    sys.stdout.write(('\\r%0.2f%% (done)\\x1b[K\\n' % 100))\n    print('')\n    with open(outputpath, 'wb') as fp:\n        pickle.dump({'handwriting_datasets': raw_datasets, 'formula_id2latex': loaded['formula_id2latex'], 'preprocessing_queue': preprocessing_queue}, fp, 2)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(holdout, model_type, continue_from=None):\n    batch_size = 4\n    train_percentage = (80 if (model_type == 'masses') else 90)\n    (train_files, holdout_files) = get_train_holdout_files(model_type, holdout, train_percentage, frame_count=CHANNEL_COUNT)\n    tmp_gen = image_generator(train_files[:2], 2, True, model_type)\n    for i in range(10):\n        x = next(tmp_gen)\n        img = x[0][0].reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n        img *= 255\n        img = x[1][0].reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n        img *= 255\n    train_gen = image_generator(train_files, batch_size, True, model_type)\n    holdout_gen = image_generator(holdout_files, batch_size, False, model_type)\n    if (continue_from is None):\n        model = get_unet(0.001)\n    else:\n        model = get_unet(0.0001)\n        model.load_weights(continue_from)\n    checkpoint1 = ModelCheckpoint((((('workdir\/' + model_type) + '_model_h') + str(holdout)) + '_{epoch:02d}-{val_loss:.2f}.hd5'), monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    checkpoint2 = ModelCheckpoint((((('workdir\/' + model_type) + '_model_h') + str(holdout)) + '_best.hd5'), monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    files = []\n    idx = 0\n    while (idx < len(holdout_files)):\n        files.append(holdout_files[idx])\n        idx += 5\n    dumper = DumpPredictions(holdout_files[::10], model_type)\n    epoch_div = 1\n    epoch_count = (200 if (model_type == 'masses') else 50)\n    model.fit_generator(train_gen, (len(train_files) \/ epoch_div), epoch_count, validation_data=holdout_gen, nb_val_samples=(len(holdout_files) \/ epoch_div), callbacks=[checkpoint1, checkpoint2, dumper])\n    if (not os.path.exists('models')):\n        os.mkdir('models')\n    shutil.copy((((('workdir\/' + model_type) + '_model_h') + str(holdout)) + '_best.hd5'), (((('models\/' + model_type) + '_model_h') + str(holdout)) + '_best.hd5'))\n"}
{"label_name":"train","label":0,"method_name":"wrap_train","method":"\n\ndef wrap_train(env):\n    from baselines.common.atari_wrappers import wrap_deepmind, FrameStack\n    env = wrap_deepmind(env, clip_rewards=False)\n    env = FrameStack(env, 3)\n    return env\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input, label):\n    optimizer.zero_grad()\n    output = model(input)\n    loss = criterion(output, label)\n    loss.backward()\n    optimizer.step()\n    return (output, loss.data[0])\n"}
{"label_name":"process","label":2,"method_name":"deprocess","method":"\n\ndef deprocess(image):\n    with tf.name_scope('deprocess'):\n        return ((image + 1) \/ 2)\n"}
{"label_name":"train","label":0,"method_name":"_check_time_series_max_train_size","method":"\n\ndef _check_time_series_max_train_size(splits, check_splits, max_train_size):\n    for ((train, test), (check_train, check_test)) in zip(splits, check_splits):\n        assert_array_equal(test, check_test)\n        assert_true((len(check_train) <= max_train_size))\n        suffix_start = max((len(train) - max_train_size), 0)\n        assert_array_equal(check_train, train[suffix_start:])\n"}
{"label_name":"train","label":0,"method_name":"read_train_data","method":"\n\ndef read_train_data(path):\n    file_path = os.path.normpath(path)\n    reader = Reader(line_format='timestamp user item rating', sep=',')\n    data = Dataset.load_from_file(file_path, reader=reader)\n    return data\n"}
{"label_name":"process","label":2,"method_name":"_process_desc","method":"\n\ndef _process_desc(s):\n    s = s.replace('Minifigure, Head', '')\n    s = s.lower()\n    s = [i.strip() for i in s.split('-')]\n    if (len(s) > 2):\n        s = [' '.join(s[:(- 1)]), s[(- 1)]]\n    s = s[0]\n    return s\n"}
{"label_name":"save","label":1,"method_name":"save_image","method":"\n\ndef save_image(image, save_dir, name, mean=None):\n    '\\n  \ub9cc\uc57d \ud3c9\uade0\uac12\uc744 argument\ub85c \ubc1b\uc73c\uba74 \ud3c9\uade0\uac12\uc744 \ub354\ud55c\ub4a4\uc5d0 \uc774\ubbf8\uc9c0\ub97c \uc800\uc7a5\ud558\uace0, \uc544\ub2c8\uba74 \ubc14\ub85c \uc774\ubbf8\uc9c0\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\\n  '\n    if mean:\n        image = unprocess_image(image, mean)\n    misc.imsave(os.path.join(save_dir, (name + '.png')), image)\n"}
{"label_name":"train","label":0,"method_name":"get_im_info_list_from_train_mode","method":"\n\ndef get_im_info_list_from_train_mode(setting, train_mode, load_mode='Single', read_pair_mode=None, stage=None):\n    \"\\n    :param setting:\\n    :param train_mode: should be in ['Training', 'Validation', 'Testing']\\n    :param load_mode: 'Single': mostly used in synthetic images, so you only need no know one image the other one will be generated.\\n                      'Pair'  : mostly used in real pair\\n                      default value is 'Single'\\n    :return: im_info_list:\\n                    load_model='Single': A list of dictionaries with single image information including:\\n                                        'data', 'deform_exp', 'type_im', 'cn', 'dsmooth', 'deform_method', 'deform_number'\\n                    load_model='Pair': Two list of dictionaries with information including:\\n                                        'data', 'type_im', 'cn'\\n    \"\n    if (train_mode not in ['Training', 'Validation', 'Testing']):\n        raise ValueError((\"train_mode should be in ['Training', 'Validation', 'Testing'], but it is set to\" + train_mode))\n    clean_data_exp_dict = []\n    for data_exp in setting['DataExpDict']:\n        dict_general = dict()\n        for key in data_exp.keys():\n            if (key in ['data', 'deform_exp']):\n                dict_general[key] = copy.deepcopy(data_exp[key])\n            elif (train_mode in key):\n                key_new = key.replace(train_mode, '')\n                dict_general[key_new] = copy.deepcopy(data_exp[key])\n        clean_data_exp_dict.append(dict_general)\n    im_info_list = []\n    if (load_mode == 'Single'):\n        for data_dict in clean_data_exp_dict:\n            for cn in data_dict['CNList']:\n                for type_im in data_dict['TypeImList']:\n                    for dsmooth in data_dict['DSmoothList']:\n                        im_info_dict = {'data': data_dict['data'], 'type_im': type_im, 'cn': cn}\n                        if ('DeformMethods' in setting['deform_exp'][data_dict['deform_exp']].keys()):\n                            deform_methods = copy.deepcopy(setting['deform_exp'][data_dict['deform_exp']]['DeformMethods'])\n                            deform_method = deform_methods[(dsmooth % len(deform_methods))]\n                            deform_number = get_deform_number_from_dsmooth(setting, dsmooth, deform_exp=data_dict['deform_exp'])\n                            im_info_dict['deform_exp'] = data_dict['deform_exp']\n                            im_info_dict['dsmooth'] = dsmooth\n                            im_info_dict['deform_method'] = deform_method\n                            im_info_dict['deform_number'] = deform_number\n                        if ('DeformedImExt' in data_dict.keys()):\n                            im_info_dict['deformed_im_ext'] = data_dict['DeformedImExt']\n                        if (('stage' in setting.keys()) or (stage is not None)):\n                            if (stage is None):\n                                stage = setting['stage']\n                            im_info_dict['stage'] = stage\n                            if ('PadTo' in setting.keys()):\n                                if (('stage' + str(stage)) in setting['PadTo'].keys()):\n                                    im_info_dict['padto'] = setting['PadTo'][('stage' + str(stage))]\n                        if ('Spacing' in data_dict.keys()):\n                            im_info_dict['spacing'] = data_dict['Spacing']\n                        im_info_list.append(im_info_dict)\n    elif (load_mode == 'Pair'):\n        if (read_pair_mode is None):\n            read_pair_mode = setting['read_pair_mode']\n        if (read_pair_mode == 'real'):\n            for data_dict in clean_data_exp_dict:\n                for cn in data_dict['CNList']:\n                    for pair in data_dict['PairList']:\n                        pair_dict = [{'data': data_dict['data'], 'type_im': copy.copy(pair[0]), 'cn': cn}, {'data': data_dict['data'], 'type_im': copy.copy(pair[1]), 'cn': cn}]\n                        if ('Spacing' in data_dict):\n                            pair_dict[0]['spacing'] = copy.copy(data_dict['Spacing'])\n                            pair_dict[1]['spacing'] = copy.copy(data_dict['Spacing'])\n                        im_info_list.append(pair_dict)\n        elif (read_pair_mode == 'synthetic'):\n            for data_dict in clean_data_exp_dict:\n                for cn in data_dict['CNList']:\n                    for type_im in data_dict['TypeImList']:\n                        for dsmooth in data_dict['DSmoothList']:\n                            im_info_moving = {'data': data_dict['data'], 'type_im': type_im, 'cn': cn}\n                            if ('DeformMethods' in setting['deform_exp'][data_dict['deform_exp']].keys()):\n                                deform_methods = copy.deepcopy(setting['deform_exp'][data_dict['deform_exp']]['DeformMethods'])\n                                deform_method = deform_methods[(dsmooth % len(deform_methods))]\n                                deform_number = get_deform_number_from_dsmooth(setting, dsmooth, deform_exp=data_dict['deform_exp'])\n                                im_info_moving['deform_exp'] = data_dict['deform_exp']\n                                im_info_moving['dsmooth'] = dsmooth\n                                im_info_moving['deform_method'] = deform_method\n                                im_info_moving['deform_number'] = deform_number\n                            if ('DeformedImExt' in data_dict.keys()):\n                                im_info_moving['deformed_im_ext'] = data_dict['DeformedImExt']\n                            if (('stage' in setting.keys()) or (stage is not None)):\n                                if (stage is None):\n                                    stage = setting['stage']\n                                im_info_moving['stage'] = stage\n                                if ('PadTo' in setting.keys()):\n                                    if (('stage' + str(stage)) in setting['PadTo'].keys()):\n                                        im_info_moving['padto'] = setting['PadTo'][('stage' + str(stage))]\n                            if ('Spacing' in data_dict.keys()):\n                                im_info_moving['spacing'] = data_dict['Spacing']\n                            im_info_fixed = copy.deepcopy(im_info_moving)\n                            pair_dict = [im_info_fixed, im_info_moving]\n                            im_info_list.append(pair_dict)\n    else:\n        raise ValueError((\"load_mode should be in ['Single', 'Pair'], but it is set to\" + train_mode))\n    return im_info_list\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(args):\n    print(('[%s] Preparing dialog data in %s' % (args.model_name, args.data_dir)))\n    setup_workpath(workspace=args.workspace)\n    (train_data, dev_data, _) = data_utils.prepare_dialog_data(args.data_dir, args.vocab_size)\n    if args.reinforce_learn:\n        args.batch_size = 1\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_usage)\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n        print(('Creating %d layers of %d units.' % (args.num_layers, args.size)))\n        model = seq2seq_model_utils.create_model(sess, args, forward_only=False)\n        print(('Reading development and training data (limit: %d).' % args.max_train_data_size))\n        dev_set = data_utils.read_data(dev_data, args.buckets, reversed=args.rev_model)\n        train_set = data_utils.read_data(train_data, args.buckets, args.max_train_data_size, reversed=args.rev_model)\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(args.buckets))]\n        train_total_size = float(sum(train_bucket_sizes))\n        train_buckets_scale = [(sum(train_bucket_sizes[:(i + 1)]) \/ train_total_size) for i in xrange(len(train_bucket_sizes))]\n        (step_time, loss) = (0.0, 0.0)\n        current_step = 0\n        previous_losses = []\n        vocab_path = os.path.join(args.data_dir, ('vocab%d.in' % args.vocab_size))\n        (vocab, rev_vocab) = data_utils.initialize_vocabulary(vocab_path)\n        while True:\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if (train_buckets_scale[i] > random_number_01)])\n            start_time = time.time()\n            (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(train_set, bucket_id)\n            if args.reinforce_learn:\n                (_, step_loss, _) = model.step_rf(args, sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, rev_vocab=rev_vocab)\n            else:\n                (_, step_loss, _) = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False, force_dec_input=True)\n            step_time += ((time.time() - start_time) \/ args.steps_per_checkpoint)\n            loss += (step_loss \/ args.steps_per_checkpoint)\n            current_step += 1\n            if (args.reinforce_learn == 1):\n                perplexity = (math.exp(loss) if (loss < 300) else float('inf'))\n                print(('global step %d learning rate %.4f step-time %.2f perplexity %.2f @ %s' % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity, datetime.now())))\n            if ((current_step % args.steps_per_checkpoint) == 0):\n                perplexity = (math.exp(loss) if (loss < 300) else float('inf'))\n                print(('global step %d learning rate %.4f step-time %.2f perplexity %.2f @ %s' % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity, datetime.now())))\n                if ((len(previous_losses) > 2) and (loss > max(previous_losses[(- 3):]))):\n                    sess.run(model.learning_rate_decay_op)\n                previous_losses.append(loss)\n                checkpoint_path = os.path.join(args.model_dir, 'model.ckpt')\n                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n                (step_time, loss) = (0.0, 0.0)\n                for bucket_id in xrange(len(args.buckets)):\n                    (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(dev_set, bucket_id)\n                    (_, eval_loss, _) = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=True, force_dec_input=False)\n                    eval_ppx = (math.exp(eval_loss) if (eval_loss < 300) else float('inf'))\n                    print(('  eval: bucket %d perplexity %.2f' % (bucket_id, eval_ppx)))\n                sys.stdout.flush()\n"}
{"label_name":"save","label":1,"method_name":"save_top_k","method":"\n\ndef save_top_k(args):\n    '\\n    This function runs forward computation on an ensemble of trained models\\n    using binarized parallel training data and saves the top-k probabilities\\n    and their corresponding token indices for each output step.\\n\\n    Note that the Python binary accepts all generation params, but ignores\\n    inapplicable ones (such as those related to output length). --max-tokens\\n    is of particular importance to prevent memory errors.\\n    '\n    pytorch_translate_options.print_args(args)\n    use_cuda = (torch.cuda.is_available() and (not getattr(args, 'cpu', False)))\n    (models, model_args, task) = pytorch_translate_utils.load_diverse_ensemble_for_inference(args.path.split(CHECKPOINT_PATHS_DELIMITER))\n    for model in models:\n        model.eval()\n        if use_cuda:\n            model.cuda()\n    append_eos_to_source = model_args[0].append_eos_to_source\n    reverse_source = model_args[0].reverse_source\n    assert all((((a.append_eos_to_source == append_eos_to_source) and (a.reverse_source == reverse_source)) for a in model_args))\n    assert ((args.source_binary_file != '') and (args.target_binary_file != '')), 'collect_top_k_probs requires binarized data.'\n    task.load_dataset(args.gen_subset, args.source_binary_file, args.target_binary_file)\n    assert (args.top_k_probs_binary_file != ''), 'must specify output file (--top-k-probs-binary-file)!'\n    output_path = args.top_k_probs_binary_file\n    dataset = task.dataset(args.gen_subset)\n    (top_k_scores, top_k_indices) = compute_top_k(task=task, models=models, dataset=dataset, k=args.k_probs_to_collect, use_cuda=use_cuda, max_tokens=args.teacher_max_tokens, max_sentences=args.max_sentences, progress_bar_args=args)\n    np.savez(output_path, top_k_scores=top_k_scores, top_k_indices=top_k_indices)\n    print(f'Saved top {top_k_scores.shape[1]} probs for a total of {top_k_scores.shape[0]} tokens to file {output_path}')\n"}
{"label_name":"train","label":0,"method_name":"get_gqcnn_trainer","method":"\n\ndef get_gqcnn_trainer(backend='tf'):\n    'Get the GQ-CNN Trainer for the provided backend.\\n\\n    Note\\n    ----\\n    Currently only TensorFlow is supported.\\n\\n    Parameters\\n    ----------\\n    backend : str\\n        The backend to use, currently only \"tf\" is supported.\\n\\n    Returns\\n    -------\\n    :obj:`gqcnn.training.tf.GQCNNTrainerTF`\\n        GQ-CNN Trainer with TensorFlow backend.\\n    '\n    if (backend == 'tf'):\n        return GQCNNTrainerTF\n    else:\n        raise ValueError('Invalid backend: {}'.format(backend))\n"}
{"label_name":"save","label":1,"method_name":"save_img","method":"\n\ndef save_img(path, x, data_format='channels_last', file_format=None, scale=True, **kwargs):\n    img = array_to_img(x, data_format=data_format, scale=scale)\n    if ((img.mode == 'RGBA') and ((file_format == 'jpg') or (file_format == 'jpeg'))):\n        warnings.warn('The JPG format does not support RGBA images, converting to RGB.')\n        img = img.convert('RGB')\n    img.save(path, format=file_format, **kwargs)\n"}
{"label_name":"train","label":0,"method_name":"remove_training_nodes","method":"\n\ndef remove_training_nodes(input_graph, protected_nodes=None):\n    \"Prunes out nodes that aren't needed for inference.\\n\\n  There are nodes like Identity and CheckNumerics that are only useful\\n  during training, and can be removed in graphs that will be used for\\n  nothing but inference. Here we identify and remove them, returning an\\n  equivalent graph. To be specific, CheckNumerics nodes are always removed, and\\n  Identity nodes that aren't involved in control edges are spliced out so that\\n  their input and outputs are directly connected.\\n\\n  Args:\\n    input_graph: Model to analyze and prune.\\n    protected_nodes: An optional list of names of nodes to be kept\\n      unconditionally. This is for example useful to preserve Identity output\\n      nodes.\\n\\n  Returns:\\n    A list of nodes with the unnecessary ones removed.\\n  \"\n    if (not protected_nodes):\n        protected_nodes = []\n    types_to_remove = {'CheckNumerics': True}\n    input_nodes = input_graph.node\n    names_to_remove = {}\n    for node in input_nodes:\n        if ((node.op in types_to_remove) and (node.name not in protected_nodes)):\n            names_to_remove[node.name] = True\n    nodes_after_removal = []\n    for node in input_nodes:\n        if (node.name in names_to_remove):\n            continue\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(node)\n        input_before_removal = node.input\n        del new_node.input[:]\n        for full_input_name in input_before_removal:\n            input_name = re.sub('^\\\\^', '', full_input_name)\n            if (input_name in names_to_remove):\n                continue\n            new_node.input.append(full_input_name)\n        nodes_after_removal.append(new_node)\n    types_to_splice = {'Identity': True}\n    names_to_splice = {}\n    for node in nodes_after_removal:\n        if ((node.op in types_to_splice) and (node.name not in protected_nodes)):\n            has_control_edge = False\n            for input_name in node.input:\n                if re.match('^\\\\^', input_name):\n                    has_control_edge = True\n            if (not has_control_edge):\n                names_to_splice[node.name] = node.input[0]\n    nodes_after_splicing = []\n    for node in nodes_after_removal:\n        if (node.name in names_to_splice):\n            continue\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(node)\n        input_before_removal = node.input\n        del new_node.input[:]\n        for full_input_name in input_before_removal:\n            input_name = re.sub('^\\\\^', '', full_input_name)\n            while (input_name in names_to_splice):\n                full_input_name = names_to_splice[input_name]\n                input_name = re.sub('^\\\\^', '', full_input_name)\n            new_node.input.append(full_input_name)\n        nodes_after_splicing.append(new_node)\n    output_graph = graph_pb2.GraphDef()\n    output_graph.node.extend(nodes_after_splicing)\n    return output_graph\n"}
{"label_name":"train","label":0,"method_name":"parse_args_subsubtrain_split_entry_point","method":"\n\ndef parse_args_subsubtrain_split_entry_point() -> None:\n    documentation = gepd.subsubtrain_split_entry_point()\n    _parse_args(documentation)\n"}
{"label_name":"train","label":0,"method_name":"gan_train_ops","method":"\n\ndef gan_train_ops(model, loss, generator_optimizer, discriminator_optimizer, check_for_unused_update_ops=True, **kwargs):\n    'Returns GAN train ops.\\n\\n  The highest-level call in TFGAN. It is composed of functions that can also\\n  be called, should a user require more control over some part of the GAN\\n  training process.\\n\\n  Args:\\n    model: A GANModel.\\n    loss: A GANLoss.\\n    generator_optimizer: The optimizer for generator updates.\\n    discriminator_optimizer: The optimizer for the discriminator updates.\\n    check_for_unused_update_ops: If `True`, throws an exception if there are\\n      update ops outside of the generator or discriminator scopes.\\n    **kwargs: Keyword args to pass directly to\\n      `training.create_train_op` for both the generator and\\n      discriminator train op.\\n\\n  Returns:\\n    A GANTrainOps tuple of (generator_train_op, discriminator_train_op) that can\\n    be used to train a generator\/discriminator pair.\\n  '\n    global_step = training_util.get_or_create_global_step()\n    global_step_inc = global_step.assign_add(1)\n    (gen_update_ops, dis_update_ops) = _get_update_ops(kwargs, model.generator_scope.name, model.discriminator_scope.name, check_for_unused_update_ops)\n    generator_global_step = None\n    if isinstance(generator_optimizer, sync_replicas_optimizer.SyncReplicasOptimizer):\n        generator_global_step = variable_scope.get_variable('dummy_global_step_generator', shape=[], dtype=global_step.dtype.base_dtype, initializer=init_ops.zeros_initializer(), trainable=False, collections=[ops.GraphKeys.GLOBAL_VARIABLES])\n        gen_update_ops += [generator_global_step.assign(global_step)]\n    with ops.name_scope('generator_train'):\n        gen_train_op = training.create_train_op(total_loss=loss.generator_loss, optimizer=generator_optimizer, variables_to_train=model.generator_variables, global_step=generator_global_step, update_ops=gen_update_ops, **kwargs)\n    discriminator_global_step = None\n    if isinstance(discriminator_optimizer, sync_replicas_optimizer.SyncReplicasOptimizer):\n        discriminator_global_step = variable_scope.get_variable('dummy_global_step_discriminator', shape=[], dtype=global_step.dtype.base_dtype, initializer=init_ops.zeros_initializer(), trainable=False, collections=[ops.GraphKeys.GLOBAL_VARIABLES])\n        dis_update_ops += [discriminator_global_step.assign(global_step)]\n    with ops.name_scope('discriminator_train'):\n        disc_train_op = training.create_train_op(total_loss=loss.discriminator_loss, optimizer=discriminator_optimizer, variables_to_train=model.discriminator_variables, global_step=discriminator_global_step, update_ops=dis_update_ops, **kwargs)\n    return namedtuples.GANTrainOps(gen_train_op, disc_train_op, global_step_inc)\n"}
{"label_name":"process","label":2,"method_name":"process_dir","method":"\n\ndef process_dir(base_path, process_func):\n    for fp in iter_files(base_path):\n        process_func(fp)\n"}
{"label_name":"train","label":0,"method_name":"trainNB0","method":"\n\ndef trainNB0(trainMatrix, trainCategory):\n    numTrainDocs = len(trainMatrix)\n    numWords = len(trainMatrix[0])\n    pAbusive = (sum(trainCategory) \/ float(numTrainDocs))\n    p0Num = ones(numWords)\n    p1Num = ones(numWords)\n    p0Denom = 2.0\n    p1Denom = 2.0\n    for i in range(numTrainDocs):\n        if (trainCategory[i] == 1):\n            p1Num += trainMatrix[i]\n            p1Denom += sum(trainMatrix[i])\n        else:\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n    p1Vec = log((p1Num \/ p1Denom))\n    p0Vec = log((p0Num \/ p0Denom))\n    return (p0Vec, p1Vec, pAbusive)\n"}
{"label_name":"save","label":1,"method_name":"_save","method":"\n\ndef _save(im, fp, filename):\n    if (im.mode != '1'):\n        raise IOError(('cannot write mode %s as XBM' % im.mode))\n    fp.write(('#define im_width %d\\n' % im.size[0]).encode('ascii'))\n    fp.write(('#define im_height %d\\n' % im.size[1]).encode('ascii'))\n    hotspot = im.encoderinfo.get('hotspot')\n    if hotspot:\n        fp.write(('#define im_x_hot %d\\n' % hotspot[0]).encode('ascii'))\n        fp.write(('#define im_y_hot %d\\n' % hotspot[1]).encode('ascii'))\n    fp.write(b'static char im_bits[] = {\\n')\n    ImageFile._save(im, fp, [('xbm', ((0, 0) + im.size), 0, None)])\n    fp.write(b'};\\n')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(use_cuda, save_dirname, is_local, use_bf16):\n    x = fluid.layers.data(name='x', shape=[13], dtype='float32')\n    y_predict = fluid.layers.fc(input=x, size=1, act=None)\n    y = fluid.layers.data(name='y', shape=[1], dtype='float32')\n    cost = fluid.layers.square_error_cost(input=y_predict, label=y)\n    avg_cost = fluid.layers.mean(cost)\n    sgd_optimizer = fluid.optimizer.SGD(learning_rate=0.001)\n    if use_bf16:\n        paddle.static.amp.rewrite_program_bf16(fluid.default_main_program())\n    sgd_optimizer.minimize(avg_cost)\n    BATCH_SIZE = 20\n    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.uci_housing.train(), buf_size=500), batch_size=BATCH_SIZE)\n    place = (fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace())\n    exe = fluid.Executor(place)\n\n    def train_loop(main_program):\n        feeder = fluid.DataFeeder(place=place, feed_list=[x, y])\n        exe.run(fluid.default_startup_program())\n        PASS_NUM = 100\n        for pass_id in range(PASS_NUM):\n            for data in train_reader():\n                (avg_loss_value,) = exe.run(main_program, feed=feeder.feed(data), fetch_list=[avg_cost])\n                print(avg_loss_value)\n                if (avg_loss_value[0] < 10.0):\n                    if (save_dirname is not None):\n                        fluid.io.save_inference_model(save_dirname, ['x'], [y_predict], exe)\n                    return\n                if math.isnan(float(avg_loss_value)):\n                    sys.exit('got NaN loss, training failed.')\n        raise AssertionError('Fit a line cost is too large, {0:2.2}'.format(avg_loss_value[0]))\n    if is_local:\n        train_loop(fluid.default_main_program())\n    else:\n        port = os.getenv('PADDLE_PSERVER_PORT', '6174')\n        pserver_ips = os.getenv('PADDLE_PSERVER_IPS')\n        eplist = []\n        for ip in pserver_ips.split(','):\n            eplist.append(':'.join([ip, port]))\n        pserver_endpoints = ','.join(eplist)\n        trainers = int(os.getenv('PADDLE_TRAINERS'))\n        current_endpoint = ((os.getenv('POD_IP') + ':') + port)\n        trainer_id = int(os.getenv('PADDLE_TRAINER_ID'))\n        training_role = os.getenv('PADDLE_TRAINING_ROLE', 'TRAINER')\n        t = fluid.DistributeTranspiler()\n        t.transpile(trainer_id, pservers=pserver_endpoints, trainers=trainers)\n        if (training_role == 'PSERVER'):\n            pserver_prog = t.get_pserver_program(current_endpoint)\n            pserver_startup = t.get_startup_program(current_endpoint, pserver_prog)\n            exe.run(pserver_startup)\n            exe.run(pserver_prog)\n        elif (training_role == 'TRAINER'):\n            train_loop(t.get_trainer_program())\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(path, X, y, X_, y_, image_dim, final_vec_dim, epoch, batch_size):\n    if ((path is None) or (not os.path.isfile((path + '0')))):\n        print(colored('Creating a new CNN.', 'cyan'))\n        cnn = CNN(image_dim, final_vec_dim)\n    else:\n        print(colored('Loading model from : {0}'.format(path), 'green'))\n        cnn = load_model(path, image_dim, final_vec_dim)\n    print(colored('Training started.', 'green'))\n    cnn.train(X, y, X_, y_, batch_size, epoch)\n    print(colored('Training finished.', 'green'))\n    return cnn\n"}
{"label_name":"train","label":0,"method_name":"_create_embedded_train_val_split","method":"\n\ndef _create_embedded_train_val_split(layer, model_path, train_val_split_file):\n    data_info = load_organized_data_info(IMGS_DIM_1D)\n    (dir_tr, num_tr) = (data_info['dir_tr'], data_info['num_tr'])\n    (dir_val, num_val) = (data_info['dir_val'], data_info['num_val'])\n    model = LAYER_RESULT_FUNCS[layer](model_path)\n    gen = testing_generator(dir_tr=dir_tr)\n    (X_tr, y_tr, names_tr) = _create_embedded_data_from_dir(model, gen, dir_tr, num_tr, LAYER_SIZES[layer])\n    (X_val, y_val, names_val) = _create_embedded_data_from_dir(model, gen, dir_val, num_val, LAYER_SIZES[layer])\n    _save_np_compressed_data(train_val_split_file, X_tr, y_tr, names_tr, X_val, y_val, names_val)\n    return (X_tr, y_tr, names_tr, X_val, y_val, names_val)\n"}
{"label_name":"process","label":2,"method_name":"process_full_VH_VL_sequence","method":"\n\ndef process_full_VH_VL_sequence(full_sequence):\n    cdrs = get_CDR_simple(full_sequence.replace(\"'\", '').replace('\"', '').replace(' ', '').upper())\n    print(cdrs)\n    for cdr_name in sorted(cdrs):\n        print(('--- %-4s + %d residue per side ---' % (cdr_name, NUM_EXTRA_RESIDUES)))\n        process_single_cdr(cdrs[cdr_name])\n"}
{"label_name":"train","label":0,"method_name":"load_train_data","method":"\n\ndef load_train_data(filename):\n    ext = filename.split('.')[(- 1)]\n    if (ext == 'csv'):\n        return read_smiles_csv(filename)\n    if (ext == 'smi'):\n        return read_smi(filename)\n    else:\n        raise ValueError('data is not smi or csv!')\n    return\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(data, model, optimizer, n_iter, batch_size):\n    ((train_X, train_Y), (dev_X, dev_Y)) = data\n    indices = model.ops.xp.arange(train_X.shape[0], dtype='i')\n    for i in range(n_iter):\n        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n        for (X, Y) in tqdm(batches, leave=False):\n            (Yh, backprop) = model.begin_update(X)\n            backprop((Yh - Y))\n            model.finish_update(optimizer)\n        correct = 0\n        total = 0\n        for (X, Y) in model.ops.multibatch(batch_size, dev_X, dev_Y):\n            Yh = model.predict(X)\n            correct += (Yh.argmax(axis=1) == Y.argmax(axis=1)).sum()\n            total += Yh.shape[0]\n        score = (correct \/ total)\n        print(f' {i} {float(score):.3f}')\n"}
{"label_name":"process","label":2,"method_name":"process_a_text","method":"\n\ndef process_a_text(text_file, tokenizer, max_seq_length, short_seq_prob=0.05):\n    '\\n    Create features from a single raw text file, in which one line is treated\\n    as a sentence, and double blank lines represent document separators.\\n\\n    In this process, mxnet-unrelated features are generated, to easily convert\\n     to features of a particular deep learning framework in subsequent steps\\n\\n    Parameters\\n    ----------\\n    text_file\\n        The path to a single text file\\n    tokenizer\\n        The trained tokenizer\\n    max_seq_length\\n        Maximum sequence length of the training features\\n    short_seq_prob\\n        The probability of sampling sequences shorter than the max_seq_length.\\n\\n    Returns\\n    -------\\n    features\\n        A list of processed features from a single text file\\n    '\n    vocab = tokenizer.vocab\n    features = []\n    with io.open(text_file, 'r', encoding='utf-8') as reader:\n        lines = reader.readlines()\n        tokenized_lines = tokenize_lines_to_ids(lines, tokenizer)\n        target_seq_length = max_seq_length\n        current_sentences = []\n        current_length = 0\n        for tokenized_line in tokenized_lines:\n            current_sentences.append(tokenized_line)\n            current_length += len(tokenized_line)\n            if (((not tokenized_line) and (current_length != 0)) or (current_length >= target_seq_length)):\n                (first_segment, second_segment) = sentenceize(current_sentences, max_seq_length, target_seq_length)\n                input_id = (([vocab.cls_id] + first_segment) + [vocab.sep_id])\n                segment_id = ([0] * len(input_id))\n                if second_segment:\n                    input_id += (second_segment + [vocab.sep_id])\n                    segment_id += ([1] * (len(second_segment) + 1))\n                valid_length = len(input_id)\n                input_id += ([0] * (max_seq_length - len(input_id)))\n                segment_id += ([0] * (max_seq_length - len(segment_id)))\n                feature = PretrainFeature(input_id=input_id, segment_id=segment_id, valid_length=valid_length)\n                features.append(feature)\n                current_sentences = []\n                current_length = 0\n                if (random.random() < short_seq_prob):\n                    target_seq_length = random.randint(5, max_seq_length)\n                else:\n                    target_seq_length = max_seq_length\n    return features\n"}
{"label_name":"save","label":1,"method_name":"save_dot_to_file","method":"\n\ndef save_dot_to_file(dot_graph, path):\n    dot_graph.write(path, format='dot')\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n\ndef preprocess_image(image, height, width, is_training=False, bbox=None, fast_mode=True, add_image_summaries=True):\n    'Pre-process one image for training or evaluation.\\n\\n  Args:\\n    image: 3-D Tensor [height, width, channels] with the image. If dtype is\\n      tf.float32 then the range should be [0, 1], otherwise it would converted\\n      to tf.float32 assuming that the range is [0, MAX], where MAX is largest\\n      positive representable number for int(8\/16\/32) data type (see\\n      `tf.image.convert_image_dtype` for details).\\n    height: integer, image expected height.\\n    width: integer, image expected width.\\n    is_training: Boolean. If true it would transform an image for train,\\n      otherwise it would transform it for evaluation.\\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\\n      where each coordinate is [0, 1) and the coordinates are arranged as\\n      [ymin, xmin, ymax, xmax].\\n    fast_mode: Optional boolean, if True avoids slower transformations.\\n    add_image_summaries: Enable image summaries.\\n\\n  Returns:\\n    3-D float Tensor containing an appropriately scaled image\\n\\n  Raises:\\n    ValueError: if user does not provide bounding box\\n  '\n    if is_training:\n        return preprocess_for_train(image, height, width, bbox, fast_mode, add_image_summaries=add_image_summaries)\n    else:\n        return preprocess_for_eval(image, height, width)\n"}
{"label_name":"train","label":0,"method_name":"sklearn_training_job","method":"\n\n@pytest.fixture(scope='module')\n@pytest.mark.skip(reason='This test has always failed, but the failure was masked by a bug. This test should be fixed. Details in https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/968')\ndef sklearn_training_job(sagemaker_session, sklearn_latest_version, sklearn_latest_py_version, cpu_instance_type):\n    return _run_mnist_training_job(sagemaker_session, cpu_instance_type, sklearn_latest_version, sklearn_latest_py_version)\n    sagemaker_session.boto_region_name\n"}
{"label_name":"process","label":2,"method_name":"_preprocess_observation","method":"\n\ndef _preprocess_observation(observation):\n    'Transforms the specified observation into a 47x47x1 grayscale image.\\n\\n    Returns:\\n        A 47x47x1 tensor with float32 values between 0 and 1.\\n    '\n    grayscale_observation = observation.mean(2)\n    resized_observation = misc.imresize(grayscale_observation, (47, 47)).astype(np.float32)\n    return np.expand_dims(resized_observation, 2)\n"}
{"label_name":"predict","label":4,"method_name":"check_fit2d_predict1d","method":"\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_fit2d_predict1d(name, estimator_orig):\n    rnd = np.random.RandomState(0)\n    X = (3 * rnd.uniform(size=(20, 3)))\n    y = X[:, 0].astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n    if hasattr(estimator, 'n_components'):\n        estimator.n_components = 1\n    if hasattr(estimator, 'n_clusters'):\n        estimator.n_clusters = 1\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n    for method in ['predict', 'transform', 'decision_function', 'predict_proba']:\n        if hasattr(estimator, method):\n            assert_raise_message(ValueError, 'Reshape your data', getattr(estimator, method), X[0])\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(mf=False, ex=False, extra=False, bs=10000, dim=16, epo=300, sgd='adam', whole=False, nor=True, bias=True):\n    (x, y, z) = load_data(nor=nor)\n    vx = None\n    vy = None\n    vz = None\n    if (not whole):\n        (x, y, z, vx, vy, vz) = split_valid(x, y, z, rand=True)\n    model = None\n    model_name = 'model.h5'\n    if mf:\n        model_name = 'mf_model.h5'\n        model = build_model(int((np.max(x) + 1)), int((np.max(y) + 1)), dim, sgd, bias)\n    elif ex:\n        model_name = 'ex_model.h5'\n        model = build_ex_model(int((np.max(x) + 1)), int((np.max(y) + 1)), dim, sgd)\n    cb = [EarlyStopping(monitor='val_rmse', patience=30, verbose=1, mode='min'), TensorBoard(log_dir='.\/log', write_images=True), ModelCheckpoint(filepath=model_name, monitor='val_rmse', mode='min', save_best_only=True)]\n    history = None\n    if (not whole):\n        history = model.fit([x, y], z, batch_size=bs, epochs=epo, validation_data=([vx, vy], vz), callbacks=cb, verbose=1)\n    else:\n        history = model.fit([x, y], z, batch_size=bs, epochs=epo, callbacks=cb, verbose=1)\n    model.save('model.h5')\n    return history\n"}
{"label_name":"process","label":2,"method_name":"process_corpus","method":"\n\ndef process_corpus(corpus_path, sentence_normalizer, bpe_tokenizer, base_tokenizer=None, add_bos=True, add_eos=True):\n    processed_token_ids = []\n    raw_lines = []\n    with open(corpus_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            raw_lines.append(line)\n            line = sentence_normalizer(line)\n            if (base_tokenizer is not None):\n                line = ' '.join(base_tokenizer.encode(line))\n            bpe_token_ids = bpe_tokenizer.encode(line, output_type=int)\n            if add_bos:\n                bpe_token_ids = ([bpe_tokenizer.vocab.bos_id] + bpe_token_ids)\n            if add_eos:\n                bpe_token_ids.append(bpe_tokenizer.vocab.eos_id)\n            processed_token_ids.append(bpe_token_ids)\n    return (processed_token_ids, raw_lines)\n"}
{"label_name":"train","label":0,"method_name":"make_training_functions","method":"\n\ndef make_training_functions(cfg, model):\n    X = T.TensorType('float32', ([False] * 4))('X')\n    Z = T.TensorType('float32', ([False] * 2))('Z')\n    y = T.TensorType('float32', ([False] * 2))('y')\n    (p1, p2, p3) = (T.TensorType('int32', ([False] * 2))('p1'), T.TensorType('int32', ([False] * 2))('p2'), T.TensorType('int32', ([False] * 2))('p3'))\n    X_shared = lasagne.utils.shared_empty(4, dtype='float32')\n    y_shared = lasagne.utils.shared_empty(2, dtype='float32')\n    Z_shared = lasagne.utils.shared_empty(2, dtype='float32')\n    p1_shared = lasagne.utils.shared_empty(2, dtype='int32')\n    p2_shared = lasagne.utils.shared_empty(2, dtype='int32')\n    p3_shared = lasagne.utils.shared_empty(2, dtype='int32')\n    pi = np.cast[theano.config.floatX](np.pi)\n    l_in = model['l_in']\n    l_out = model['l_out']\n    l_Z = model['l_Z']\n    l_Z_IAF = model['l_Z_IAF']\n    l_mu = model['l_mu']\n    l_ls = model['l_ls']\n    l_IAF_mu = model['l_IAF_mu']\n    l_IAF_ls = model['l_IAF_ls']\n    l_introspect = model['l_introspect']\n    l_discrim = model['l_discrim']\n    batch_index = T.iscalar('batch_index')\n    batch_slice = slice((batch_index * cfg['batch_size']), ((batch_index + 1) * cfg['batch_size']))\n    rng = RandomStreams(lasagne.random.get_rng().randint(1, 69))\n    outputs = lasagne.layers.get_output((((((([l_out] + [l_mu]) + [l_ls]) + [l_discrim]) + [l_IAF_mu]) + [l_IAF_ls]) + l_introspect), {l_in: X})\n    X_hat = outputs[0]\n    Z_mu = outputs[1]\n    Z_ls = outputs[2]\n    p_X = outputs[3]\n    Z_IAF_mu = outputs[4]\n    Z_IAF_ls = outputs[5]\n    g_X = outputs[6:]\n    out_hat = lasagne.layers.get_output(([l_discrim] + l_introspect), {l_in: X_hat})\n    p_X_hat = out_hat[0]\n    g_X_hat = out_hat[1:]\n    p_X_gen = lasagne.layers.get_output(l_discrim, {l_in: lasagne.layers.get_output(l_out, {l_Z_IAF: Z})})\n\n    def ortho_res(z):\n        s = 0\n        for x in z:\n            if ((x.name[(- 1)] is 'W') and (x.ndim == 4)):\n                y = T.batched_tensordot(x, x.dimshuffle(0, 1, 3, 2), [[1, 3], [1, 2]])\n                y -= T.eye(x.shape[2], x.shape[3]).dimshuffle('x', 0, 1).repeat(x.shape[0], 0)\n                s += T.sum(T.abs_(y))\n        return s\n    pixel_loss = T.mean((2 * T.abs_(((X_hat - X) + 1e-08))))\n    kl_div = ((- 0.5) * T.mean((((1 + (2 * Z_ls)) - T.sqr(Z_mu)) - T.exp((2 * Z_ls)))))\n    params = lasagne.layers.get_all_params(l_out, trainable=True)\n    encoder_params = lasagne.layers.get_all_params(l_discrim, trainable=True)\n    Z_params = [p for p in lasagne.layers.get_all_params(l_Z_IAF, trainable=True) if (p not in lasagne.layers.get_all_params(l_discrim, trainable=True))]\n    print(Z_params)\n    decoder_params = [p for p in lasagne.layers.get_all_params(l_out, trainable=True) if (p not in lasagne.layers.get_all_params(l_Z, trainable=True))]\n    if isinstance(cfg['learning_rate'], dict):\n        learning_rate = theano.shared(np.float32(cfg['learning_rate'][0]))\n    else:\n        learning_rate = theano.shared(np.float32(cfg['learning_rate']))\n    print('Calculating Adversarial Loss and Grads...')\n    l2_Z = (cfg['reg'] * lasagne.regularization.apply_penalty([p for p in lasagne.layers.get_all_params(l_Z_IAF, trainable=True, regularizable=True) if (p not in lasagne.layers.get_all_params(l_discrim, trainable=True))], lasagne.regularization.l2))\n    if ('ortho' in cfg):\n        print('Applying orthogonal regularization...')\n        l2_discrim = (cfg['ortho'] * lasagne.regularization.apply_penalty((lasagne.layers.get_all_params(l_Z, trainable=True, regularizable=True) + l_discrim.get_params(trainable=True, regularizable=True)), ortho_res))\n        l2_gen = (cfg['ortho'] * lasagne.regularization.apply_penalty([p for p in lasagne.layers.get_all_params(l_out, trainable=True, regularizable=True) if (p not in encoder_params)], ortho_res))\n    discrim_g_loss = (T.mean(T.nnet.categorical_crossentropy(p_X_hat, p2)) + T.mean(T.nnet.categorical_crossentropy(p_X_gen, p3)))\n    discrim_d_loss = T.mean(T.nnet.categorical_crossentropy(p_X, p1))\n    adversarial_discrim_loss = ((cfg['dg_weight'] * discrim_g_loss) + (cfg['dd_weight'] * discrim_d_loss))\n    discrim_accuracy = (((T.mean(T.eq(T.argmax(p_X, axis=1), T.argmax(p1, axis=1))) + T.mean(T.eq(T.argmax(p_X_hat, axis=1), T.argmax(p2, axis=1)))) + T.mean(T.eq(T.argmax(p_X_gen, axis=1), T.argmax(p3, axis=1)))) \/ 3.0)\n    feature_loss = T.cast(T.mean([T.mean(lasagne.objectives.squared_error(g_X[i], g_X_hat[i])) for i in xrange(len(g_X_hat))]), 'float32')\n    gen_recon_loss = T.mean(T.nnet.categorical_crossentropy(p_X_hat, p1))\n    gen_sample_loss = T.mean(T.nnet.categorical_crossentropy(p_X_gen, p1))\n    adversarial_gen_loss = ((cfg['agr_weight'] * gen_recon_loss) + (cfg['ags_weight'] * gen_sample_loss))\n    discrim_updates = lasagne.updates.adam(T.grad((adversarial_discrim_loss + l2_discrim), encoder_params, consider_constant=[X_hat]), encoder_params, learning_rate, beta1=cfg['beta1'])\n    gen_updates = lasagne.updates.adam((((adversarial_gen_loss + (cfg['recon_weight'] * pixel_loss)) + (cfg['feature_weight'] * feature_loss)) + l2_gen), decoder_params, learning_rate, beta1=cfg['beta1'])\n    Z_gen_updates = lasagne.updates.adam((((((cfg['feature_weight'] * feature_loss) + (cfg['recon_weight'] * pixel_loss)) + adversarial_gen_loss) + kl_div) + l2_Z), Z_params, learning_rate=learning_rate, beta1=cfg['beta1'])\n    for ud in Z_gen_updates:\n        gen_updates[ud] = Z_gen_updates[ud]\n        discrim_updates[ud] = Z_gen_updates[ud]\n    error_rate = T.cast(T.mean(T.sqr((X_hat - X))), 'float32')\n    sample = theano.function([Z], lasagne.layers.get_output(l_out, {l_Z_IAF: Z}, deterministic=True), on_unused_input='warn')\n    Zfn = theano.function([X], lasagne.layers.get_output(l_Z_IAF, {l_in: X}, deterministic=True), on_unused_input='warn')\n    gd = OrderedDict()\n    gd['gen_recon_loss'] = gen_recon_loss\n    gd['gen_sample_loss'] = gen_sample_loss\n    gd['pixel_loss'] = pixel_loss\n    gd['feature_loss'] = feature_loss\n    gd['pixel_acc'] = (1 - error_rate)\n    dd = OrderedDict()\n    dd['discrim_g_loss'] = discrim_g_loss\n    dd['discrim_d_loss'] = discrim_d_loss\n    dd['discrim_acc'] = discrim_accuracy\n    dd['pixel_loss'] = pixel_loss\n    dd['pixel_acc'] = (1 - error_rate)\n    update_gen = theano.function([batch_index], [gd[i] for i in gd], updates=gen_updates, givens={X: X_shared[batch_slice], y: y_shared[batch_slice], Z: Z_shared[batch_slice], p1: p1_shared[batch_slice], p2: p2_shared[batch_slice], p3: p3_shared[batch_slice]}, on_unused_input='warn')\n    update_discrim = theano.function([batch_index], [dd[i] for i in dd], updates=discrim_updates, givens={X: X_shared[batch_slice], y: y_shared[batch_slice], Z: Z_shared[batch_slice], p1: p1_shared[batch_slice], p2: p2_shared[batch_slice], p3: p3_shared[batch_slice]}, on_unused_input='warn')\n    tfuncs = {'update_gen': update_gen, 'update_discrim': update_discrim, 'sample': sample, 'Zfn': Zfn}\n    tvars = {'X': X, 'y': y, 'Z': Z, 'X_shared': X_shared, 'y_shared': y_shared, 'Z_shared': Z_shared, 'p1': p1_shared, 'p2': p2_shared, 'p3': p3_shared, 'batch_slice': batch_slice, 'batch_index': batch_index, 'learning_rate': learning_rate, 'gd': gd, 'dd': dd}\n    return (tfuncs, tvars, model)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\n@route('\/predict', method='POST')\ndef predict():\n    response.content_type = 'application\/json'\n    try:\n        body = request.json\n        body = (body if isinstance(body, list) else [body])\n        X = pandas.DataFrame.from_dict(body)\n        if hasattr(model, 'get_booster'):\n            X = X[model.get_booster().feature_names]\n        result = model.predict(X)\n        return json.dumps(result.tolist())\n    except Exception as error:\n        response.status = 500\n        return json.dumps({'error': str(error)})\n"}
{"label_name":"process","label":2,"method_name":"process_files","method":"\n\ndef process_files(files):\n    for f in files:\n        logger.info('Reading {}'.format(f), newline=False)\n        image = imread(f)\n        image = imresize(image, args['scale'])\n        detected_faces = face_tool.detect_faces(image)\n        landmarks = face_tool.face_landmarks(image, detected_faces)\n        encodings = face_tool.face_encodings(image, landmarks)\n        prepared_known_faces = [face.encoding for face in faces]\n        labels = []\n        for face_encoding in encodings:\n            (index, _) = face_tool.compare_faces(prepared_known_faces, face_encoding)\n            face = face_tool.Face(face_encoding, path=f)\n            if (index >= 0):\n                face.id = faces[index].id\n            labels.append(str(face.id))\n            faces.append(face)\n        logger.info('. Done', prefix=False)\n        logger.info('Details , {} faces , labels {}'.format(len(detected_faces), labels))\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(image, model, shape_predictor=None):\n    if (NETWORK.use_landmarks or NETWORK.use_hog_and_landmarks or NETWORK.use_hog_sliding_window_and_landmarks):\n        face_rects = [dlib.rectangle(left=0, top=0, right=NETWORK.input_size, bottom=NETWORK.input_size)]\n        face_landmarks = np.array([get_landmarks(image, face_rects, shape_predictor)])\n        features = face_landmarks\n        if NETWORK.use_hog_sliding_window_and_landmarks:\n            hog_features = sliding_hog_windows(image)\n            hog_features = np.asarray(hog_features)\n            face_landmarks = face_landmarks.flatten()\n            features = np.concatenate((face_landmarks, hog_features))\n        else:\n            (hog_features, _) = hog(image, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualise=True)\n            hog_features = np.asarray(hog_features)\n            face_landmarks = face_landmarks.flatten()\n            features = np.concatenate((face_landmarks, hog_features))\n        tensor_image = image.reshape([(- 1), NETWORK.input_size, NETWORK.input_size, 1])\n        predicted_label = model.predict([tensor_image, features.reshape((1, (- 1)))])\n        return get_emotion(predicted_label[0])\n    else:\n        tensor_image = image.reshape([(- 1), NETWORK.input_size, NETWORK.input_size, 1])\n        predicted_label = model.predict(tensor_image)\n        return get_emotion(predicted_label[0])\n    return None\n"}
{"label_name":"train","label":0,"method_name":"load_train_data","method":"\n\ndef load_train_data(filename):\n    ext = filename.split('.')[(- 1)]\n    if (ext == 'csv'):\n        return read_smiles_csv(filename)\n    if (ext == 'smi'):\n        return read_smi(filename)\n    else:\n        raise ValueError('data is not smi or csv!')\n    return\n"}
{"label_name":"predict","label":4,"method_name":"generate_predict","method":"\n\ndef generate_predict(trainning_set, neighbor_k=1, more_details=False):\n    '\\n        \u8f93\u5165\u8bad\u7ec3\u96c6\uff0c\u6700\u8fd1\u90bb\u7684\u70b9\u4e2a\u6570\u4ee5\u8fd4\u56de\u5224\u65ad\u51fd\u6570\\n    '\n    kd_tree = KDTree.generate_tree(trainning_set)\n\n    def distance_measure(point0, point1):\n        '\\n            # \u8ddd\u79bb\u5ea6\u91cf\u51fd\u6570\uff0cp=2 \u4e3a\u6b27\u6c0f\u8ddd\u79bb\uff0c p=1\u4e3a\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\\n            # point: (x1, x2, x3, ...)\\n            # L = (\u03a3|point0_j - point1_j|**p) ** 1\/p\\n            # \u5982\u679cp\u662f\u221e L= max|point0_j - point1_j|\\n            \u8fd4\u56depoint0\uff0c point1\u4e4b\u95f4\u7684\u6b27\u5f0f\u8ddd\u79bb\\n        '\n        point0 = np.array(point0)\n        point1 = np.array(point1)\n        return np.sqrt(np.sum(np.square((point0 - point1))))\n\n    def predict(feature):\n        '\\n            \u5224\u65ad\u51fd\u6570\uff0c\u8f93\u5165\u7279\u5f81\u503c\u8f93\u51fa\u5224\u65ad\u7ed3\u679c\\n        '\n        if (not (neighbor_k % 2)):\n            warnings.warn('K\u503c\u4e3a\u5076\u6570\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u51b3\u7b56\u65f6\u6b63\u53cd\u6837\u672c\u6570\u76f8\u7b49\u7684\u60c5\u51b5')\n        if ((neighbor_k > len(trainning_set)) or (neighbor_k < 0)):\n            raise RuntimeError('K\u503c\u4e0d\u5408\u6cd5')\n        if (neighbor_k > (len(trainning_set) ** 0.5)):\n            warnings.warn('K\u503c\u8d85\u8fc7\u4e86\u6837\u672c\u603b\u6570\u7684\u5f00\u65b9')\n        dots = trainning_set[:neighbor_k]\n        near_dots_heap = MaxHeap(map((lambda item: (distance_measure(item[0], feature), item)), dots))\n\n        def search(tree, near_dots_heap, stack=None):\n            '\\n                \u6839\u636e\u8f93\u5165\u7684KD\u6811\u4ee5\u53ca\u5176\u5212\u5206\u7b56\u7565\uff0c\u68c0\u7d22\u6837\u672c\u70b9\u6240\u5728\u533a\u57df\uff08\u6216\u8005\u6700\u8fd1\u533a\u57df\uff09\u3002\u5e76\u5c06\u9014\u7ecf\u8fc7\u7684\u70b9\u5165\u6700\u5927\u503c\u5806\u5e76\u5c06\u5176\u5165\u6808\\n                tree\uff1aKD_tree\\n            '\n            if tree.is_leaf:\n                '\\n                    \u5982\u679c\u67e5\u627e\u5230\u4e86\u6700\u63a5\u8fd1\u6837\u672c\u70b9\u7684\u53f6\u8282\u70b9\uff0c\u5219\u5f00\u59cb\u56de\u6eaf\\n                '\n                return (near_dots_heap, stack)\n            if (not stack):\n                stack = []\n            distance = distance_measure(tree.dot[0], feature)\n            if (distance < near_dots_heap.max_key):\n                near_dots_heap.pushpop((distance, tree.dot))\n            if (feature[tree.axis] < tree.edge):\n                stack.append(('left_tree', tree))\n                return search(tree.left_tree, near_dots_heap, stack)\n            stack.append(('right_tree', tree))\n            return search(tree.right_tree, near_dots_heap, stack)\n        (near_dots_heap, stack) = search(kd_tree, near_dots_heap)\n        '\\n            \u8fd4\u56de\u904d\u5386\u6808\u4ee5\u65b9\u4fbf\u540e\u9762\u7684\u56de\u6eaf\\n        '\n\n        def review(stack, near_dots_heap):\n            '\\n                \u56de\u6eaf\uff0c\u8f93\u5165\u56de\u6eaf\u6808\uff0c\u6700\u5927\u503c\u5806\\n                \u8fd4\u56de\u6700\u5927\u503c\u5806\\n            '\n            if (not stack):\n                return near_dots_heap\n            radius = near_dots_heap.max_key\n            (label, tree) = stack.pop()\n            if (label == 'left_tree'):\n                subtree = tree.right_tree\n            elif (label == 'right_tree'):\n                subtree = tree.left_tree\n            if (((feature[tree.axis] - radius) < tree.edge) and ((feature[tree.axis] + radius) > tree.edge)):\n                near_dots_heap = search(subtree, near_dots_heap)[0]\n            return review(stack, near_dots_heap)\n        near_dots_heap = review(stack, near_dots_heap)\n        result = (1 if (sum(map((lambda item: item[1][1]), near_dots_heap.to_list())) > 0) else (- 1))\n        if more_details:\n            return (result, near_dots_heap.to_list(), kd_tree)\n        return result\n    return predict\n"}
{"label_name":"train","label":0,"method_name":"pretrain","method":"\n\ndef pretrain(sess, generator, target_lstm, train_discriminator):\n    gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n    gen_data_loader.create_batches(positive_samples)\n    results = OrderedDict({'exp_name': PREFIX})\n    print('Start pre-training...')\n    start = time.time()\n    for epoch in tqdm(range(PRE_EPOCH_NUM)):\n        print(' gen pre-train')\n        loss = pre_train_epoch(sess, generator, gen_data_loader)\n        if ((epoch == 10) or ((epoch % 40) == 0)):\n            samples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\n            likelihood_data_loader.create_batches(samples)\n            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n            print('\\t test_loss {}, train_loss {}'.format(test_loss, loss))\n            mm.compute_results(samples, train_samples, ord_dict, results)\n    samples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\n    likelihood_data_loader.create_batches(samples)\n    test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n    samples = generate_samples(sess, generator, BATCH_SIZE, SAMPLE_NUM)\n    likelihood_data_loader.create_batches(samples)\n    print('Start training discriminator...')\n    for i in tqdm(range(dis_alter_epoch)):\n        print(' discriminator pre-train')\n        (d_loss, acc) = train_discriminator()\n    end = time.time()\n    print('Total time was {:.4f}s'.format((end - start)))\n    return\n"}
{"label_name":"process","label":2,"method_name":"process_birch","method":"\n\ndef process_birch(sample):\n    instance = birch(sample, NUMBER_CLUSTERS)\n    (ticks, _) = timedcall(instance.process)\n    return ticks\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(sess, minimize_ops, num_epochs, num_unrolls):\n    'L2L training.'\n    (step, update, reset, loss_last, x_last) = minimize_ops\n    for _ in xrange(num_epochs):\n        sess.run(reset)\n        for _ in xrange(num_unrolls):\n            (cost, final_x, unused_1, unused_2) = sess.run([loss_last, x_last, update, step])\n    return (cost, final_x)\n"}
{"label_name":"save","label":1,"method_name":"register_save","method":"\n\ndef register_save(id, driver):\n    '\\n    Registers an image save function.  This function should not be\\n    used in application code.\\n\\n    :param id: An image format identifier.\\n    :param driver: A function to save images in this format.\\n    '\n    SAVE[id.upper()] = driver\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict():\n    prediction_window = 15\n    training_window = 60\n    symbol = 'AMZN'\n    ticker = yf.Ticker(symbol)\n    now = datetime.datetime.now()\n    start_window = (now - timedelta(days=(training_window + prediction_window)))\n    print(now)\n    print(start_window)\n    hist = ticker.history(start=start_window.strftime('%Y-%m-%d'), end=now.strftime('%Y-%m-%d'))\n    (t, w) = ((len(hist) - prediction_window), prediction_window)\n    ts = pd.Series(hist['Close'].values)\n    ts = ts.astype(float)\n    model = ARIMA(ts[0:t], order=(0, 1, 0))\n    res = model.fit()\n    print(res.summary())\n    (fig, ax) = plt.subplots()\n    print(t)\n    res.plot_predict(start=2, end=(t + w), alpha=0.05, ax=ax)\n    plt.plot(ts[(t - 1):(t + w)], label='realisations')\n    plt.title(((((symbol + '    ') + start_window.strftime('%Y-%m-%d')) + '    ') + now.strftime('%Y-%m-%d')))\n    print(('Std residuals: ' + str(statistics.stdev(res.resid))))\n    plt.show()\n"}
{"label_name":"predict","label":4,"method_name":"predict_keras","method":"\n\ndef predict_keras(X_train, y_train, X_test, hid_num=30, epc_num=50, verb=False):\n    y_tr = np.where((y_train == 1), 1, (- 1))\n    model = Sequential()\n    model.add(Dense(hid_num, input_dim=3))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(output_dim=1, W_regularizer=l2(0.01)))\n    model.add(Activation('linear'))\n    model.compile(loss='hinge', optimizer='adadelta', metrics=['accuracy'])\n    model.fit(X_train, y_tr, nb_epoch=epc_num, verbose=verb)\n    predictions = model.predict(X_test)\n    pred_rounded = np.sign(predictions)\n    pred_rounded = np.where((pred_rounded == 1), 1, 0)\n    pred_rounded = np.squeeze(pred_rounded)\n    if verb:\n        print('Test predictions \\n', pred_rounded)\n    return pred_rounded\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n\ndef preprocess_image(src):\n    img = cv2.resize(src, (300, 300))\n    img = (img - 127.5)\n    img = (img * 0.007843)\n    return img\n"}
{"label_name":"process","label":2,"method_name":"process_samples","method":"\n\ndef process_samples(samples: SampleBatchType):\n    filter_keys = [SampleBatch.CUR_OBS, SampleBatch.ACTIONS, SampleBatch.NEXT_OBS]\n    filtered = {}\n    for key in filter_keys:\n        filtered[key] = samples[key]\n    return SampleBatch(filtered)\n"}
{"label_name":"train","label":0,"method_name":"check_freeze_layers_train_on_catdog_datasets_int","method":"\n\ndef check_freeze_layers_train_on_catdog_datasets_int(train_path, val_path, trainer_args={}):\n    with TemporaryDirectory() as output_model_dir, TemporaryDirectory() as output_logs_dir:\n        trainer = Trainer(train_dataset_dir=train_path, val_dataset_dir=val_path, output_model_dir=output_model_dir, output_logs_dir=output_logs_dir, epochs=1, batch_size=1, model_kwargs={'alpha': 0.25}, freeze_layers_list=list(range(1, 10)), **trainer_args)\n        trainer.run()\n        for i in range(1, 10):\n            actual = trainer.model.layers[i].trainable\n            expected = False\n            assert (actual == expected)\n"}
{"label_name":"process","label":2,"method_name":"_process_utterance","method":"\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    'Preprocesses a single utterance audio\/text pair.\\n\\n    This writes the mel and linear scale spectrograms to disk and returns a tuple to write\\n    to the train.txt file.\\n\\n    Args:\\n      out_dir: The directory to write the spectrograms into\\n      index: The numeric index to use in the spectrogram filenames.\\n      wav_path: Path to the audio file containing the speech input\\n      text: The text spoken in the input audio file\\n\\n    Returns:\\n      A (spectrogram_filename, mel_filename, n_frames, text) tuple to write to train.txt\\n    '\n    wav = audio.load_wav(wav_path)\n    if hparams.rescaling:\n        wav = ((wav \/ np.abs(wav).max()) * hparams.rescaling_max)\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n    spectrogram_filename = ('nikl-single-spec-%05d.npy' % index)\n    mel_filename = ('nikl-single-mel-%05d.npy' % index)\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n    return (spectrogram_filename, mel_filename, n_frames, text)\n"}
{"label_name":"process","label":2,"method_name":"_postprocess_image","method":"\n\ndef _postprocess_image(image):\n    if (image.ndim == 2):\n        image = image[(..., None)]\n    return image.transpose(2, 0, 1)\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\n\ndef save_model(model, destination):\n    '\\n    Save a model into destination.\\n    '\n    if ('model' not in destination):\n        destination.create_group('model')\n    for (name, value) in model.state_dict().items():\n        save_params(destination, ('model\/' + name), value)\n    return\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(num_batches, batch_size, learning_rate):\n    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n    labels = tf.placeholder(tf.float32, [None, 10])\n    is_training = tf.placeholder(tf.bool)\n    layer = inputs\n    for layer_i in range(1, 20):\n        layer = conv_layer(layer, layer_i, is_training)\n    orig_shape = layer.get_shape().as_list()\n    layer = tf.reshape(layer, shape=[(- 1), ((orig_shape[1] * orig_shape[2]) * orig_shape[3])])\n    layer = fully_connected(layer, 100, is_training)\n    logits = tf.layers.dense(layer, 10)\n    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for batch_i in range(num_batches):\n            (batch_xs, batch_ys) = mnist.train.next_batch(batch_size)\n            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n            if ((batch_i % 100) == 0):\n                (loss, acc) = sess.run([model_loss, accuracy], {inputs: mnist.validation.images, labels: mnist.validation.labels, is_training: False})\n                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n            elif ((batch_i % 25) == 0):\n                (loss, acc) = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n        acc = sess.run(accuracy, {inputs: mnist.validation.images, labels: mnist.validation.labels, is_training: False})\n        print('Final validation accuracy: {:>3.5f}'.format(acc))\n        acc = sess.run(accuracy, {inputs: mnist.test.images, labels: mnist.test.labels, is_training: False})\n        print('Final test accuracy: {:>3.5f}'.format(acc))\n        correct = 0\n        for i in range(100):\n            correct += sess.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], labels: [mnist.test.labels[i]], is_training: False})\n        print('Accuracy on 100 samples:', (correct \/ 100))\n"}
{"label_name":"save","label":1,"method_name":"save_run","method":"\n\ndef save_run():\n    '\\n    Append current run git revision + python arguments to exp.lisp\\n    :return: git revision string\\n    '\n    git_rev = subprocess.Popen('git rev-parse --short HEAD', shell=True, stdout=subprocess.PIPE).stdout.read()[:(- 1)]\n    print(('Saving run to runs.sh - %s' % git_rev))\n    with open('runs.sh', 'a') as f:\n        f.write(datetime.now().strftime('# %H:%M - %d %b %y\\n'))\n        f.write(('# git checkout %s\\n' % git_rev))\n        f.write('# python main.py ')\n        for (k, v) in args._get_kwargs():\n            f.write((\"--%s='%s' \" % (k, v)))\n        f.write('\\n# git checkout master\\n\\n')\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(neighbors):\n    classVote = {}\n    for x in range(len(neighbors)):\n        res = neighbors[x][(- 2)]\n        if (res in classVote):\n            classVote[res] += 1\n        else:\n            classVote[res] = 1\n    sortedVotes = sorted(classVote.items(), key=operator.itemgetter(1), reverse=True)\n    return sortedVotes[0][0]\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\n@gc.command(help='Start a training job')\n@click.option('--job-id', help='Identifies the training job in Google Cloud. Will use it to name the folder where checkpoints and logs will be stored, except when resuming a previous training job.')\n@click.option('--resume', 'resume_job_id', help='Id of the previous job to resume (start from last stored checkpoint). In case you are resuming multiple times, must always point to the first job (ie. the one that first created the checkpoint).')\n@click.option('--bucket', 'bucket_name', help='Bucket where to create the folder to save checkpoints and logs. If resuming a job, it must match the bucket used for the original job.')\n@click.option('--region', default='us-central1', help='Region in which to run the job.')\n@click.option('--dataset', help='Complete path (bucket included) to the folder where the dataset is located (TFRecord files).')\n@click.option('config_files', '--config', '-c', required=True, multiple=True, help='Path to config to use in training.')\n@click.option('--scale-tier', default=DEFAULT_SCALE_TIER, type=click.Choice(SCALE_TIERS))\n@click.option('--master-type', default=DEFAULT_MASTER_TYPE, type=click.Choice(MACHINE_TYPES))\n@click.option('--worker-type', default=DEFAULT_WORKER_TYPE, type=click.Choice(MACHINE_TYPES))\n@click.option('--worker-count', default=DEFAULT_WORKER_COUNT, type=int)\n@click.option('--parameter-server-type', default=DEFAULT_PS_TYPE, type=click.Choice(MACHINE_TYPES))\n@click.option('--parameter-server-count', default=DEFAULT_PS_COUNT, type=int)\n@check_dependencies\ndef train(job_id, resume_job_id, bucket_name, region, config_files, dataset, scale_tier, master_type, worker_type, worker_count, parameter_server_type, parameter_server_count):\n    account = ServiceAccount()\n    account.validate_region(region)\n    if (bucket_name is None):\n        bucket_name = 'luminoth-{}'.format(account.client_id)\n        click.echo('Bucket name not specified. Using \"{}\".'.format(bucket_name))\n    bucket = account.get_bucket(bucket_name)\n    if (not job_id):\n        job_id = 'train_{}'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n    base_path = 'lumi_{}'.format((resume_job_id if resume_job_id else job_id))\n    package_path = build_package(bucket, base_path)\n    job_dir = 'gs:\/\/{}\/{}'.format(bucket_name, base_path)\n    override_params = ['train.job_dir={}'.format(job_dir)]\n    if dataset:\n        if (not dataset.startswith('gs:\/\/')):\n            dataset = 'gs:\/\/{}'.format(dataset)\n        override_params.append('dataset.dir={}'.format(dataset))\n    config = get_config(config_files, override_params=override_params)\n    config_path = '{}\/{}'.format(base_path, DEFAULT_CONFIG_FILENAME)\n    upload_data(bucket, config_path, dump_config(config))\n    args = ['--config', '{}\/{}'.format(job_dir, DEFAULT_CONFIG_FILENAME)]\n    cloudml = account.cloud_service('ml')\n    training_inputs = {'scaleTier': scale_tier, 'packageUris': ['gs:\/\/{}\/{}'.format(bucket_name, package_path)], 'pythonModule': 'luminoth.train', 'args': args, 'region': region, 'jobDir': job_dir, 'runtimeVersion': RUNTIME_VERSION, 'pythonVersion': PYTHON_VERSION}\n    if (scale_tier == 'CUSTOM'):\n        training_inputs['masterType'] = master_type\n        if (worker_count > 0):\n            training_inputs['workerCount'] = worker_count\n            training_inputs['workerType'] = worker_type\n        if (parameter_server_count > 0):\n            training_inputs['parameterServerCount'] = parameter_server_count\n            training_inputs['parameterServerType'] = parameter_server_type\n    job_spec = {'jobId': job_id, 'trainingInput': training_inputs}\n    jobrequest = cloudml.projects().jobs().create(body=job_spec, parent='projects\/{}'.format(account.project_id))\n    try:\n        click.echo('Submitting training job.')\n        res = jobrequest.execute()\n        click.echo('Job submitted successfully.')\n        click.echo('state = {}, createTime = {}'.format(res.get('state'), res.get('createTime')))\n        if resume_job_id:\n            click.echo('\\nNote: this job is resuming job {}.\\n'.format(resume_job_id))\n        click.echo('Job id: {}'.format(job_id))\n        click.echo('Job directory: {}'.format(job_dir))\n        save_run(config, environment='gcloud', extra_config=job_spec)\n    except Exception as err:\n        click.echo('There was an error creating the training job. Check the details: \\n{}'.format(err._get_reason()))\n"}
{"label_name":"train","label":0,"method_name":"trainData","method":"\n\ndef trainData(useIntegrate=False, platform=PlatformType.Default, problemOrCategoryWise=problemOrCategoryKeys['category'], modelNumber=1, mlAlgos=ClassifierType.allClassifierTypes, test_size=defaultTestSize, number_of_top_words=10):\n    if useIntegrate:\n        platform = PlatformType.Default\n    print(str(type(number_of_top_words)))\n    metricsFileName = (get_problem_metrics_file_convention(platform=platform, model_number=modelNumber, category_or_problemWise=('catWise' if (problemOrCategoryWise is 1) else 'probWise'), test_size=test_size, number_of_top_words=number_of_top_words) + '_metrics.csv')\n    if (problemOrCategoryWise == problemOrCategoryKeys['category']):\n        classifierMetricsMap = {}\n        for classifier in mlAlgos:\n            print(('CLASSIFICATION USING ' + ClassifierType.classifierTypeString[classifier]))\n            metricsList = []\n            for category in categories:\n                print(('Processing for category: ' + category))\n                if (modelNumber == 1):\n                    generateLazyLoad(useIntegrate, category, platform, test_size=test_size, number_of_top_words=number_of_top_words)\n                    m = train_for_categoryModel1(platform=platform, number_of_top_words=number_of_top_words, category=category, classifier=classifier, test_size=test_size)\n                elif (modelNumber == 2):\n                    tempDataFileConv = ''\n                    uniqueFileConvention = ((((((PlatformType.platformString[platform] + '_') + ('model1' if (modelNumber is 1) else 'model2')) + '_') + ('catWise' if (problemOrCategoryWise is 1) else 'probWise')) + '_') + str(number_of_top_words))\n                    dataFileConvention = ((((PlatformType.platformString[platform] + '_') + ('model1' if (modelNumber is 1) else 'model2')) + '_') + str(number_of_top_words))\n                    generateLazyLoadForModel2(useIntegrate, category, platform, uniqueFileConvention, tempDataFileConv, test_size=test_size)\n                    m = train_for_categoryModel2(category, classifier, uniqueFileConvention, tempDataFileConv, test_size=test_size)\n                metricsList.append(m)\n                print('================ CATEGORY OVER ================')\n                if m.isValid:\n                    print(((((('Metrics: ' + str(m.precision[1])) + ' ') + str(m.recall[1])) + ' ') + str(m.fScore[1])))\n                else:\n                    print('Metrics invalid')\n            classifierMetricsMap[classifier] = metricsList\n            print('================ CLASSIFIER OVER ================')\n        Metrics.writeMultipleMetics(metricsFileName, classifierMetricsMap, isPositiveBased=True)\n        print('Metrics are written to the file')\n    else:\n        classifierMetricsMap = {}\n        for classifier in mlAlgos:\n            if (modelNumber == 1):\n                if (classifier in ClassifierType.onlyHyperClassifiers):\n                    print('Cannot apply hyper based classifiers to problem-wise modelling technique')\n                    continue\n                print(('Processing for classifier: ' + ClassifierType.classifierTypeString[classifier]))\n                classifierMetricsMap[classifier] = get_accuracy(categories, classifier, useIntegrated=useIntegrate, platform=platform, modelNumber=1, test_size=test_size, number_of_top_words=number_of_top_words)\n                print('=================== CLASSIFICATION OVER ===================')\n        Metrics.writeMultipleProblemwiseMetics(metricsFileName=metricsFileName, classifierMetricsMap=classifierMetricsMap)\n"}
{"label_name":"train","label":0,"method_name":"train_models","method":"\n\ndef train_models(component_builder, data):\n\n    def train(cfg_name, project_name):\n        from rasa_nlu.train import create_persistor\n        from rasa_nlu import training_data\n        cfg = config.load(cfg_name)\n        trainer = Trainer(cfg, component_builder)\n        training_data = training_data.load_data(data)\n        trainer.train(training_data)\n        trainer.persist('test_projects', project_name=project_name)\n    train('sample_configs\/config_spacy.yml', 'test_project_spacy_sklearn')\n    train('sample_configs\/config_mitie.yml', 'test_project_mitie')\n    train('sample_configs\/config_mitie_sklearn.yml', 'test_project_mitie_sklearn')\n"}
{"label_name":"save","label":1,"method_name":"save_on_master","method":"\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n"}
{"label_name":"save","label":1,"method_name":"save_trimmed_model","method":"\n\ndef save_trimmed_model(model, name):\n    jsonfilename = (str(name) + '.json')\n    weightsfilename = (str(name) + '.h5')\n    with open(jsonfilename, 'w') as json_file:\n        json_file.write(model.to_json())\n    model.save_weights(weightsfilename)\n    return\n"}
{"label_name":"process","label":2,"method_name":"phrase_processed","method":"\n\ndef phrase_processed(list_of_words):\n    words = []\n    for word in list_of_words:\n        split = word.split()\n        for word in split:\n            alpha_word = ''.join([i for i in word if (i.isalpha() or i.isdigit())])\n            if ((alpha_word != '') and (alpha_word != ' ')):\n                words.append(alpha_word)\n    return words\n"}
{"label_name":"save","label":1,"method_name":"save_json","method":"\n\ndef save_json(json_data, output_file):\n    with open(output_file, 'w') as fp:\n        json.dump(json_data, fp)\n"}
{"label_name":"save","label":1,"method_name":"save_npz","method":"\n\ndef save_npz(file, obj, compression=True):\n    'Saves an object to the file in NPZ format.\\n\\n    This is a short-cut function to save only one object into an NPZ file.\\n\\n    Args:\\n        file (str or file-like): Target file to write to.\\n        obj: Object to be serialized. It must support serialization protocol.\\n            If it is a dictionary object, the serialization will be skipped.\\n        compression (bool): If ``True``, compression in the resulting zip file\\n            is enabled.\\n\\n    .. seealso::\\n        :func:`chainer.serializers.load_npz`\\n\\n    '\n    if isinstance(file, six.string_types):\n        with open(file, 'wb') as f:\n            save_npz(f, obj, compression)\n        return\n    if isinstance(obj, dict):\n        target = obj\n    else:\n        s = DictionarySerializer()\n        s.save(obj)\n        target = s.target\n    if compression:\n        numpy.savez_compressed(file, **target)\n    else:\n        numpy.savez(file, **target)\n"}
{"label_name":"predict","label":4,"method_name":"predict_wsgi","method":"\n\ndef predict_wsgi(environ, start_response):\n    img = read_image_from_wsgi_request(environ)\n    if (not img):\n        return Response('no file uploaded', 400)(environ, start_response)\n    global predictor\n    if (not predictor):\n        predictor = Predictor()\n    prediction = predictor.predict_digit(img)\n    print(json.dumps({'vh_metadata': {k: v for (k, v) in prediction.items() if (k != 'predictions')}}))\n    response = Response(json.dumps(prediction), mimetype='application\/json')\n    return response(environ, start_response)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\n@hvd.elastic.run\ndef train(state):\n    state.model.fit(dataset, steps_per_epoch=(500 \/\/ hvd.size()), epochs=(24 - state.epoch), callbacks=callbacks, verbose=1)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(y, phi, theta):\n    a = np.ones(((1 + theta.shape[0]),), dtype=y.dtype)\n    b = np.ones(((1 + phi.shape[0]),), dtype=y.dtype)\n    a[1:] = theta\n    b[1:] = (- phi)\n    residual = signal.lfilter(b, a, y.copy())\n    return residual\n"}
{"label_name":"process","label":2,"method_name":"check_processed_cols","method":"\n\ndef check_processed_cols(col, utility):\n    return (True in [True for y in col if ((y != utility.FLAGS.pad_int) and (y != utility.FLAGS.bad_number_pre_process))])\n"}
{"label_name":"save","label":1,"method_name":"save_json","method":"\n\ndef save_json(obj, fname, sort_keys=True, indent=4, separators=None, encoder=ComplexEncoder, *args, **kwargs):\n    with open(fname, 'w') as fd:\n        json.dump(obj, fd, *args, indent=indent, sort_keys=sort_keys, cls=encoder, separators=separators, **kwargs)\n"}
{"label_name":"process","label":2,"method_name":"_process_input_file","method":"\n\ndef _process_input_file(filename, vocab, stats):\n    'Processes the sentences in an input file.\\n\\n  Args:\\n    filename: Path to a pre-tokenized input .txt file.\\n    vocab: A dictionary of word to id.\\n    stats: A Counter object for statistics.\\n\\n  Returns:\\n    processed: A list of serialized Example protos\\n  '\n    tf.logging.info('Processing input file: %s', filename)\n    processed = []\n    predecessor = None\n    current = None\n    successor = None\n    for successor_str in tf.gfile.FastGFile(filename):\n        stats.update(['sentences_seen'])\n        successor = successor_str.split()\n        if (predecessor and current and successor):\n            stats.update(['sentences_considered'])\n            if (FLAGS.max_sentence_length and ((len(predecessor) >= FLAGS.max_sentence_length) or (len(current) >= FLAGS.max_sentence_length) or (len(successor) >= FLAGS.max_sentence_length))):\n                stats.update(['sentences_too_long'])\n            else:\n                serialized = _create_serialized_example(predecessor, current, successor, vocab)\n                processed.append(serialized)\n                stats.update(['sentences_output'])\n        predecessor = current\n        current = successor\n        sentences_seen = stats['sentences_seen']\n        sentences_output = stats['sentences_output']\n        if (sentences_seen and ((sentences_seen % 100000) == 0)):\n            tf.logging.info('Processed %d sentences (%d output)', sentences_seen, sentences_output)\n        if (FLAGS.max_sentences and (sentences_output >= FLAGS.max_sentences)):\n            break\n    tf.logging.info('Completed processing file %s', filename)\n    return processed\n"}
{"label_name":"save","label":1,"method_name":"save_doc2vec_model","method":"\n\ndef save_doc2vec_model(model, model_path):\n    if use_tags:\n        model.save(model_path, pickle_protocol=3)\n    else:\n        model.save(model_path)\n"}
{"label_name":"process","label":2,"method_name":"_should_postprocess","method":"\n\ndef _should_postprocess(layer_type):\n    return (layer_type not in ['timing', 'pos_emb'])\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\n\ndef save_model(svm, filename):\n    joblib.dump(svm, ('model_params\/' + filename))\n"}
{"label_name":"process","label":2,"method_name":"process_file","method":"\n\ndef process_file(file: str) -> LintResult:\n    header_printed = False\n    with open(file, 'rb') as stream:\n        result = lint(stream)\n    for item in result.messages:\n        if (not header_printed):\n            print('>>>', file)\n            header_printed = True\n        print('{}: {}'.format(item['type'], item['message']))\n        print(('-' * 60))\n    return result\n"}
{"label_name":"save","label":1,"method_name":"save_reusable_ftrl_csv","method":"\n\ndef save_reusable_ftrl_csv(tmpdir, X, columns=None, opt_y=None):\n    filename = (('reusable_' + str(abs(X.hashcode(opt_y)))) + 'csv.gz')\n    filename = ((tmpdir + '\/') + filename)\n    if os.path.isfile(filename):\n        print('reusing past file:', filename)\n        return filename\n    print('creating new ftrl compatible file:', filename)\n    return save_ftrl_csv(filename, X, columns, opt_y)\n"}
{"label_name":"process","label":2,"method_name":"_process_dataset","method":"\n\ndef _process_dataset(name, directory, num_shards, synset_to_human, image_to_bboxes):\n    \"Process a complete data set and save it as a TFRecord.\\n\\n  Args:\\n    name: string, unique identifier specifying the data set.\\n    directory: string, root path to the data set.\\n    num_shards: integer number of shards for this data set.\\n    synset_to_human: dict of synset to human labels, e.g.,\\n      'n02119022' --> 'red fox, Vulpes vulpes'\\n    image_to_bboxes: dictionary mapping image file names to a list of\\n      bounding boxes. This list contains 0+ bounding boxes.\\n  \"\n    (filenames, synsets, labels) = _find_image_files(directory, FLAGS.labels_file)\n    humans = _find_human_readable_labels(synsets, synset_to_human)\n    bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n    _process_image_files(name, filenames, synsets, labels, humans, bboxes, num_shards)\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\n\ndef forward(inputs_i, Q, G, A, b, h, U_Q, U_S, R, verbose=False):\n    '\\n    b = A z_0\\n    h = G z_0 + s_0\\n    U_Q, U_S, R = pre_factor_kkt(Q, G, A, nineq, neq)\\n    '\n    (nineq, nz, neq, _) = get_sizes(G, A)\n    d = torch.ones(nineq).type_as(Q)\n    nb = ((- b) if (b is not None) else None)\n    factor_kkt(U_S, R, d)\n    (x, s, z, y) = solve_kkt(U_Q, d, G, A, U_S, inputs_i, torch.zeros(nineq).type_as(Q), (- h), nb)\n    if (torch.min(s) < 0):\n        s -= (torch.min(s) - 1)\n    if (torch.min(z) < 0):\n        z -= (torch.min(z) - 1)\n    prev_resid = None\n    for i in range(20):\n        rx = ((((torch.mv(A.t(), y) if (neq > 0) else 0.0) + torch.mv(G.t(), z)) + torch.mv(Q, x)) + inputs_i)\n        rs = z\n        rz = ((torch.mv(G, x) + s) - h)\n        ry = ((torch.mv(A, x) - b) if (neq > 0) else torch.Tensor([0.0]))\n        mu = (torch.dot(s, z) \/ nineq)\n        pri_resid = (torch.norm(ry) + torch.norm(rz))\n        dual_resid = torch.norm(rx)\n        resid = ((pri_resid + dual_resid) + (nineq * mu))\n        d = (z \/ s)\n        if verbose:\n            print(('primal_res = {0:.5g}, dual_res = {1:.5g}, ' + 'gap = {2:.5g}, kappa(d) = {3:.5g}').format(pri_resid, dual_resid, mu, (min(d) \/ max(d))))\n        improved = ((prev_resid is None) or (resid < (prev_resid + 1e-06)))\n        if ((not improved) or (resid < 1e-06)):\n            return (x, y, z)\n        prev_resid = resid\n        factor_kkt(U_S, R, d)\n        (dx_aff, ds_aff, dz_aff, dy_aff) = solve_kkt(U_Q, d, G, A, U_S, rx, rs, rz, ry)\n        alpha = min(min(get_step(z, dz_aff), get_step(s, ds_aff)), 1.0)\n        sig = ((torch.dot((s + (alpha * ds_aff)), (z + (alpha * dz_aff))) \/ torch.dot(s, z)) ** 3)\n        (dx_cor, ds_cor, dz_cor, dy_cor) = solve_kkt(U_Q, d, G, A, U_S, torch.zeros(nz).type_as(Q), (((((- mu) * sig) * torch.ones(nineq).type_as(Q)) + (ds_aff * dz_aff)) \/ s), torch.zeros(nineq).type_as(Q), torch.zeros(neq).type_as(Q))\n        dx = (dx_aff + dx_cor)\n        ds = (ds_aff + ds_cor)\n        dz = (dz_aff + dz_cor)\n        dy = ((dy_aff + dy_cor) if (neq > 0) else None)\n        alpha = min(1.0, (0.999 * min(get_step(s, ds), get_step(z, dz))))\n        dx_norm = torch.norm(dx)\n        dz_norm = torch.norm(dz)\n        if (np.isnan(dx_norm) or (dx_norm > 100000.0) or (dz_norm > 100000.0)):\n            return (x, y, z)\n        x += (alpha * dx)\n        s += (alpha * ds)\n        z += (alpha * dz)\n        y = ((y + (alpha * dy)) if (neq > 0) else None)\n    return (x, y, z)\n"}
{"label_name":"train","label":0,"method_name":"pre_train_epoch","method":"\n\ndef pre_train_epoch(sess, trainable_model, data_loader):\n    supervised_g_losses = []\n    data_loader.reset_pointer()\n    for it in range(data_loader.num_batch):\n        batch = data_loader.next_batch()\n        (_, g_loss, g_pred) = trainable_model.pretrain_step(sess, batch)\n        supervised_g_losses.append(g_loss)\n    return np.mean(supervised_g_losses)\n"}
{"label_name":"train","label":0,"method_name":"run_training","method":"\n\ndef run_training(train_op, loss, global_step, variables_to_restore=None, pretrained_model_dir=None):\n    'Sets up and runs training loop.'\n    tf.gfile.MakeDirs(FLAGS.train_dir)\n    if pretrained_model_dir:\n        assert variables_to_restore\n        tf.logging.info('Will attempt restore from %s: %s', pretrained_model_dir, variables_to_restore)\n        saver_for_restore = tf.train.Saver(variables_to_restore)\n    if FLAGS.sync_replicas:\n        local_init_op = tf.get_collection('local_init_op')[0]\n        ready_for_local_init_op = tf.get_collection('ready_for_local_init_op')[0]\n    else:\n        local_init_op = tf.train.Supervisor.USE_DEFAULT\n        ready_for_local_init_op = tf.train.Supervisor.USE_DEFAULT\n    is_chief = (FLAGS.task == 0)\n    sv = tf.train.Supervisor(logdir=FLAGS.train_dir, is_chief=is_chief, save_summaries_secs=(5 * 60), save_model_secs=(5 * 60), local_init_op=local_init_op, ready_for_local_init_op=ready_for_local_init_op, global_step=global_step)\n    with sv.managed_session(master=FLAGS.master, config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement), start_standard_services=False) as sess:\n        if is_chief:\n            if pretrained_model_dir:\n                maybe_restore_pretrained_model(sess, saver_for_restore, pretrained_model_dir)\n            if FLAGS.sync_replicas:\n                sess.run(tf.get_collection('chief_init_op')[0])\n            sv.start_standard_services(sess)\n        sv.start_queue_runners(sess)\n        global_step_val = 0\n        while ((not sv.should_stop()) and (global_step_val < FLAGS.max_steps)):\n            global_step_val = train_step(sess, train_op, loss, global_step)\n        sv.stop()\n        if is_chief:\n            sv.saver.save(sess, sv.save_path, global_step=global_step)\n"}
{"label_name":"save","label":1,"method_name":"write_saved_model","method":"\n\ndef write_saved_model(saved_model_path, frozen_graph_def, inputs, outputs):\n    'Writes SavedModel to disk.\\n\\n  If checkpoint_path is not None bakes the weights into the graph thereby\\n  eliminating the need of checkpoint files during inference. If the model\\n  was trained with moving averages, setting use_moving_averages to true\\n  restores the moving averages, otherwise the original set of variables\\n  is restored.\\n\\n  Args:\\n    saved_model_path: Path to write SavedModel.\\n    frozen_graph_def: tf.GraphDef holding frozen graph.\\n    inputs: The input image tensor to use for detection.\\n    outputs: A tensor dictionary containing the outputs of a DetectionModel.\\n  '\n    with tf.Graph().as_default():\n        with session.Session() as sess:\n            tf.import_graph_def(frozen_graph_def, name='')\n            builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)\n            tensor_info_inputs = {'inputs': tf.saved_model.utils.build_tensor_info(inputs)}\n            tensor_info_outputs = {}\n            for (k, v) in outputs.items():\n                tensor_info_outputs[k] = tf.saved_model.utils.build_tensor_info(v)\n            detection_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=tensor_info_inputs, outputs=tensor_info_outputs, method_name=signature_constants.PREDICT_METHOD_NAME)\n            builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING], signature_def_map={signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: detection_signature})\n            builder.save()\n"}
{"label_name":"train","label":0,"method_name":"create_train","method":"\n\ndef create_train(opt, src, global_step, loss, it, global_it, learn, scaled):\n    ' Function for create optimizer to each layer.\\n        Args:\\n            src: name of layer which be optimize.\\n            glogal_step: tenforflow Variable. Need to count train steps.\\n            loss: loss function.\\n            it: number of last iteraion for current layer.\\n            global_it: number of last interation for all layers.\\n            learn: Basic learning rate for current layer.\\n            scaled: method of disable layers.\\n        Output:\\n            New optimizer. '\n\n    def learning_rate(last, src, global_it, learn, scaled):\n        ' Function for create step of changing learning rate.\\n        Args:\\n            last: number of last iteration.\\n            src: mane of layer which be optimize.\\n            global_it: number of last interation for all layers.\\n            learn: Basic learning rate for current layer.\\n            scaled: method of disable layers.\\n        Output:\\n            bound: list of bounders - number of iteration, after which learning rate will change.\\n            values: list of new learnings rates.\\n            var: name of optimize layers'\n        last = int(last)\n        if (scaled is True):\n            values = ([(((0.5 * learn) \/ last) * (1 + np.cos(((np.pi * i) \/ last)))) for i in range(2, (last + 1))] + [0.01])\n        else:\n            values = ([((0.5 * learn) * (1 + np.cos(((np.pi * i) \/ last)))) for i in range(2, (last + 1))] + [0.01])\n        bound = (list(np.linspace(0, last, len(range(2, (last + 1))), dtype=np.int32)) + [global_it])\n        var = [i for i in tf.trainable_variables() if ((src in i.name) or ('dense' in i.name))]\n        return (list(np.int32(bound)), list(np.float32(values)), var)\n    (b, val, var) = learning_rate(it, src, global_it, learn, scaled)\n    learning_rate = tf.train.piecewise_constant(global_step, b, val)\n    return opt(learning_rate, 0.9, use_nesterov=True).minimize(loss, global_step, var)\n"}
{"label_name":"save","label":1,"method_name":"save_obj_detect_image","method":"\n\ndef save_obj_detect_image(id_, project, annos, dset=None):\n    dset = (get_random_dset() if (dset is None) else dset)\n    entry = data.make_obj_detect_entry(annos)\n    fold = data.load_fold(project)\n    if (id_ in fold[cfg.UNLABELED]):\n        data.move_unlabeled_to_labeled(fold, dset, id_, entry)\n    else:\n        for dset in [cfg.VAL, cfg.TRAIN]:\n            if (id_ in fold[dset]):\n                fold[dset][id_] = entry\n                break\n"}
{"label_name":"save","label":1,"method_name":"save_smi","method":"\n\ndef save_smi(name, smiles):\n    if (not os.path.exists('epoch_data')):\n        os.makedirs('epoch_data')\n    smi_file = os.path.join('epoch_data', '{}.smi'.format(name))\n    with open(smi_file, 'w') as afile:\n        afile.write('\\n'.join(smiles))\n    return\n"}
{"label_name":"train","label":0,"method_name":"training","method":"\n\ndef training(local_rank, config, logger=None):\n    if (not getattr(config, 'use_fp16', True)):\n        raise RuntimeError('This training script uses by default fp16 AMP')\n    torch.backends.cudnn.benchmark = True\n    set_seed((config.seed + local_rank))\n    (train_loader, val_loader, train_eval_loader) = (config.train_loader, config.val_loader, config.train_eval_loader)\n    (model, optimizer, criterion) = initialize(config)\n    if (not hasattr(config, 'prepare_batch')):\n        config.prepare_batch = _prepare_batch\n    trainer = create_trainer(model, optimizer, criterion, train_loader.sampler, config, logger)\n    if getattr(config, 'benchmark_dataflow', False):\n        benchmark_dataflow_num_iters = getattr(config, 'benchmark_dataflow_num_iters', 1000)\n        DataflowBenchmark(benchmark_dataflow_num_iters, prepare_batch=config.prepare_batch).attach(trainer, train_loader)\n    val_metrics = {'Accuracy': Accuracy(), 'Top-5 Accuracy': TopKCategoricalAccuracy(k=5)}\n    if (hasattr(config, 'val_metrics') and isinstance(config.val_metrics, dict)):\n        val_metrics.update(config.val_metrics)\n    (evaluator, train_evaluator) = create_evaluators(model, val_metrics, config)\n\n    @trainer.on((Events.EPOCH_COMPLETED(every=getattr(config, 'val_interval', 1)) | Events.COMPLETED))\n    def run_validation():\n        epoch = trainer.state.epoch\n        state = train_evaluator.run(train_eval_loader)\n        log_metrics(logger, epoch, state.times['COMPLETED'], 'Train', state.metrics)\n        state = evaluator.run(val_loader)\n        log_metrics(logger, epoch, state.times['COMPLETED'], 'Test', state.metrics)\n    if getattr(config, 'start_by_validation', False):\n        trainer.add_event_handler(Events.STARTED, run_validation)\n    score_metric_name = 'Accuracy'\n    if hasattr(config, 'es_patience'):\n        common.add_early_stopping_by_val_score(config.es_patience, evaluator, trainer, metric_name=score_metric_name)\n    common.save_best_model_by_val_score(config.output_path.as_posix(), evaluator, model=model, metric_name=score_metric_name, n_saved=3, trainer=trainer, tag='val')\n    if (idist.get_rank() == 0):\n        tb_logger = common.setup_tb_logging(config.output_path.as_posix(), trainer, optimizer, evaluators={'training': train_evaluator, 'validation': evaluator})\n        exp_tracking_logger = exp_tracking.setup_logging(trainer, optimizer, evaluators={'training': train_evaluator, 'validation': evaluator})\n        tb_logger.attach(evaluator, log_handler=predictions_gt_images_handler(img_denormalize_fn=config.img_denormalize, n_images=15, another_engine=trainer, prefix_tag='validation'), event_name=Events.ITERATION_COMPLETED(once=(len(val_loader) \/\/ 2)))\n        tb_logger.attach(train_evaluator, log_handler=predictions_gt_images_handler(img_denormalize_fn=config.img_denormalize, n_images=15, another_engine=trainer, prefix_tag='training'), event_name=Events.ITERATION_COMPLETED(once=(len(train_eval_loader) \/\/ 2)))\n    trainer.run(train_loader, max_epochs=config.num_epochs)\n    if (idist.get_rank() == 0):\n        tb_logger.close()\n        exp_tracking_logger.close()\n"}
{"label_name":"predict","label":4,"method_name":"_predictor","method":"\n\ndef _predictor(args):\n    'Make a feature random noise.\\n\\n    Parameters\\n    ----------\\n    args : list\\n        A list of arguments:\\n\\n        index : int\\n            The index of the feature to be shuffled.\\n        train_features : array\\n            The original training data matrix.\\n        test_features : array\\n            The original test data matrix.\\n\\n    Returns\\n    -------\\n    f : int\\n        Feature index.\\n    result : float or list\\n        Importance and optionally metadata as following elements in a list.\\n    '\n    f = args[0]\n    train_features = args[1]\n    test_features = args[2]\n    test_targets = args[4]\n    predict = args[5]\n    transform = args[6]\n    test_predict = args[7]\n    (train, test) = transform((f, train_features, test_features))\n    result = test_predict(predict, test, test_targets)\n    if isinstance(result, list):\n        error = result[0]\n        meta = result[1:]\n        return (f, error, meta)\n    return (f, result)\n"}
{"label_name":"save","label":1,"method_name":"save_fold","method":"\n\ndef save_fold(fold):\n    fpath = get_fpath(fold['name'], cfg.FOLD_FNAME)\n    return utils.files.save_json(fpath, fold)\n"}
{"label_name":"train","label":0,"method_name":"load_yt_train_all","method":"\n\ndef load_yt_train_all(dir, t, o, l):\n    name = (((('yt_train_all_' + str(t)) + str(o)) + str(l)) + '.txt')\n    with open(os.path.join(dir, name)) as fp:\n        dataSet = []\n        for line in fp:\n            line = line.strip('\\n')\n            features = line.split(',')\n            features = [float(x) for x in features]\n            dataSet.append(features)\n    yt_train_all = np.array(dataSet)\n    return yt_train_all\n"}
{"label_name":"predict","label":4,"method_name":"predictwords","method":"\n\ndef predictwords(rt, re, rm, summary, encoder, decoder, lang, embedding_size, encoder_style, beam_size):\n    'The function will predict the sentecnes given boxscore.\\n\\n    Encode the given box score, decode it to sentences, and then\\n    return the prediction and attention matrix.\\n\\n    While decoding, beam search will be conducted with default beam_size as 1.\\n\\n    '\n    batch_length = rt.size()[0]\n    input_length = rt.size()[1]\n    target_length = 1000\n    encoder_outputs = Variable(torch.zeros(batch_length, input_length, embedding_size))\n    encoder_outputs = (encoder_outputs.cuda() if use_cuda else encoder_outputs)\n    if (encoder_style == 'BiLSTM'):\n        init_hidden = encoder.initHidden(batch_length)\n        (encoder_hidden, encoder_hiddens) = encoder(rt, re, rm, init_hidden)\n        for ei in range(input_length):\n            encoder_outputs[:, ei] = encoder_hiddens[:, ei]\n    else:\n        encoder_hidden = encoder.initHidden(batch_length)\n        (out, encoder_hidden) = encoder(rt, re, rm, encoder_hidden)\n        encoder_outputs = out.permute(1, 0, 2)\n        encoder_hidden = out[(- 1), :]\n    decoder_attentions = torch.zeros(target_length, input_length)\n    beams = [[0, [SOS_TOKEN], encoder_hidden, decoder_attentions]]\n    for di in range(target_length):\n        q = PriorityQueue()\n        for beam in beams:\n            (prob, route, decoder_hidden, atten) = beam\n            destination = (len(route) - 1)\n            decoder_input = route[(- 1)]\n            if (decoder_input == EOS_TOKEN):\n                q.push(beam, prob)\n                continue\n            decoder_input = Variable(torch.LongTensor([decoder_input]))\n            decoder_input = (decoder_input.cuda() if use_cuda else decoder_input)\n            (decoder_output, decoder_hidden, decoder_context, decoder_attention, pgen) = decoder(decoder_input, decoder_hidden, encoder_outputs)\n            if decoder.copy:\n                decoder_output = decoder_output.exp()\n                prob = Variable(torch.zeros(decoder_output.shape), requires_grad=False)\n                prob = (prob.cuda() if use_cuda else prob)\n                for i in range(decoder_attention.shape[2]):\n                    prob[:, rm[:, i]] += ((1 - pgen) * decoder_attention[:, 0, i])\n                decoder_output_new = (decoder_output + prob)\n                decoder_output_new = decoder_output_new.log()\n            else:\n                decoder_output_new = decoder_output\n            atten[destination, :decoder_attention.shape[2]] = decoder_attention.data[0, 0, :]\n            (topv, topi) = decoder_output.data.topk(beam_size)\n            for i in range(beam_size):\n                p = topv[0][i]\n                idp = topi[0][i]\n                new_beam = [(prob + p), (route + [idp]), decoder_hidden, atten]\n                q.push(new_beam, new_beam[0])\n        beams = [q.pop() for i in range(beam_size)]\n        if (beams[0][1][(- 1)] == 1):\n            break\n    decoded_words = [lang.index2word[w] for w in beams[0][1][1:]]\n    decoder_attentions = beams[0][3]\n    return (decoded_words, decoder_attentions[:len(decoded_words)])\n"}
{"label_name":"train","label":0,"method_name":"train_classifier","method":"\n\ndef train_classifier(classifier, data, labels, msg):\n    print('  {}'.format(msg), end=': ')\n    _st = time()\n    classifier.fit(X=data, y=labels)\n    dtype_names = ['{}{}'.format(['-', '+'][val], msg) for val in np.unique(labels)]\n    dtype_formats = ([float] * np.unique(labels).size)\n    model_dtype_dict = dict(names=dtype_names, formats=dtype_formats)\n    print_elapsed(_st)\n    return (classifier, model_dtype_dict)\n"}
{"label_name":"process","label":2,"method_name":"vgg_preprocessing_numpy","method":"\n\ndef vgg_preprocessing_numpy(img, height, width):\n    'Performs VGG-style preprocessing of an image.\\n\\n    The image is resized (aspect-preserving) to the desired size and then\\n    centered by `IMG_MEAN_IMAGENET`.\\n\\n    Args:\\n        img: an image\\n        height: desired height after preprocessing\\n        width: desired width after preprocessing\\n\\n    Returns:\\n        the preprocessed images, in float32 format\\n    '\n    if etai.is_gray(img):\n        img = etai.gray_to_rgb(img)\n    elif etai.has_alpha(img):\n        img = img[:, :, :3]\n    if (img.shape[0] < img.shape[1]):\n        img = etai.resize(img, height=256)\n    else:\n        img = etai.resize(img, width=256)\n    img = etai.central_crop(img, shape=(height, width))\n    img = (np.asarray(img, dtype=np.float32) - IMG_MEAN_IMAGENET)\n    return img\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input_variable, target_variable, encoder, encoder_optimizer, criterion):\n    encoder_optimizer.zero_grad()\n    loss = 0\n    encoder_outputs = encoder(input_variable)\n    loss = criterion(encoder_outputs, target_variable)\n    loss.backward()\n    encoder_optimizer.step()\n    return loss.data[0]\n"}
{"label_name":"predict","label":4,"method_name":"_is_valid_predict_signature","method":"\n\ndef _is_valid_predict_signature(signature_def):\n    \"Determine whether the argument is a servable 'predict' SignatureDef.\"\n    if (signature_def.method_name != signature_constants.PREDICT_METHOD_NAME):\n        return False\n    if (not signature_def.inputs.keys()):\n        return False\n    if (not signature_def.outputs.keys()):\n        return False\n    return True\n"}
{"label_name":"process","label":2,"method_name":"preprocess_french","method":"\n\ndef preprocess_french(trans, fr_nlp, remove_brackets_content=True):\n    ' Takes a list of sentences in french and preprocesses them.'\n    if remove_brackets_content:\n        trans = pangloss.remove_content_in_brackets(trans, '[]')\n    trans = fr_nlp(' '.join(trans.split()[:]))\n    trans = ' '.join([token.lower_ for token in trans if (not token.is_punct)])\n    return trans\n"}
{"label_name":"train","label":0,"method_name":"run_training","method":"\n\ndef run_training():\n    'Train MNIST for a number of steps.'\n    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n    with tf.Graph().as_default():\n        (images_placeholder, labels_placeholder) = placeholder_inputs(FLAGS.batch_size)\n        logits = mnist.inference(images_placeholder, FLAGS.hidden1, FLAGS.hidden2)\n        loss = mnist.loss(logits, labels_placeholder)\n        train_op = mnist.training(loss, FLAGS.learning_rate)\n        eval_correct = mnist.evaluation(logits, labels_placeholder)\n        summary = tf.summary.merge_all()\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n        sess = tf.Session()\n        summary_writer = tf.train.SummaryWriter(FLAGS.log_dir, sess.graph)\n        sess.run(init)\n        for step in xrange(FLAGS.max_steps):\n            start_time = time.time()\n            feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n            (_, loss_value) = sess.run([train_op, loss], feed_dict=feed_dict)\n            duration = (time.time() - start_time)\n            if ((step % 100) == 0):\n                print(('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration)))\n                summary_str = sess.run(summary, feed_dict=feed_dict)\n                summary_writer.add_summary(summary_str, step)\n                summary_writer.flush()\n            if ((((step + 1) % 1000) == 0) or ((step + 1) == FLAGS.max_steps)):\n                checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n                saver.save(sess, checkpoint_file, global_step=step)\n                print('Training Data Eval:')\n                do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_sets.train)\n                print('Validation Data Eval:')\n                do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_sets.validation)\n                print('Test Data Eval:')\n                do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_sets.test)\n"}
{"label_name":"save","label":1,"method_name":"save_visualization","method":"\n\ndef save_visualization(trace, graph_output):\n    '\\n    (DEPRECATED) Take a trace generated by poutine.trace with `graph_type=\\'dense\\'`\\n    and render the graph with the output saved to file.\\n\\n    - non-reparameterized stochastic nodes are salmon\\n    - reparameterized stochastic nodes are half salmon, half grey\\n    - observation nodes are green\\n\\n    :param pyro.poutine.Trace trace: a trace to be visualized\\n    :param graph_output: the graph will be saved to graph_output.pdf\\n    :type graph_output: str\\n\\n\\n    Example:\\n\\n    trace = pyro.poutine.trace(model, graph_type=\"dense\").get_trace()\\n    save_visualization(trace, \\'output\\')\\n    '\n    warnings.warn('`save_visualization` function is deprecated and will be removed in a future version.')\n    import graphviz\n    g = graphviz.Digraph()\n    for (label, node) in trace.nodes.items():\n        if site_is_subsample(node):\n            continue\n        shape = 'ellipse'\n        if ((label in trace.stochastic_nodes) and (label not in trace.reparameterized_nodes)):\n            fillcolor = 'salmon'\n        elif (label in trace.reparameterized_nodes):\n            fillcolor = 'lightgrey;.5:salmon'\n        elif (label in trace.observation_nodes):\n            fillcolor = 'darkolivegreen3'\n        else:\n            continue\n        g.node(label, label=label, shape=shape, style='filled', fillcolor=fillcolor)\n    for (label1, label2) in trace.edges:\n        if site_is_subsample(trace.nodes[label1]):\n            continue\n        if site_is_subsample(trace.nodes[label2]):\n            continue\n        g.edge(label1, label2)\n    g.render(graph_output, view=False, cleanup=True)\n"}
{"label_name":"process","label":2,"method_name":"MockUndecoratedPreprocessor","method":"\n\ndef MockUndecoratedPreprocessor(text: str) -> str:\n    'A mock preprocessor which is not decorated with @clgen_preprocessor.'\n    return text\n"}
{"label_name":"predict","label":4,"method_name":"predict_dygraph_jit","method":"\n\ndef predict_dygraph_jit(args, data):\n    with fluid.dygraph.guard(args.place):\n        model = fluid.dygraph.jit.load(args.model_save_prefix)\n        model.eval()\n        pred_res = model(data)\n        return pred_res.numpy()\n"}
{"label_name":"save","label":1,"method_name":"save_results","method":"\n\ndef save_results(sess, folder, name, results_rows=None):\n    if (results_rows is not None):\n        df = pd.DataFrame(results_rows)\n        df.to_csv('{}_results.csv'.format(folder), index=False)\n    model_saver = tf.train.Saver()\n    ckpt_dir = os.path.join(params['CHK_PATH'], folder)\n    if (not os.path.exists(ckpt_dir)):\n        os.makedirs(ckpt_dir)\n    ckpt_file = os.path.join(ckpt_dir, '{}.ckpt'.format(name))\n    path = model_saver.save(sess, ckpt_file)\n    print('Model saved at {}'.format(path))\n    return\n"}
{"label_name":"train","label":0,"method_name":"prepare_pretrain_npz_dataset","method":"\n\ndef prepare_pretrain_npz_dataset(filename, allow_pickle=False):\n    'Create dataset based on the numpy npz file'\n    if isinstance(filename, (list, tuple)):\n        assert (len(filename) == 1), 'When .npy\/.npz data file is loaded, len(filename) must be 1. Received len(filename)={}.'.format(len(filename))\n        filename = filename[0]\n    logging.debug('start to load file %s ...', filename)\n    return NumpyDataset(filename, allow_pickle=allow_pickle)\n"}
{"label_name":"train","label":0,"method_name":"nn_train","method":"\n\ndef nn_train() -> None:\n    ((x_train, y_train), (x_test, y_test)) = mnist.load_data()\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.float32)\n    x_test = x_test.astype(np.float32)\n    y_test = y_test.astype(np.float32)\n    train_size = x_train.shape[0]\n    test_size = x_test.shape[0]\n    num_features = 784\n    num_classes = 10\n    y_train = to_categorical(y_train, num_classes=num_classes)\n    y_test = to_categorical(y_test, num_classes=num_classes)\n    x_train = x_train.reshape(train_size, num_features)\n    x_test = x_test.reshape(test_size, num_features)\n    epochs = 10\n    batch_size = 256\n    model = create_model()\n    model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n    model.save_weights(MODEL_PATH)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input, categorization_tensor, num):\n    if (((num - 1) % batch_size) == 0):\n        character_embedding.zero_grad()\n        sentence_model.zero_grad()\n        word_in_vocab_model.zero_grad()\n    hidden = sentence_model.initHidden()\n    tensor_list = to_tensor(input)\n    for tensor in tensor_list:\n        (output, hidden) = sentence_model(tensor.view(1, 1, (- 1)), hidden)\n    loss = criterion(output, categorization_tensor)\n    loss.backward()\n    if ((num % batch_size) == 0):\n        for p in character_embedding.parameters():\n            p.data.add_((- learning_rate), p.grad.data)\n        for p in sentence_model.parameters():\n            p.data.add_((- learning_rate), p.grad.data)\n        for p in word_in_vocab_model.parameters():\n            p.data.add_((- learning_rate), p.grad.data)\n    return (output, loss.data[0])\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train():\n    rl_agent = LearningAgent()\n    ref_agent = ReferenceAgent()\n    results = []\n    for i in range(10000):\n        rl_agent.begin_episode()\n        result = simulate_game(rl_agent, ref_agent)\n        rl_agent.learn(result)\n        results.append(result)\n        print(('Score %d\/10' % sum(results[(- 10):])))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(net, dataset, epochs, batch_size, print_every=10, show_every=100, figsize=(5, 5)):\n    saver = tf.train.Saver()\n    sample_z = np.random.uniform((- 1), 1, size=(72, z_size))\n    (samples, losses) = ([], [])\n    steps = 0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for e in range(epochs):\n            for (x, y) in dataset.batches(batch_size):\n                steps += 1\n                batch_z = np.random.uniform((- 1), 1, size=(batch_size, z_size))\n                _ = sess.run(net.d_opt, feed_dict={net.input_real: x, net.input_z: batch_z})\n                _ = sess.run(net.g_opt, feed_dict={net.input_z: batch_z, net.input_real: x})\n                if ((steps % print_every) == 0):\n                    train_loss_d = net.d_loss.eval({net.input_z: batch_z, net.input_real: x})\n                    train_loss_g = net.g_loss.eval({net.input_z: batch_z})\n                    print('Epoch {}\/{}...'.format((e + 1), epochs), 'Discriminator Loss: {:.4f}...'.format(train_loss_d), 'Generator Loss: {:.4f}'.format(train_loss_g))\n                    losses.append((train_loss_d, train_loss_g))\n                if ((steps % show_every) == 0):\n                    gen_samples = sess.run(generator(net.input_z, 3, reuse=True, training=False), feed_dict={net.input_z: sample_z})\n                    samples.append(gen_samples)\n                    _ = view_samples((- 1), samples, 6, 12, figsize=figsize)\n                    plt.show()\n        saver.save(sess, '.\/checkpoints\/generator.ckpt')\n    with open('samples.pkl', 'wb') as f:\n        pkl.dump(samples, f)\n    return (losses, samples)\n"}
{"label_name":"train","label":0,"method_name":"_train_model","method":"\n\ndef _train_model():\n    data_info = load_organized_data_info(IMGS_DIM_3D[1])\n    dir_tr = data_info['dir_tr']\n    dir_val = data_info['dir_val']\n    (gen_tr, gen_val) = train_val_dirs_generators(BATCH_SIZE, dir_tr, dir_val)\n    model = _cnn(IMGS_DIM_3D)\n    model.fit_generator(generator=gen_tr, nb_epoch=MAX_EPOCHS, samples_per_epoch=data_info['num_tr'], validation_data=gen_val, nb_val_samples=data_info['num_val'], callbacks=[ModelCheckpoint(CNN_MODEL_FILE, save_best_only=True)], verbose=2)\n"}
{"label_name":"save","label":1,"method_name":"save_split","method":"\n\ndef save_split(data: dict, filename: str):\n    converted = {}\n    for (eq_class, samples) in data.items():\n        if (len(samples) > 0):\n            converted[eq_class] = dict(Original=samples[0], Noise=samples[1:])\n    save_result_as_gzipped_json(filename, converted)\n"}
{"label_name":"train","label":0,"method_name":"create_supervised_tbptt_trainer","method":"\n\ndef create_supervised_tbptt_trainer(model: nn.Module, optimizer: Optimizer, loss_fn: nn.Module, tbtt_step: int, dim: int=0, device: Optional[str]=None, non_blocking: bool=False, prepare_batch: Callable=_prepare_batch) -> Engine:\n    \"Create a trainer for truncated backprop through time supervised models.\\n\\n    Training recurrent model on long sequences is computationally intensive as\\n    it requires to process the whole sequence before getting a gradient.\\n    However, when the training loss is computed over many outputs\\n    (`X to many <https:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/>`_),\\n    there is an opportunity to compute a gradient over a subsequence. This is\\n    known as\\n    `truncated backpropagation through time <https:\/\/machinelearningmastery.com\/\\n    gentle-introduction-backpropagation-time\/>`_.\\n    This supervised trainer apply gradient optimization step every `tbtt_step`\\n    time steps of the sequence, while backpropagating through the same\\n    `tbtt_step` time steps.\\n\\n    Args:\\n        model: the model to train.\\n        optimizer: the optimizer to use.\\n        loss_fn: the loss function to use.\\n        tbtt_step: the length of time chunks (last one may be smaller).\\n        dim: axis representing the time dimension.\\n        device: device type specification (default: None).\\n            Applies to batches.\\n        non_blocking: if True and this copy is between CPU and GPU,\\n            the copy may occur asynchronously with respect to the host. For other cases,\\n            this argument has no effect.\\n        prepare_batch: function that receives `batch`, `device`,\\n            `non_blocking` and outputs tuple of tensors `(batch_x, batch_y)`.\\n\\n    Returns:\\n        a trainer engine with supervised update function.\\n\\n    .. warning::\\n\\n        The internal use of `device` has changed.\\n        `device` will now *only* be used to move the input data to the correct device.\\n        The `model` should be moved by the user before creating an optimizer.\\n\\n        For more information see:\\n\\n        * `PyTorch Documentation <https:\/\/pytorch.org\/docs\/stable\/optim.html#constructing-it>`_\\n        * `PyTorch's Explanation <https:\/\/github.com\/pytorch\/pytorch\/issues\/7844#issuecomment-503713840>`_\\n    \"\n\n    def _update(engine: Engine, batch: Sequence[torch.Tensor]) -> float:\n        loss_list = []\n        hidden = None\n        (x, y) = batch\n        for batch_t in zip(x.split(tbtt_step, dim=dim), y.split(tbtt_step, dim=dim)):\n            (x_t, y_t) = prepare_batch(batch_t, device=device, non_blocking=non_blocking)\n            engine.fire_event(Tbptt_Events.TIME_ITERATION_STARTED)\n            model.train()\n            optimizer.zero_grad()\n            if (hidden is None):\n                (y_pred_t, hidden) = model(x_t)\n            else:\n                hidden = _detach_hidden(hidden)\n                (y_pred_t, hidden) = model(x_t, hidden)\n            loss_t = loss_fn(y_pred_t, y_t)\n            loss_t.backward()\n            optimizer.step()\n            engine.state.output = loss_t.item()\n            loss_list.append(loss_t.item())\n            engine.fire_event(Tbptt_Events.TIME_ITERATION_COMPLETED)\n        return (sum(loss_list) \/ len(loss_list))\n    engine = Engine(_update)\n    engine.register_events(*Tbptt_Events)\n    return engine\n"}
{"label_name":"save","label":1,"method_name":"save_log","method":"\n\ndef save_log(cfg, trial_num, trial_log):\n    name = cfg.checkpoint_file.format(trial_num)\n    path = os.path.join(os.getcwd(), name)\n    log.info(f'T{trial_num} : Saving log {path}')\n    torch.save(trial_log, path)\n"}
{"label_name":"save","label":1,"method_name":"save","method":"\n\ndef save(operations, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(operations, f)\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n\ndef preprocess_image(image, height, width, is_training=False, bbox=None, fast_mode=True, add_image_summaries=True):\n    'Pre-process one image for training or evaluation.\\n\\n  Args:\\n    image: 3-D Tensor [height, width, channels] with the image. If dtype is\\n      tf.float32 then the range should be [0, 1], otherwise it would converted\\n      to tf.float32 assuming that the range is [0, MAX], where MAX is largest\\n      positive representable number for int(8\/16\/32) data type (see\\n      `tf.image.convert_image_dtype` for details).\\n    height: integer, image expected height.\\n    width: integer, image expected width.\\n    is_training: Boolean. If true it would transform an image for train,\\n      otherwise it would transform it for evaluation.\\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\\n      where each coordinate is [0, 1) and the coordinates are arranged as\\n      [ymin, xmin, ymax, xmax].\\n    fast_mode: Optional boolean, if True avoids slower transformations.\\n    add_image_summaries: Enable image summaries.\\n\\n  Returns:\\n    3-D float Tensor containing an appropriately scaled image\\n\\n  Raises:\\n    ValueError: if user does not provide bounding box\\n  '\n    if is_training:\n        return preprocess_for_train(image, height, width, bbox, fast_mode, add_image_summaries=add_image_summaries)\n    else:\n        return preprocess_for_eval(image, height, width)\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\n\ndef forward(model: Model[(InT, InT)], X: InT, is_train: bool) -> Tuple[(InT, Callable)]:\n    (N, mu, var) = _get_moments(model.ops, X)\n    Xhat = ((X - mu) * (var ** ((- 1.0) \/ 2.0)))\n    (Y, backprop_rescale) = _begin_update_scale_shift(model, Xhat)\n\n    def backprop(dY: InT) -> InT:\n        dY = backprop_rescale(dY)\n        (dist, sum_dy, sum_dy_dist) = _get_d_moments(model.ops, dY, X, mu)\n        d_xhat = (((N * dY) - sum_dy) - ((dist * (var ** (- 1.0))) * sum_dy_dist))\n        d_xhat *= (var ** ((- 1.0) \/ 2))\n        d_xhat \/= N\n        return d_xhat\n    return (Y, backprop)\n"}
{"label_name":"train","label":0,"method_name":"download_wmt16_train","method":"\n\ndef download_wmt16_train(lang_pair: str='en-de', path: str=_BASE_DATASET_PATH) -> Tuple[(List[str], List[str])]:\n    'Download the train dataset used for WMT2016\\n\\n    Parameters\\n    ----------\\n    lang_pair\\n    path\\n\\n    Returns\\n    -------\\n    train_src_paths\\n    train_tgt_paths\\n\\n    '\n    if ((lang_pair == 'en-de') or (lang_pair == 'de-en')):\n        (train_src_paths, train_tgt_paths) = fetch_wmt_parallel_dataset([['europarl', 'v7'], ['commoncrawl', 'wmt13'], ['newscommentary', 'v11']], lang_pair, path=path)\n    else:\n        raise NotImplementedError\n    return (train_src_paths, train_tgt_paths)\n"}
{"label_name":"process","label":2,"method_name":"load_img_and_preprocess","method":"\n\ndef load_img_and_preprocess(path, shape=None):\n    img = image.load_img(path, target_size=shape)\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x\n"}
{"label_name":"process","label":2,"method_name":"preprocess_valid","method":"\n\ndef preprocess_valid(train_path, valid_path):\n    (x, y) = (list(), list())\n    with open(train_path, 'rb') as f:\n        train_data = AttributeDict(pickle.load(f))\n    s_dict = read_data(valid_path)\n    for (s, ro) in s_dict.items():\n        try:\n            _ = train_data.e_to_index[s]\n        except KeyError:\n            continue\n        for (r, objects) in ro.items():\n            try:\n                _ = train_data.r_to_index[r]\n            except KeyError:\n                continue\n            filtered_objects = list()\n            for o in objects:\n                try:\n                    _ = train_data.e_to_index[o]\n                    filtered_objects.append(o)\n                except KeyError:\n                    continue\n            x.append((s, r))\n            y.append(filtered_objects)\n    data = {'x': x, 'y': y}\n    save_file_path = (os.path.splitext(valid_path)[0] + '.pkl')\n    pickle.dump(data, open(save_file_path, 'wb'))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    input_length = input_variable.size()[0]\n    target_length = target_variable.size()[0]\n    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n    encoder_outputs = (encoder_outputs.cuda() if use_cuda else encoder_outputs)\n    loss = 0\n    for ei in range(input_length):\n        (encoder_output, encoder_hidden) = encoder(input_variable[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0][0]\n    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n    decoder_input = (decoder_input.cuda() if use_cuda else decoder_input)\n    decoder_hidden = encoder_hidden\n    use_teacher_forcing = (True if (random.random() < teacher_forcing_ratio) else False)\n    if use_teacher_forcing:\n        for di in range(target_length):\n            (decoder_output, decoder_hidden, decoder_attention) = decoder(decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n            loss += criterion(decoder_output, target_variable[di])\n            decoder_input = target_variable[di]\n    else:\n        for di in range(target_length):\n            (decoder_output, decoder_hidden, decoder_attention) = decoder(decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n            (topv, topi) = decoder_output.data.topk(1)\n            ni = topi[0][0]\n            decoder_input = Variable(torch.LongTensor([[ni]]))\n            decoder_input = (decoder_input.cuda() if use_cuda else decoder_input)\n            loss += criterion(decoder_output, target_variable[di])\n            if (ni == EOS_token):\n                break\n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    return (loss.data[0] \/ target_length)\n"}
{"label_name":"process","label":2,"method_name":"process_image_signals_problem","method":"\n\ndef process_image_signals_problem(image, image_type, height, width, is_test=False):\n    '\\n    Process signal image\\n    :param image: The image to change\\n    :param image_type: Gray Scale, RGB, HSV\\n    :param height: image height\\n    :param width: image width\\n    :param is_test: flag with True if image is in test set\\n    :return: \\n    '\n    image = cv2.imread(image, image_type)\n    image = cv2.resize(image, (height, width))\n    image = cv2.equalizeHist(image)\n    image = cv2.equalizeHist(image)\n    if (not is_test):\n        random_percentage = random.randint(3, 20)\n        to_crop_height = int(((random_percentage * height) \/ 100))\n        to_crop_width = int(((random_percentage * width) \/ 100))\n        image = image[to_crop_height:(height - to_crop_height), to_crop_width:(width - to_crop_width)]\n        image = cv2.copyMakeBorder(image, top=to_crop_height, bottom=to_crop_height, left=to_crop_width, right=to_crop_width, borderType=cv2.BORDER_CONSTANT)\n    image = image.reshape((- 1))\n    return image\n"}
{"label_name":"train","label":0,"method_name":"train_data","method":"\n\ndef train_data():\n    inputs = [[random.uniform((- 1), 1), random.uniform((- 1), 1)] for i in range(100000)]\n    labels = np.asarray([div(x_t) for x_t in inputs])\n    labels = (np.arange(3) == labels[:, None]).astype(np.float32)\n    print(inputs[0])\n    print(div(inputs[0]))\n    print(labels[0])\n    return (inputs, labels)\n"}
{"label_name":"process","label":2,"method_name":"pre_process_dialouge","method":"\n\ndef pre_process_dialouge(raw_dialogue_file_path, pre_processed_dir, movie_dir_name, dialogue_file1, dialogue_file2, train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2, vocab_file1, vocab_file2, max_dialouge_count, max_dialouge_count_train_test, train_percentile, symbol_seq, type_d, max_len, max_test_dev_count):\n    print('Extracting file')\n    start = time.time()\n    delete_all_file_dir(pre_processed_dir, type_d)\n    (dialogue_file1, dialogue_file2) = dialouge_seperator_cornell_movie(raw_dialogue_file_path, dialogue_file1, dialogue_file2, symbol_seq, max_dialouge_count, max_len)\n    (train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2) = train_test_split(dialogue_file1, dialogue_file2, max_dialouge_count_train_test, train_percentile, train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2, max_test_dev_count)\n    end = time.time()\n    time_lib.elapsed_time(start, end)\n    return (train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2)\n"}
{"label_name":"process","label":2,"method_name":"preprocess_dataset","method":"\n\ndef preprocess_dataset(data: TrainingData, train_size=0.9, test_size=0.1, vocab=None, class_to_i=None, i_to_class=None, create_runs=False, full_question=False):\n    '\\n    This function does primarily text preprocessing on the dataset. It will return x_train and x_test as a list of\\n    examples where each word is a tokenized word list (not padded). y_train and y_test is a list of indices coresponding\\n    to the class labels that are associated with i_to_class and class_to_i. vocab consists of any word which occurred\\n    in the training set.\\n    \\n    TODO: Implement an option for maximum vocab size which takes the most frequently occurring words only.\\n    \\n    :param data: \\n    :param train_size: \\n    :param vocab: \\n    :param class_to_i: \\n    :param i_to_class: \\n    :param create_runs: \\n    :param full_question: \\n    :return:\\n    '\n    if (full_question and create_runs):\n        raise ValueError('The options create_runs={} and full_question={} are not compatible'.format(create_runs, full_question))\n    if ((train_size + test_size) > 1):\n        raise ValueError(f'Train + test must sum to 1 or less: train={train_size} test={test_size} sum={(train_size + test_size)}')\n    classes = set(data[1])\n    if ((class_to_i is None) or (i_to_class is None)):\n        class_to_i = {}\n        i_to_class = []\n        for (i, ans_class) in enumerate(classes):\n            class_to_i[ans_class] = i\n            i_to_class.append(ans_class)\n    x_train = []\n    y_train = []\n    x_test = []\n    y_test = []\n    if (vocab is None):\n        vocab = set()\n    question_runs_with_answer = list(zip(data[0], data[1]))\n    if (train_size != 1):\n        (train, test) = train_test_split(question_runs_with_answer, train_size=train_size, test_size=test_size)\n    else:\n        train = question_runs_with_answer\n        test = []\n    for (q, ans) in train:\n        q_text = []\n        for sentence in q:\n            t_question = tokenize_question(sentence)\n            if (create_runs or full_question):\n                q_text.extend(t_question)\n            else:\n                q_text = t_question\n            if (len(t_question) > 0):\n                for w in t_question:\n                    vocab.add(w)\n                if create_runs:\n                    x_train.append(list(q_text))\n                elif (not full_question):\n                    x_train.append(q_text)\n                if (not full_question):\n                    y_train.append(class_to_i[ans])\n        if full_question:\n            x_train.append(q_text)\n            y_train.append(class_to_i[ans])\n    for (q, ans) in test:\n        q_text = []\n        for sentence in q:\n            t_question = tokenize_question(sentence)\n            if (create_runs or full_question):\n                q_text.extend(t_question)\n                if (not full_question):\n                    x_test.append(list(q_text))\n            else:\n                q_text = t_question\n                x_test.append(q_text)\n            if (not full_question):\n                y_test.append(class_to_i[ans])\n        if full_question:\n            x_test.append(q_text)\n            y_test.append(class_to_i[ans])\n    return (x_train, y_train, x_test, y_test, vocab, class_to_i, i_to_class)\n"}
{"label_name":"predict","label":4,"method_name":"combine_prediction_metadata_batches","method":"\n\ndef combine_prediction_metadata_batches(metadata_list):\n    'Combines a list of dicts with the same keys and lists as values into a single dict with concatenated lists\\n        for each corresponding key\\n\\n    Args:\\n        metadata_list (list): list of dicts with matching keys and lists for values\\n\\n    Returns:\\n        dict: combined single dict\\n    '\n    combined_metadata = {}\n    for metadata_batch in metadata_list:\n        for meta_el in metadata_batch:\n            if (meta_el not in combined_metadata):\n                combined_metadata[meta_el] = []\n            combined_metadata[meta_el] += metadata_batch[meta_el]\n    return combined_metadata\n"}
{"label_name":"process","label":2,"method_name":"_preprocess_conv2d_input","method":"\n\ndef _preprocess_conv2d_input(x, data_format):\n    'Transpose and cast the input before the conv2d.\\n\\n    # Arguments\\n        x: input tensor.\\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\\n\\n    # Returns\\n        A tensor.\\n    '\n    if (K.dtype(x) == 'float64'):\n        x = tf.cast(x, 'float32')\n    if (data_format == 'channels_first'):\n        x = tf.transpose(x, (0, 2, 3, 1))\n    return x\n"}
{"label_name":"process","label":2,"method_name":"preprocess_egomotion","method":"\n\ndef preprocess_egomotion(locs, thetas):\n    with tf.name_scope('pre_ego'):\n        pre_ego = tf.concat([locs, tf.sin(thetas), tf.cos(thetas)], 2)\n        sh = pre_ego.get_shape().as_list()\n        pre_ego = tf.reshape(pre_ego, [(- 1), sh[(- 1)]])\n    return pre_ego\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(X, y):\n    w = np.mat(np.zeros((X.shape[1], 1)))\n    w = batch_adagrad_desc(X, w, y, batch_size=20, epoch=150, alpha=0.3, lamb=0.08)\n    return w\n"}
{"label_name":"predict","label":4,"method_name":"predict_wrapper","method":"\n\ndef predict_wrapper(X, gpr):\n    'Predict that can handle 1-D input'\n    X = np.expand_dims(X, axis=0)\n    return gpr.predict(X, return_std=True)\n"}
{"label_name":"process","label":2,"method_name":"_process_file_shard","method":"\n\ndef _process_file_shard(tce_table, file_name):\n    'Processes a single file shard.\\n\\n  Args:\\n    tce_table: A Pandas DateFrame containing the TCEs in the shard.\\n    file_name: The output TFRecord file.\\n  '\n    process_name = multiprocessing.current_process().name\n    shard_name = os.path.basename(file_name)\n    shard_size = len(tce_table)\n    tf.logging.info('%s: Processing %d items in shard %s', process_name, shard_size, shard_name)\n    with tf.python_io.TFRecordWriter(file_name) as writer:\n        num_processed = 0\n        for (_, tce) in tce_table.iterrows():\n            example = _process_tce(tce)\n            if (example is not None):\n                writer.write(example.SerializeToString())\n            num_processed += 1\n            if (not (num_processed % 10)):\n                tf.logging.info('%s: Processed %d\/%d items in shard %s', process_name, num_processed, shard_size, shard_name)\n    tf.logging.info('%s: Wrote %d items in shard %s', process_name, shard_size, shard_name)\n"}
{"label_name":"save","label":1,"method_name":"save_jsongz","method":"\n\ndef save_jsongz(filename, data):\n    'Save a dictionary to a gzipped json file.'\n    with gzip.GzipFile(filename, 'wb') as fp:\n        json_str = (json.dumps(data) + '\\n')\n        json_bytes = json_str.encode('utf-8')\n        fp.write(json_bytes)\n"}
{"label_name":"train","label":0,"method_name":"train_step","method":"\n\ndef train_step(model, loss_fn, optimizer_fn, metric, image, label):\n    'Perform one training step for the model.\\n\\n  Args:\\n    model: Keras model to train.\\n    loss_fn: Loss function to use.\\n    optimizer_fn: Optimizer function to use.\\n    metric: keras.metric to use.\\n    image: Tensor of training images of shape [batch_size, 28, 28, 1].\\n    label: Tensor of class labels of shape [batch_size].\\n  '\n    with tf.GradientTape() as tape:\n        preds = model(image)\n        label_onehot = tf.one_hot(label, 10)\n        loss_ = loss_fn(label_onehot, preds)\n    grads = tape.gradient(loss_, model.trainable_variables)\n    optimizer_fn.apply_gradients(zip(grads, model.trainable_variables))\n    metric(loss_)\n"}
{"label_name":"save","label":1,"method_name":"save_module","method":"\n\ndef save_module(module_path, graph, lib, params, cross=None):\n    '\\n    Create a tarball containing the generated TVM graph,\\n    exported library and parameters\\n\\n    Parameters\\n    ----------\\n    module_path : str\\n        path to the target tar.gz file to be created,\\n        including the file name\\n    graph : str\\n        A JSON-serialized TVM execution graph.\\n    lib : tvm.module.Module\\n        A TVM module containing the compiled functions.\\n    params : dict\\n        The parameters (weights) for the TVM module.\\n    cross : str or callable object, optional\\n        Function that performs the actual compilation\\n\\n    '\n    lib_name = 'mod.so'\n    graph_name = 'mod.json'\n    param_name = 'mod.params'\n    temp = utils.tempdir()\n    path_lib = temp.relpath(lib_name)\n    if (not cross):\n        logger.debug('exporting library to %s', path_lib)\n        lib.export_library(path_lib)\n    else:\n        logger.debug('exporting library to %s , using cross compiler %s', path_lib, cross)\n        lib.export_library(path_lib, cc.cross_compiler(cross))\n    with open(temp.relpath(graph_name), 'w') as graph_file:\n        logger.debug('writing graph to file to %s', graph_file.name)\n        graph_file.write(graph)\n    with open(temp.relpath(param_name), 'wb') as params_file:\n        logger.debug('writing params to file to %s', params_file.name)\n        params_file.write(runtime.save_param_dict(params))\n    logger.debug('saving module as tar file to %s', module_path)\n    with tarfile.open(module_path, 'w') as tar:\n        tar.add(path_lib, lib_name)\n        tar.add(temp.relpath(graph_name), graph_name)\n        tar.add(temp.relpath(param_name), param_name)\n"}
{"label_name":"train","label":0,"method_name":"_translate_train_sizes","method":"\n\ndef _translate_train_sizes(train_sizes, n_max_training_samples):\n    \"Determine absolute sizes of training subsets and validate 'train_sizes'.\\n\\n    Examples:\\n        _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]\\n        _translate_train_sizes([5, 10], 10) -> [5, 10]\\n\\n    Parameters\\n    ----------\\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\\n        Numbers of training examples that will be used to generate the\\n        learning curve. If the dtype is float, it is regarded as a\\n        fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].\\n\\n    n_max_training_samples : int\\n        Maximum number of training samples (upper bound of 'train_sizes').\\n\\n    Returns\\n    -------\\n    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\\n        Numbers of training examples that will be used to generate the\\n        learning curve. Note that the number of ticks might be less\\n        than n_ticks because duplicate entries will be removed.\\n    \"\n    train_sizes_abs = np.asarray(train_sizes)\n    n_ticks = train_sizes_abs.shape[0]\n    n_min_required_samples = np.min(train_sizes_abs)\n    n_max_required_samples = np.max(train_sizes_abs)\n    if np.issubdtype(train_sizes_abs.dtype, np.floating):\n        if ((n_min_required_samples <= 0.0) or (n_max_required_samples > 1.0)):\n            raise ValueError(('train_sizes has been interpreted as fractions of the maximum number of training samples and must be within (0, 1], but is within [%f, %f].' % (n_min_required_samples, n_max_required_samples)))\n        train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(dtype=np.int, copy=False)\n        train_sizes_abs = np.clip(train_sizes_abs, 1, n_max_training_samples)\n    elif ((n_min_required_samples <= 0) or (n_max_required_samples > n_max_training_samples)):\n        raise ValueError(('train_sizes has been interpreted as absolute numbers of training samples and must be within (0, %d], but is within [%d, %d].' % (n_max_training_samples, n_min_required_samples, n_max_required_samples)))\n    train_sizes_abs = np.unique(train_sizes_abs)\n    if (n_ticks > train_sizes_abs.shape[0]):\n        warnings.warn((\"Removed duplicate entries from 'train_sizes'. Number of ticks will be less than the size of 'train_sizes' %d instead of %d).\" % (train_sizes_abs.shape[0], n_ticks)), RuntimeWarning)\n    return train_sizes_abs\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(x, model_path):\n    output_layer = network(image)\n    with gzip.open(model_path, 'r') as openFile:\n        parameters = paddle.parameters.Parameters.from_tar(openFile)\n    result = paddle.infer(input=x, parameters=parameters, output_layer=output_layer, feeding={'image': 0})\n    return result\n"}
{"label_name":"train","label":0,"method_name":"read_train_dev_files","method":"\n\ndef read_train_dev_files(trainx, devx, trainy, devy):\n    warnings.filterwarnings('ignore')\n    (X_train, n_feats_train, k_x_train) = getX(trainx)\n    (X_dev, n_feats_dev, k_x_dev) = getX(devx)\n    (y_train, k_y_train) = getY(trainy)\n    (y_dev, k_y_dev) = getY(devy)\n    if (n_feats_train != n_feats_dev):\n        print('Error n_feats in train and dev. They are not equal.')\n        sys.exit(1)\n    n_feats = n_feats_train\n    if (k_x_train != k_y_train):\n        print('Error, train is of different size')\n        sys.exit(1)\n    k_train = k_x_train\n    if (k_x_dev != k_y_dev):\n        print('Error, dev is of different size')\n        sys.exit(1)\n    k_dev = k_x_dev\n    print((((((('Data has ' + str(n_feats)) + ' features and ') + str(k_train)) + ' training points and ') + str(k_dev)) + ' dev points.'))\n    return (np.array(X_train), np.array(y_train), np.array(X_dev), np.array(y_dev))\n"}
{"label_name":"train","label":0,"method_name":"_retrieve_svhn_training","method":"\n\ndef _retrieve_svhn_training():\n    url = 'http:\/\/ufldl.stanford.edu\/housenumbers\/train_32x32.mat'\n    return _retrieve_svhn('train.npz', url)\n"}
{"label_name":"process","label":2,"method_name":"pre_process_batch","method":"\n\ndef pre_process_batch(data_batch: t.Dict[(structure.BrainImageTypes, structure.BrainImage)], pre_process_params: dict=None, multi_process=True) -> t.List[structure.BrainImage]:\n    'Loads and pre-processes a batch of images.\\n\\n    The pre-processing includes:\\n\\n    - Registration\\n    - Pre-processing\\n    - Feature extraction\\n\\n    Args:\\n        data_batch (Dict[structure.BrainImageTypes, structure.BrainImage]): Batch of images to be processed.\\n        pre_process_params (dict): Pre-processing parameters.\\n        multi_process (bool): Whether to use the parallel processing on multiple cores or to run sequentially.\\n\\n    Returns:\\n        List[structure.BrainImage]: A list of images.\\n    '\n    if (pre_process_params is None):\n        pre_process_params = {}\n    params_list = list(data_batch.items())\n    if multi_process:\n        images = mproc.MultiProcessor.run(pre_process, params_list, pre_process_params, mproc.PreProcessingPickleHelper)\n    else:\n        images = [pre_process(id_, path, **pre_process_params) for (id_, path) in params_list]\n    return images\n"}
{"label_name":"process","label":2,"method_name":"process_store","method":"\n\n@pytest.fixture\ndef process_store(request):\n    from palladium.util import process_store\n    orig = deepcopy(process_store)\n    (yield process_store)\n    process_store.clear()\n    process_store.update(orig)\n"}
{"label_name":"process","label":2,"method_name":"regress_process","method":"\n\ndef regress_process(estimator, train_x, train_y_regress, test_x, test_y_regress):\n    estimator.fit(train_x, train_y_regress)\n    test_y_prdict_regress = estimator.predict(test_x)\n    plt.plot(test_y_regress.cumsum())\n    plt.plot(test_y_prdict_regress.cumsum())\n    from abupy import cross_val_score\n    from abupy.CoreBu.ABuFixes import mean_squared_error_scorer\n    scores = cross_val_score(estimator, train_x, train_y_regress, cv=10, scoring=mean_squared_error_scorer)\n    mean_sc = (- np.mean(np.sqrt((- scores))))\n    print('{} RMSE: {}'.format(estimator.__class__.__name__, mean_sc))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(self):\n    loss_ph = self.framework.placeholders\n    loss_mva = None\n    profile = list()\n    batches = self.framework.shuffle()\n    loss_op = self.framework.loss\n    for (i, (x_batch, datum)) in enumerate(batches):\n        if (not i):\n            self.say(train_stats.format(self.FLAGS.lr, self.FLAGS.batch, self.FLAGS.epoch, self.FLAGS.save))\n        feed_dict = {loss_ph[key]: datum[key] for key in loss_ph}\n        feed_dict[self.inp] = x_batch\n        feed_dict.update(self.feed)\n        fetches = [self.train_op, loss_op]\n        if self.FLAGS.summary:\n            fetches.append(self.summary_op)\n        fetched = self.sess.run(fetches, feed_dict)\n        loss = fetched[1]\n        if (loss_mva is None):\n            loss_mva = loss\n        loss_mva = ((0.9 * loss_mva) + (0.1 * loss))\n        step_now = ((self.FLAGS.load + i) + 1)\n        if self.FLAGS.summary:\n            self.writer.add_summary(fetched[2], step_now)\n        form = 'step {} - loss {} - moving ave loss {}'\n        self.say(form.format(step_now, loss, loss_mva))\n        profile += [(loss, loss_mva)]\n        ckpt = ((i + 1) % (self.FLAGS.save \/\/ self.FLAGS.batch))\n        args = [step_now, profile]\n        if (not ckpt):\n            _save_ckpt(self, *args)\n    if ckpt:\n        _save_ckpt(self, *args)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train():\n    g_input_size = 1\n    g_hidden_size = 5\n    g_output_size = 1\n    d_input_size = 500\n    d_hidden_size = 10\n    d_output_size = 1\n    minibatch_size = d_input_size\n    d_learning_rate = 0.001\n    g_learning_rate = 0.001\n    sgd_momentum = 0.9\n    num_epochs = 5000\n    print_interval = 100\n    d_steps = 20\n    g_steps = 20\n    (dfe, dre, ge) = (0, 0, 0)\n    (d_real_data, d_fake_data, g_fake_data) = (None, None, None)\n    discriminator_activation_function = torch.sigmoid\n    generator_activation_function = torch.tanh\n    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n    gi_sampler = get_generator_input_sampler()\n    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size, f=generator_activation_function)\n    D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size, f=discriminator_activation_function)\n    criterion = nn.BCELoss()\n    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n    for epoch in range(num_epochs):\n        for d_index in range(d_steps):\n            D.zero_grad()\n            d_real_data = Variable(d_sampler(d_input_size))\n            d_real_decision = D(preprocess(d_real_data))\n            d_real_error = criterion(d_real_decision, Variable(torch.ones([1, 1])))\n            d_real_error.backward()\n            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n            d_fake_data = G(d_gen_input).detach()\n            d_fake_decision = D(preprocess(d_fake_data.t()))\n            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1, 1])))\n            d_fake_error.backward()\n            d_optimizer.step()\n            (dre, dfe) = (extract(d_real_error)[0], extract(d_fake_error)[0])\n        for g_index in range(g_steps):\n            G.zero_grad()\n            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n            g_fake_data = G(gen_input)\n            dg_fake_decision = D(preprocess(g_fake_data.t()))\n            g_error = criterion(dg_fake_decision, Variable(torch.ones([1, 1])))\n            g_error.backward()\n            g_optimizer.step()\n            ge = extract(g_error)[0]\n        if ((epoch % print_interval) == 0):\n            print(('Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) ' % (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data)))))\n    if matplotlib_is_available:\n        print('Plotting the generated distribution...')\n        values = extract(g_fake_data)\n        print((' Values: %s' % str(values)))\n        plt.hist(values, bins=50)\n        plt.xlabel('Value')\n        plt.ylabel('Count')\n        plt.title('Histogram of Generated Distribution')\n        plt.grid(True)\n        plt.show()\n"}
{"label_name":"process","label":2,"method_name":"queue_process","method":"\n\ndef queue_process(q):\n    (promise, fn, args, kwargs) = q.get()\n    process(promise, fn, args, kwargs)\n"}
{"label_name":"train","label":0,"method_name":"execute_train_scripts","method":"\n\ndef execute_train_scripts(pop, gen):\n    str = ''\n    for thread in range(_NUM_THREADS):\n        str += (script_filename(gen, thread) + ' & ')\n    os.system(str)\n"}
{"label_name":"train","label":0,"method_name":"get_variables_to_train","method":"\n\ndef get_variables_to_train():\n    train_variables = []\n    for var in tf.trainable_variables():\n        if ('Inception' not in var.name):\n            train_variables.append(var)\n    return train_variables\n"}
{"label_name":"save","label":1,"method_name":"save_tokenlization_result","method":"\n\ndef save_tokenlization_result(data, target, file_path='.\/data\/tags_token_results'):\n    with codecs.open(file_path, 'w', 'utf-8') as f:\n        for x in data:\n            f.write((' '.join(x) + '\\n'))\n    with open((file_path + '_tag'), 'w') as f:\n        for x in target:\n            f.write((x + '\\n'))\n"}
{"label_name":"process","label":2,"method_name":"_postprocess","method":"\n\ndef _postprocess(x, c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, complete=False, undo=[], status=0, message=''):\n    '\\n    Given solution x to presolved, standard form linear program x, add\\n    fixed variables back into the problem and undo the variable substitutions\\n    to get solution to original linear program. Also, calculate the objective\\n    function value, slack in original upper bound constraints, and residuals\\n    in original equality constraints.\\n\\n    Parameters\\n    ----------\\n    x : 1-D array\\n        Solution vector to the standard-form problem.\\n    c : 1-D array\\n        Original coefficients of the linear objective function to be minimized.\\n    A_ub : 2-D array\\n        Original upper bound constraint matrix.\\n    b_ub : 1-D array\\n        Original upper bound constraint vector.\\n    A_eq : 2-D array\\n        Original equality constraint matrix.\\n    b_eq : 1-D array\\n        Original equality constraint vector.\\n    bounds : sequence of tuples\\n        Bounds, as modified in presolve\\n    complete : bool\\n        Whether the solution is was determined in presolve (``True`` if so)\\n    undo: list of tuples\\n        (`index`, `value`) pairs that record the original index and fixed value\\n        for each variable removed from the problem\\n    status : int\\n        An integer representing the exit status of the optimization::\\n\\n             0 : Optimization terminated successfully\\n             1 : Iteration limit reached\\n             2 : Problem appears to be infeasible\\n             3 : Problem appears to be unbounded\\n             4 : Serious numerical difficulties encountered\\n\\n    message : str\\n        A string descriptor of the exit status of the optimization.\\n\\n    Returns\\n    -------\\n    x : 1-D array\\n        Solution vector to original linear programming problem\\n    fun: float\\n        optimal objective value for original problem\\n    slack: 1-D array\\n        The (non-negative) slack in the upper bound constraints, that is,\\n        ``b_ub - A_ub * x``\\n    con : 1-D array\\n        The (nominally zero) residuals of the equality constraints, that is,\\n        ``b - A_eq * x``\\n    status : int\\n        An integer representing the exit status of the optimization::\\n\\n             0 : Optimization terminated successfully\\n             1 : Iteration limit reached\\n             2 : Problem appears to be infeasible\\n             3 : Problem appears to be unbounded\\n             4 : Serious numerical difficulties encountered\\n\\n    message : str\\n        A string descriptor of the exit status of the optimization.\\n\\n    '\n    n_x = len(c)\n    no_adjust = set()\n    if (len(undo) > 0):\n        no_adjust = set(undo[0])\n        x = x.tolist()\n        for (i, val) in zip(undo[0], undo[1]):\n            x.insert(i, val)\n        x = np.array(x)\n    if ((not complete) and (bounds is not None)):\n        n_unbounded = 0\n        for (i, b) in enumerate(bounds):\n            if (i in no_adjust):\n                continue\n            (lb, ub) = b\n            if ((lb is None) and (ub is None)):\n                n_unbounded += 1\n                x[i] = (x[i] - x[((n_x + n_unbounded) - 1)])\n            elif (lb is None):\n                x[i] = (ub - x[i])\n            else:\n                x[i] += lb\n    n_x = len(c)\n    x = x[:n_x]\n    fun = x.dot(c)\n    slack = (b_ub - A_ub.dot(x))\n    con = (b_eq - A_eq.dot(x))\n    if ((status == 0) and (np.isnan(x).any() or np.isnan(fun) or np.isnan(slack).any() or np.isnan(con).any())):\n        status = 4\n        message = \"Numerical difficulties were encountered but no errors were raised. This is known to occur if the 'presolve' option is False, 'sparse' is True, and A_eq includes redundant rows. If you encounter this under different circumstances, please submit a bug report. Otherwise, remove linearly dependent equations from your equality constraints or enable presolve.\"\n    return (x, fun, slack, con, status, message)\n"}
{"label_name":"predict","label":4,"method_name":"get_predictions","method":"\n\n@app.route('\/<prediction_type>\/<pandas_orient>', methods=['POST'])\ndef get_predictions(prediction_type: str, pandas_orient: str):\n    assert (prediction_type in ['predict', 'predict_proba'])\n    return perform_prediction((prediction_type == 'predict_proba'), pandas_orient)\n"}
{"label_name":"process","label":2,"method_name":"data_preprocessing","method":"\n\ndef data_preprocessing(x_train, x_test):\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - 123.68)\n    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - 116.779)\n    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - 103.939)\n    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - 123.68)\n    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - 116.779)\n    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - 103.939)\n    return (x_train, x_test)\n"}
{"label_name":"process","label":2,"method_name":"kill_child_processes","method":"\n\ndef kill_child_processes(parent_pid, sig=signal.SIGTERM):\n    'kill all child processes recursively'\n    try:\n        parent = psutil.Process(parent_pid)\n    except psutil.NoSuchProcess:\n        return\n    children = parent.children(recursive=True)\n    for process in children:\n        try:\n            process.send_signal(sig)\n        except psutil.NoSuchProcess:\n            return\n"}
{"label_name":"process","label":2,"method_name":"process_model","method":"\n\ndef process_model(model, tensor, func, name):\n    model.eval()\n    traced_script_module = torch.jit.trace(model, tensor)\n    traced_script_module.save('model.pt')\n    py_output = model.forward(tensor)\n    cpp_output = func('model.pt', tensor)\n    assert torch.allclose(py_output, cpp_output), (('Output mismatch of ' + name) + ' models')\n"}
{"label_name":"save","label":1,"method_name":"save_checkpoint","method":"\n\ndef save_checkpoint(net, args, epoch, mIoU, is_best=False):\n    'Save Checkpoint'\n    filename = ('epoch_%04d_mIoU_%2.4f.params' % (epoch, mIoU))\n    filepath = os.path.join(args.save_dir, filename)\n    net.save_parameters(filepath)\n    if is_best:\n        shutil.copyfile(filename, os.path.join(args.save_dir, 'model_best.params'))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\n@deprecated(since='2.0.0', update_to='paddle.text.datasets.Imdb', reason='Please use new dataset API which supports paddle.io.DataLoader')\ndef train(word_idx):\n    '\\n    IMDB training set creator.\\n\\n    It returns a reader creator, each sample in the reader is an zero-based ID\\n    sequence and label in [0, 1].\\n\\n    :param word_idx: word dictionary\\n    :type word_idx: dict\\n    :return: Training reader creator\\n    :rtype: callable\\n    '\n    return reader_creator(re.compile('aclImdb\/train\/pos\/.*\\\\.txt$'), re.compile('aclImdb\/train\/neg\/.*\\\\.txt$'), word_idx)\n"}
{"label_name":"train","label":0,"method_name":"assign_pretrained_word_embedding","method":"\n\ndef assign_pretrained_word_embedding(sess, vocab, vocab_size, textrcnn, word2vec_model_path='data\/news_12g_baidubaike_20g_novel_90g_embedding_64.model'):\n    print('using pre-trained word emebedding start ...')\n    word2vec_model = KeyedVectors.load(word2vec_model_path)\n    word_embedding_2dlist = ([[]] * vocab_size)\n    word_embedding_2dlist[(- 1)] = np.zeros(FLAGS.embed_size)\n    bound = (np.sqrt(6.0) \/ np.sqrt(vocab_size))\n    count_exist = 0\n    count_not_exist = 0\n    for (i, word) in enumerate(list(vocab.keys())):\n        word = word.encode('utf8')\n        embedding = None\n        try:\n            embedding = word2vec_model[word]\n        except Exception as e:\n            embedding = None\n        if (embedding is not None):\n            word_embedding_2dlist[i] = embedding\n            count_exist = (count_exist + 1)\n        else:\n            word_embedding_2dlist[i] = np.random.uniform((- bound), bound, FLAGS.embed_size)\n            count_not_exist = (count_not_exist + 1)\n    word_embedding_final = np.array(word_embedding_2dlist)\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)\n    t_assign_embedding = tf.assign(textrcnn.Embedding, word_embedding)\n    sess.run(t_assign_embedding)\n    print('word exists embedding: {}, word not exists embedding: {}'.format(count_exist, count_not_exist))\n    print('using pre-trained word emebedding end ...')\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\n\ndef save_model(model, save_to: str):\n    torch.save(model.state_dict(), save_to)\n"}
{"label_name":"predict","label":4,"method_name":"check_predictions","method":"\n\ndef check_predictions(clf, X, y):\n    'Check that the model is able to fit the classification data'\n    n_samples = len(y)\n    classes = np.unique(y)\n    n_classes = classes.shape[0]\n    predicted = clf.fit(X, y).predict(X)\n    assert_array_equal(clf.classes_, classes)\n    assert_equal(predicted.shape, (n_samples,))\n    assert_array_equal(predicted, y)\n    probabilities = clf.predict_proba(X)\n    assert_equal(probabilities.shape, (n_samples, n_classes))\n    assert_array_almost_equal(probabilities.sum(axis=1), np.ones(n_samples))\n    assert_array_equal(probabilities.argmax(axis=1), y)\n"}
{"label_name":"process","label":2,"method_name":"pre_process_dialouge_seq_2_seq","method":"\n\ndef pre_process_dialouge_seq_2_seq(raw_dialogue_file_path, pre_processed_dir, movie_dir_name, dialogue_file1, dialogue_file2, train_file1, train_file2, test_file1, test_file2, max_dialouge_count, max_dialouge_count_train_test, train_percentile, compare_count, symbol_seq, type_d, max_len, type_rand, max_test_dev_count):\n    print('Extracting file')\n    start = time.time()\n    delete_all_file_dir(pre_processed_dir, type_d)\n    if (movie_dir_name == cornell_movie_dir):\n        (dialogue_file1, dialogue_file2) = dialouge_seperator_cornell_movie(raw_dialogue_file_path, dialogue_file1, dialogue_file2, symbol_seq, max_dialouge_count, max_len)\n    elif (movie_dir_name == open_subtitles_dir):\n        (dialogue_file1, dialogue_file2) = dialouge_seperator_open_subtitle(raw_dialogue_file_path, dialogue_file1, dialogue_file2, max_dialouge_count)\n    elif (movie_dir_name == movie_subtitles_dir):\n        (dialogue_file1, dialogue_file2) = dialouge_seperator_movie_subtitle(raw_dialogue_file_path, dialogue_file1, dialogue_file2, max_dialouge_count)\n    else:\n        print('directory match not found')\n        return\n    (train_file1, train_file2, test_file1, test_file2, dev_file1, dev_file2) = train_test_split_seq2_seq(dialogue_file1, dialogue_file2, max_dialouge_count_train_test, train_percentile, train_file1, train_file2, test_file1, test_file2, max_test_dev_count)\n    compare_chat_reply(dialogue_file1, dialogue_file2, compare_count, type_rand)\n    end = time.time()\n    time_lib.elapsed_time(start, end)\n"}
{"label_name":"predict","label":4,"method_name":"_single_value_predictions","method":"\n\ndef _single_value_predictions(activations, sequence_length, target_column, problem_type, predict_probabilities):\n    'Maps `activations` from the RNN to predictions for single value models.\\n\\n  If `predict_probabilities` is `False`, this function returns a `dict`\\n  containing single entry with key `PREDICTIONS_KEY`. If `predict_probabilities`\\n  is `True`, it will contain a second entry with key `PROBABILITIES_KEY`. The\\n  value of this entry is a `Tensor` of probabilities with shape\\n  `[batch_size, num_classes]`.\\n\\n  Args:\\n    activations: Output from an RNN. Should have dtype `float32` and shape\\n      `[batch_size, padded_length, ?]`.\\n    sequence_length: A `Tensor` with shape `[batch_size]` and dtype `int32`\\n      containing the length of each sequence in the batch. If `None`, sequences\\n      are assumed to be unpadded.\\n    target_column: An initialized `TargetColumn`, calculate predictions.\\n    problem_type: Either `ProblemType.CLASSIFICATION` or\\n      `ProblemType.LINEAR_REGRESSION`.\\n    predict_probabilities: A Python boolean, indicating whether probabilities\\n      should be returned. Should only be set to `True` for\\n      classification\/logistic regression problems.\\n  Returns:\\n    A `dict` mapping strings to `Tensors`.\\n  '\n    with ops.name_scope('SingleValuePrediction'):\n        last_activations = rnn_common.select_last_activations(activations, sequence_length)\n        predictions_name = (prediction_key.PredictionKey.CLASSES if (problem_type == constants.ProblemType.CLASSIFICATION) else prediction_key.PredictionKey.SCORES)\n        if predict_probabilities:\n            probabilities = target_column.logits_to_predictions(last_activations, proba=True)\n            prediction_dict = {prediction_key.PredictionKey.PROBABILITIES: probabilities, predictions_name: math_ops.argmax(probabilities, 1)}\n        else:\n            predictions = target_column.logits_to_predictions(last_activations, proba=False)\n            prediction_dict = {predictions_name: predictions}\n        return prediction_dict\n"}
{"label_name":"save","label":1,"method_name":"saveResultByField","method":"\n\ndef saveResultByField(results, field_name):\n    d = {res['label']: getattr(res['history'], field_name) for res in results}\n    df = pd.DataFrame(d)\n    fn = (field_name + '.csv')\n    file_name = os.path.join(exp_name, fn)\n    df.to_csv(file_name)\n"}
{"label_name":"predict","label":4,"method_name":"find_in_prediction","method":"\n\ndef find_in_prediction(prediction, target):\n    try:\n        (targets, scores) = zip(*prediction)\n        return targets.index(target)\n    except ValueError:\n        return (- 1)\n"}
{"label_name":"process","label":2,"method_name":"title_preprocess","method":"\n\ndef title_preprocess(title):\n    title.replace(';', '').replace(',', '')\n    return ' '.join([morph_analyzer.normal_forms(word)[0] for word in title.split()])\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(estimator, X_test):\n    return estimator.predict(X_test)\n"}
{"label_name":"predict","label":4,"method_name":"predict_sequence_probabilities","method":"\n\ndef predict_sequence_probabilities(seqs):\n    NUM_FEATURES = 28\n    cdr_mats = []\n    cdr_masks = []\n    for seq in seqs:\n        cdr_mat = seq_to_one_hot(seq)\n        cdr_mat_pad = np.zeros((MAX_CDR_LEN, NUM_FEATURES))\n        cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n        cdr_mats.append(cdr_mat_pad)\n        cdr_mask = np.zeros((MAX_CDR_LEN,), dtype=int)\n        cdr_mask[:len(seq)] = 1\n        cdr_masks.append(cdr_mask)\n    cdrs = np.stack(cdr_mats)\n    masks = np.stack(cdr_masks)\n    model = get_predictor()\n    probs = model.predict([cdrs, masks], batch_size=32)\n    return np.squeeze(probs, axis=(- 1))\n"}
{"label_name":"train","label":0,"method_name":"remove_trainer_send_op","method":"\n\ndef remove_trainer_send_op(program, config, heter_block_index, block_var_detaile):\n    persistables = block_var_detaile[heter_block_index]['persistables']\n    need_remove_send_op = []\n    need_remove_grad_var = []\n    for op in find_send_op(program):\n        (input_list, _) = find_op_input_output(program, program.global_block(), op)\n        for var_name in input_list:\n            origin_var_name = var_name.split('@GRAD')[0]\n            if (origin_var_name in persistables):\n                need_remove_send_op.append(op)\n                need_remove_grad_var.append(var_name)\n    need_remove_send_op = list(set(need_remove_send_op))\n    delete_ops(program.global_block(), need_remove_send_op)\n    for grad_var_name in need_remove_grad_var:\n        config.remove_var_pair_by_grad(grad_var_name)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(model, x_val):\n    model.eval()\n    x = Variable(x_val, requires_grad=False)\n    output = model.forward(x)\n    return output.data.numpy().argmax(axis=1)\n"}
{"label_name":"train","label":0,"method_name":"train_gan","method":"\n\ndef train_gan():\n    identifier = id()\n    dir_path = ('%s-gan-w' % identifier)\n    os.mkdir(dir_path)\n    G = generator()\n    D = discriminator()\n    D.compile(optimizer=Adam(lr=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n    G_input = Input(shape=(128, 128, 3))\n    G_output = G(G_input)\n    raw_image_input = Input(shape=(128, 128, 3))\n    D_output = D([raw_image_input, G_output])\n    GAN = Model(inputs=[G_input, raw_image_input], outputs=[G_output, D_output])\n    gan_loss = ['mae', 'binary_crossentropy']\n    gan_loss_weight = [100.0, 1]\n    D.trainable = False\n    GAN.compile(optimizer=Adam(lr=1e-06), loss=gan_loss, loss_weights=gan_loss_weight)\n    X = get_training_image(100000)\n    X_size = int((len(X) * 0.5))\n    train_size = int((X_size * 0.8))\n    test_size = (X_size - train_size)\n    X_train = X[0:train_size, :, :, :]\n    X_test = X[train_size:(train_size + test_size), :, :, :]\n    epochs = 400000\n    batch_size = 16\n    num_batch = int((train_size \/ batch_size))\n    checkpoint = 1\n    for epoch in range(epochs):\n        start_time = time.time()\n        batch_x = 0\n        X_test_from_G = G.predict(X_test)\n        y_test_D = np.tile([1, 0], [test_size, 1])\n        loss_D = D.evaluate([X_test, X_test_from_G], y_test_D, verbose=0)\n        acc_D = loss_D[1]\n        trainable_D = True\n        if (acc_D > 0.95):\n            trainable_D = False\n        for _ in range(num_batch):\n            lb = batch_x\n            ub = min([(batch_x + batch_size), (train_size - 1)])\n            sz = (ub - lb)\n            X_from_G = G.predict(X_train[lb:ub, :, :, :])\n            X_as_Gen = np.zeros(shape=(sz, 128, 128, 3))\n            X_as_raw = np.zeros(shape=(sz, 128, 128, 3))\n            swap_labels = np.random.randint(2, size=sz)\n            y_train_D = np.zeros(shape=(sz, 2))\n            for (i, bit) in enumerate(swap_labels):\n                if (bit == 0):\n                    X_as_Gen[i] = X_from_G[i]\n                    X_as_raw[i] = X_train[(sz + i)]\n                    y_train_D[i] = np.asarray([1, 0])\n                else:\n                    X_as_Gen[i] = X_train[(sz + i)]\n                    X_as_raw[i] = X_from_G[i]\n                    y_train_D[i] = np.asarray([0, 1])\n            y_train_GAN = np.tile([1, 0], [sz, 1])\n            GAN.train_on_batch([X_train[lb:ub, :, :, :], X_train[lb:ub, :, :, :]], [X_from_G, y_train_GAN])\n            if trainable_D:\n                D.train_on_batch([X_as_raw, X_as_Gen], y_train_D)\n            batch_x += batch_size\n        elapsed = (time.time() - start_time)\n        X_test_from_G = G.predict(X_test)\n        y_test_D = np.tile([1, 0], [test_size, 1])\n        loss_D = D.evaluate([X_test, X_test_from_G], y_test_D, verbose=0)\n        loss_GAN = GAN.evaluate([X_test, X_test], [X_test_from_G, y_test_D], verbose=0)\n        now = datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S')\n        print(('epoch %06d: D loss %.3f, acc %.3f: GAN loss %.3f, G loss %.3f D loss %.3f: trainable_D %d: %.3fs: %s' % (epoch, loss_D[0], loss_D[1], loss_GAN[0], loss_GAN[1], loss_GAN[2], trainable_D, elapsed, now)))\n        if ((epoch > 0) and ((epoch % checkpoint) == 0)):\n            pred_image_path = ('%s\/G-predict-%06d.png' % (dir_path, epoch))\n            pred = G.predict(X_test)\n            tests = np.hstack(X_test[0:10])\n            preds = np.hstack(pred[0:10])\n            if (epoch == checkpoint):\n                imsave(('%s\/tests-%06d.png' % (dir_path, epoch)), tests)\n            imsave(('%s\/preds-%06d.png' % (dir_path, epoch)), preds)\n            G.save(('%s\/G.h5' % dir_path))\n            D.save(('%s\/D.h5' % dir_path))\n            GAN.save(('%s\/GAN.h5' % dir_path))\n"}
{"label_name":"predict","label":4,"method_name":"_model_predict_keras","method":"\n\ndef _model_predict_keras(input_data, input_shape):\n    with tf.Session():\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(2, batch_input_shape=input_shape))\n        model.add(tf.keras.layers.Dense(3))\n        weights = model.get_weights()\n        config = model.get_config()\n        out = model.predict(input_data)\n    return (out, weights, config)\n"}
{"label_name":"save","label":1,"method_name":"save_smi","method":"\n\ndef save_smi(name, smiles):\n    if (not os.path.exists('epoch_data')):\n        os.makedirs('epoch_data')\n    smi_file = os.path.join('epoch_data', '{}.smi'.format(name))\n    with open(smi_file, 'w') as afile:\n        afile.write('\\n'.join(smiles))\n    return\n"}
{"label_name":"predict","label":4,"method_name":"predict_target_lengths","method":"\n\ndef predict_target_lengths(encoder_output, inputs_mask, hparams, length_diff=None):\n    'Predict target lengths.'\n    bound = hparams.lendiff_bound\n    inputs_length = tf.cast(tf.reduce_sum(inputs_mask, 1), tf.int32)\n    targets_length = inputs_length\n    loss = None\n    if hparams.predict_target_length:\n        encoder_output = gops.reduce_mean_over_l(encoder_output, inputs_mask)\n        logits = tf.stop_gradient(encoder_output)\n        logits = lenpred_mlp('lenpred', logits, hparams.hidden_size, bound)\n        if (length_diff is not None):\n            labels = tf.maximum(tf.minimum(length_diff, bound), (- bound))\n            labels = tf.cast((labels + bound), tf.int32)\n            labels = tf.stop_gradient(labels)\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n            loss = tf.reduce_mean(loss)\n        diff_pred = tf.argmax(logits, 1)\n        diff_pred = tf.cast((diff_pred - bound), tf.int32)\n        targets_length = (inputs_length + diff_pred)\n        targets_length = tf.maximum(targets_length, 1)\n    divi = 4\n    targets_length = (tf.ceil((targets_length \/ divi)) * divi)\n    targets_length = tf.cast(targets_length, tf.int32)\n    return (targets_length, loss)\n"}
{"label_name":"train","label":0,"method_name":"upsample_training_set","method":"\n\ndef upsample_training_set(country, df, measure, cut):\n    (df_test, df_train) = year_split(country, df)\n    (X_train, X_test, y_train_category, y_test_category, y_train_percent, y_test_percent) = separate_y(country, df_test, df_train)\n    category = (measure + cut)\n    y_test = y_test_category[category]\n    (X_train_upsampled, y_train_upsampled) = upsample_minority(X_train, y_train_category[category])\n    try:\n        (X_train_smote, y_train_smote) = smote_minority(X_train, y_train_category[category])\n    except:\n        (X_train_upsampled, y_train_upsampled) = upsample_minority(X_train, y_train_category[category])\n        (X_train_smote, y_train_smote) = smote_minority(X_train_upsampled, y_train_upsampled)\n    try:\n        (X_train_adasyn, y_train_adasyn) = adasyn_minority(X_train, y_train_category[category])\n    except:\n        (X_train_upsampled, y_train_upsampled) = upsample_minority(X_train, y_train_category[category])\n        (X_train_adasyn, y_train_adasyn) = adasyn_minority(X_train_upsampled, y_train_upsampled)\n    return (X_train, y_train_category[category], X_train_upsampled, y_train_upsampled, X_train_smote, y_train_smote, X_train_adasyn, y_train_adasyn, X_test, y_test)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(model, params, train_samples, val_samples, verbose=2):\n    batch_size = params['batch_size']\n    logging.info('Start training on {} samples with batch_size={}...'.format(len(train_samples), batch_size))\n    model_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=verbose, save_best_only=True, mode='auto')\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=verbose, mode='auto')\n    if (params['repres'] == 'words'):\n        generate_rows = generate_rows_words\n    elif (params['repres'] == 'pieces'):\n        generate_rows = generate_rows_pieces\n    hist = model.fit_generator(generator=generate_rows(params, computed_params, train_samples, 1), steps_per_epoch=(len(train_samples) \/\/ batch_size), epochs=NB_EPOCHS, verbose=verbose, callbacks=[model_checkpoint, early_stopping], validation_data=generate_rows(params, computed_params, val_samples, 1), validation_steps=(len(val_samples) \/\/ batch_size))\n    model.load_weights(weights_path)\n    return hist\n"}
{"label_name":"predict","label":4,"method_name":"decode_predictions","method":"\n\ndef decode_predictions(preds, top=5, index_file_path=''):\n    'Decodes the prediction of an ImageNet model.\\n\\n    # Arguments\\n        preds: Numpy tensor encoding a batch of predictions.\\n        top: integer, how many top-guesses to return.\\n\\n    # Returns\\n        A list of lists of top class prediction tuples\\n        `(class_name, class_description, score)`.\\n        One list of tuples per sample in batch input.\\n\\n    # Raises\\n        ValueError: in case of invalid shape of the `pred` array\\n            (must be 2D).\\n    '\n    global CLASS_INDEX\n    if ((len(preds.shape) != 2) or (preds.shape[1] != 1000)):\n        raise ValueError(('`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: ' + str(preds.shape)))\n    if (CLASS_INDEX is None):\n        fpath = index_file_path\n        CLASS_INDEX = json.load(open(fpath))\n    results = []\n    for pred in preds:\n        top_indices = pred.argsort()[(- top):][::(- 1)]\n        result = [(tuple(CLASS_INDEX[str(i)]) + (pred[i],)) for i in top_indices]\n        result.sort(key=(lambda x: x[2]), reverse=True)\n        results.append(result)\n    return results\n"}
{"label_name":"train","label":0,"method_name":"train_evaluate_cv","method":"\n\ndef train_evaluate_cv(pipeline_name, model_level, dev_mode):\n    if (parameter_eval(params.hyperparameter_search__method) is not None):\n        (score_mean, score_std) = train_evaluate_cv_tuning(pipeline_name, model_level, dev_mode)\n    else:\n        (score_mean, score_std) = train_evaluate_cv_one_run(pipeline_name, model_level, cfg.SOLUTION_CONFIG, dev_mode)\n    logger.info('ROC_AUC mean {}, ROC_AUC std {}'.format(score_mean, score_std))\n    ctx.channel_send('ROC_AUC', 0, score_mean)\n    ctx.channel_send('ROC_AUC STD', 0, score_std)\n"}
{"label_name":"save","label":1,"method_name":"_save","method":"\n\ndef _save(im, fp, filename):\n    if ((_handler is None) or (not hasattr('_handler', 'save'))):\n        raise IOError('FITS save handler not installed')\n    _handler.save(im, fp, filename)\n"}
{"label_name":"predict","label":4,"method_name":"predict_whole_volume_by_tiling","method":"\n\ndef predict_whole_volume_by_tiling(log, sessionTf, cnn3d, channels, roi_mask, inp_shapes_per_path, unpred_margin, batchsize, save_fms_flag, idxs_fms_to_save):\n    outp_pred_dims = cnn3d.calc_outp_dims_given_inp(inp_shapes_per_path[0])\n    stride_of_tiling = outp_pred_dims\n    n_fms_to_save = (calc_num_fms_to_save(cnn3d.pathways, idxs_fms_to_save) if save_fms_flag else 0)\n    inp_chan_dims = list(channels.shape[1:])\n    prob_maps_vols = np.zeros(([cnn3d.num_classes] + inp_chan_dims), dtype='float32')\n    array_fms_to_save = (np.zeros(([n_fms_to_save] + inp_chan_dims), dtype='float32') if save_fms_flag else None)\n    slice_coords_all_tiles = get_slice_coords_of_all_img_tiles(log, inp_shapes_per_path[0], stride_of_tiling, batchsize, inp_chan_dims, roi_mask)\n    n_tiles_for_subj = len(slice_coords_all_tiles)\n    log.print3('Ready to make predictions for all image segments (parts).')\n    log.print3(('Total number of Segments to process:' + str(n_tiles_for_subj)))\n    idx_next_tile_in_pred_vols = 0\n    idx_next_tile_in_fm_vols = 0\n    n_batches = (n_tiles_for_subj \/\/ batchsize)\n    t_fwd_pass_subj = 0\n    print_progress_step_test(log, n_batches, 0, batchsize, n_tiles_for_subj)\n    for batch_i in range(n_batches):\n        slice_coords_of_tiles_batch = slice_coords_all_tiles[(batch_i * batchsize):((batch_i + 1) * batchsize)]\n        channs_of_tiles_per_path = extractSegmentsGivenSliceCoords(cnn3d, slice_coords_of_tiles_batch, channels, inp_shapes_per_path, outp_pred_dims)\n        t_fwd_start = time.time()\n        ops_to_fetch = cnn3d.get_main_ops('test')\n        list_of_ops = ([ops_to_fetch['pred_probs']] + ops_to_fetch['list_of_fms_per_layer'])\n        feeds_dict = prepare_feeds_dict(cnn3d.get_main_feeds('test'), channs_of_tiles_per_path)\n        out_val_of_ops = sessionTf.run(fetches=list_of_ops, feed_dict=feeds_dict)\n        prob_maps_batch = out_val_of_ops[0]\n        fms_per_layer_and_path_for_batch = out_val_of_ops[1:]\n        t_fwd_pass_subj += (time.time() - t_fwd_start)\n        (idx_next_tile_in_pred_vols, prob_maps_vols) = stitch_predicted_to_prob_maps(prob_maps_vols, idx_next_tile_in_pred_vols, prob_maps_batch, batchsize, slice_coords_all_tiles, unpred_margin, stride_of_tiling)\n        if save_fms_flag:\n            (idx_next_tile_in_fm_vols, array_fms_to_save) = stitch_predicted_to_fms(array_fms_to_save, idx_next_tile_in_fm_vols, fms_per_layer_and_path_for_batch, batchsize, slice_coords_all_tiles, unpred_margin, stride_of_tiling, outp_pred_dims, cnn3d.pathways, idxs_fms_to_save)\n        print_progress_step_test(log, n_batches, (batch_i + 1), batchsize, n_tiles_for_subj)\n    log.print3(('TIMING: Segmentation of subject: [Forward Pass:] {0:.2f}'.format(t_fwd_pass_subj) + ' secs.'))\n    return (prob_maps_vols, array_fms_to_save)\n"}
{"label_name":"predict","label":4,"method_name":"_get_predict_input_fn","method":"\n\ndef _get_predict_input_fn(batch_size, noise_dims):\n\n    def predict_input_fn():\n        noise = tf.random_normal([batch_size, noise_dims])\n        return noise\n    return predict_input_fn\n"}
{"label_name":"train","label":0,"method_name":"setup_training","method":"\n\ndef setup_training(args, trainer_class=None):\n    'Perform several steps:\\n    - build model using provided criterion and task\\n    - load data\\n    - build trainer\\n    '\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        if ('file' not in kwargs):\n            builtin_print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}]\", *args, **kwargs)\n        else:\n            builtin_print(*args, **kwargs)\n    __builtin__.print = print\n    (task, model, criterion) = setup_training_model(args)\n    if (trainer_class is None):\n        trainer_class = Trainer\n    (trainer, epoch_itr) = build_trainer(args=args, task=task, model=model, criterion=criterion, trainer_class=trainer_class)\n    return (trainer, task, epoch_itr)\n"}
{"label_name":"predict","label":4,"method_name":"cross_val_predict","method":"\n\ndef cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs'):\n    \"Generate cross-validated estimates for each input data point\\n\\n    .. deprecated:: 0.18\\n        This module will be removed in 0.20.\\n        Use :func:`sklearn.model_selection.cross_val_predict` instead.\\n\\n    Read more in the :ref:`User Guide <cross_validation>`.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit' and 'predict'\\n        The object to use to fit the data.\\n\\n    X : array-like\\n        The data to fit. Can be, for example a list, or an array at least 2d.\\n\\n    y : array-like, optional, default: None\\n        The target variable to try to predict in the case of\\n        supervised learning.\\n\\n    cv : int, cross-validation generator or an iterable, optional\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 3-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - An object to be used as a cross-validation generator.\\n        - An iterable yielding train\/test splits.\\n\\n        For integer\/None inputs, if the estimator is a classifier and ``y`` is\\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\\n        other cases, :class:`KFold` is used.\\n\\n        Refer :ref:`User Guide <cross_validation>` for the various\\n        cross-validation strategies that can be used here.\\n\\n    n_jobs : integer, optional\\n        The number of CPUs to use to do the computation. -1 means\\n        'all CPUs'.\\n\\n    verbose : integer, optional\\n        The verbosity level.\\n\\n    fit_params : dict, optional\\n        Parameters to pass to the fit method of the estimator.\\n\\n    pre_dispatch : int, or string, optional\\n        Controls the number of jobs that get dispatched during parallel\\n        execution. Reducing this number can be useful to avoid an\\n        explosion of memory consumption when more jobs get dispatched\\n        than CPUs can process. This parameter can be:\\n\\n            - None, in which case all the jobs are immediately\\n              created and spawned. Use this for lightweight and\\n              fast-running jobs, to avoid delays due to on-demand\\n              spawning of the jobs\\n\\n            - An int, giving the exact number of total jobs that are\\n              spawned\\n\\n            - A string, giving an expression as a function of n_jobs,\\n              as in '2*n_jobs'\\n\\n    Returns\\n    -------\\n    preds : ndarray\\n        This is the result of calling 'predict'\\n\\n    Examples\\n    --------\\n    >>> from sklearn import datasets, linear_model\\n    >>> from sklearn.cross_validation import cross_val_predict\\n    >>> diabetes = datasets.load_diabetes()\\n    >>> X = diabetes.data[:150]\\n    >>> y = diabetes.target[:150]\\n    >>> lasso = linear_model.Lasso()\\n    >>> y_pred = cross_val_predict(lasso, X, y)\\n    \"\n    (X, y) = indexable(X, y)\n    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    preds_blocks = parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params) for (train, test) in cv))\n    preds = [p for (p, _) in preds_blocks]\n    locs = np.concatenate([loc for (_, loc) in preds_blocks])\n    if (not _check_is_partition(locs, _num_samples(X))):\n        raise ValueError('cross_val_predict only works for partitions')\n    inv_locs = np.empty(len(locs), dtype=int)\n    inv_locs[locs] = np.arange(len(locs))\n    if sp.issparse(preds[0]):\n        preds = sp.vstack(preds, format=preds[0].format)\n    else:\n        preds = np.concatenate(preds)\n    return preds[inv_locs]\n"}
{"label_name":"train","label":0,"method_name":"_train","method":"\n\ndef _train(train_dat, valid_dat, test_dat, no_views):\n    train_size = train_dat.size()\n    _log('training size = {}'.format(train_size))\n    cnn = _get_model()\n    for (b_inputs, _, b_paths) in train_dat.batches(1):\n        outputs = [cnn.classify(b_input) for b_input in b_inputs]\n        outputs = _summarize(outputs)\n        for (index, path) in enumerate(b_paths):\n            np.save(os.path.join(path, 'data.{}.{}'.format(FLAGS.model, FLAGS.summarizer)), outputs[index])\n            _log('saved {}'.format(os.path.join(path, 'data.{}.{}'.format(FLAGS.model, FLAGS.summarizer))))\n    for (b_inputs, _, b_paths) in valid_dat.batches(1):\n        outputs = [cnn.classify(b_input) for b_input in b_inputs]\n        outputs = _summarize(outputs)\n        for (index, path) in enumerate(b_paths):\n            np.save(os.path.join(path, 'data.{}.{}'.format(FLAGS.model, FLAGS.summarizer)), outputs[index])\n            _log('saved {}'.format(os.path.join(path, 'data.{}.{}'.format(FLAGS.model, FLAGS.summarizer))))\n    for (b_inputs, _, b_paths) in test_dat.batches(1):\n        outputs = [cnn.classify(b_input) for b_input in b_inputs]\n        outputs = _summarize(outputs)\n        for (index, path) in enumerate(b_paths):\n            np.save(os.path.join(path, 'data.{}.{}'.format(FLAGS.model, FLAGS.summarizer)), outputs[index])\n            _log('saved {}'.format(os.path.join(path, 'data.{}.{}'.format(FLAGS.model, FLAGS.summarizer))))\n"}
{"label_name":"train","label":0,"method_name":"training_workflow","method":"\n\ndef training_workflow(config, reporter):\n    env = gym.make('CartPole-v0')\n    policy = CustomPolicy(env.observation_space, env.action_space, {})\n    workers = [RolloutWorker.as_remote().remote(env_creator=(lambda c: gym.make('CartPole-v0')), policy=CustomPolicy) for _ in range(config['num_workers'])]\n    for _ in range(config['num_iters']):\n        weights = ray.put({DEFAULT_POLICY_ID: policy.get_weights()})\n        for w in workers:\n            w.set_weights.remote(weights)\n        T1 = SampleBatch.concat_samples(ray.get([w.sample.remote() for w in workers]))\n        new_value = (policy.w * 2.0)\n        for w in workers:\n            w.for_policy.remote((lambda p: p.update_some_value(new_value)))\n        T2 = SampleBatch.concat_samples(ray.get([w.sample.remote() for w in workers]))\n        policy.learn_on_batch(T1)\n        policy.update_some_value(sum(T2['rewards']))\n        reporter(**collect_metrics(remote_workers=workers))\n"}
{"label_name":"process","label":2,"method_name":"process_kmedoids","method":"\n\ndef process_kmedoids(sample):\n    instance = kmedoids(sample, [(CURRENT_CLUSTER_SIZE * multiplier) for multiplier in range(NUMBER_CLUSTERS)])\n    (ticks, _) = timedcall(instance.process)\n    return ticks\n"}
{"label_name":"train","label":0,"method_name":"train_texts","method":"\n\ndef train_texts(texts: List[Text], model_name: Text, model_weights: Text) -> List[Message]:\n    config = create_pretrained_transformers_config(model_name, model_weights)\n    whitespace_tokenizer = WhitespaceTokenizer()\n    transformer = HFTransformersNLP(config)\n    messages = [Message.build(text=text) for text in texts]\n    td = TrainingData(messages)\n    whitespace_tokenizer.train(td)\n    transformer.train(td)\n    return messages\n"}
{"label_name":"save","label":1,"method_name":"load_saved_params","method":"\n\ndef load_saved_params():\n    '\\n    A helper function that loads previously saved parameters and resets\\n    iteration start.\\n    '\n    st = 0\n    for f in glob.glob('saved_params_*.npy'):\n        iter = int(op.splitext(op.basename(f))[0].split('_')[2])\n        if (iter > st):\n            st = iter\n    if (st > 0):\n        with open(('saved_params_%d.npy' % st), 'rb') as f:\n            params = pickle.load(f)\n            state = pickle.load(f)\n        return (st, params, state)\n    else:\n        return (st, None, None)\n"}
{"label_name":"process","label":2,"method_name":"process_image","method":"\n\ndef process_image(img, nmbOfDilate):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    filter = (np.ones((5, 5), np.float32) \/ 25)\n    i = cv2.filter2D(gray, (- 1), filter)\n    i = cv2.adaptiveThreshold(i, 1, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 3)\n    kernel = np.ones((3, 3), np.uint8)\n    i = cv2.morphologyEx(i, cv2.MORPH_OPEN, kernel, iterations=1)\n    i = cv2.dilate(i, kernel, iterations=nmbOfDilate)\n    return i\n"}
{"label_name":"save","label":1,"method_name":"save_metrics","method":"\n\ndef save_metrics(edd, fa, md, args):\n    npz_filename = 'results_algo{}_d{}_m{}_n{}_nb{}_B{}.npz'.format(args.algo, args.d, args.m, args.n, args.nb, args.B)\n    np.savez(npz_filename, edd=edd, fa=fa, md=md)\n    return\n"}
{"label_name":"train","label":0,"method_name":"trainHMMsegmenter_fromfile","method":"\n\ndef trainHMMsegmenter_fromfile(wavFile, gtFile, hmmModelName, mt_win, mt_step):\n    if (not os.path.isfile(wavFile)):\n        print('Error: wavfile does not exist!')\n        return\n    if (not os.path.isfile(gtFile)):\n        print('Error: groundtruth does not exist!')\n        return\n    aS.train_hmm_from_file(wavFile, gtFile, hmmModelName, mt_win, mt_step)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(clf_factory, X, Y, name, plot=False):\n    labels = np.unique(Y)\n    cv = ShuffleSplit(n=len(X), n_iter=1, test_size=0.3, indices=True, random_state=0)\n    train_errors = []\n    test_errors = []\n    scores = []\n    pr_scores = defaultdict(list)\n    (precisions, recalls, thresholds) = (defaultdict(list), defaultdict(list), defaultdict(list))\n    roc_scores = defaultdict(list)\n    tprs = defaultdict(list)\n    fprs = defaultdict(list)\n    clfs = []\n    cms = []\n    for (train, test) in cv:\n        (X_train, y_train) = (X[train], Y[train])\n        (X_test, y_test) = (X[test], Y[test])\n        clf = clf_factory()\n        clf.fit(X_train, y_train)\n        clfs.append(clf)\n        train_score = clf.score(X_train, y_train)\n        test_score = clf.score(X_test, y_test)\n        scores.append(test_score)\n        train_errors.append((1 - train_score))\n        test_errors.append((1 - test_score))\n        y_pred = clf.predict(X_test)\n        cm = confusion_matrix(y_test, y_pred)\n        cms.append(cm)\n        for label in labels:\n            y_label_test = np.asarray((y_test == label), dtype=int)\n            proba = clf.predict_proba(X_test)\n            proba_label = proba[:, label]\n            (precision, recall, pr_thresholds) = precision_recall_curve(y_label_test, proba_label)\n            pr_scores[label].append(auc(recall, precision))\n            precisions[label].append(precision)\n            recalls[label].append(recall)\n            thresholds[label].append(pr_thresholds)\n            (fpr, tpr, roc_thresholds) = roc_curve(y_label_test, proba_label)\n            roc_scores[label].append(auc(fpr, tpr))\n            tprs[label].append(tpr)\n            fprs[label].append(fpr)\n    if plot:\n        for label in labels:\n            print(('Plotting %s' % genre_list[label]))\n            scores_to_sort = roc_scores[label]\n            median = np.argsort(scores_to_sort)[(len(scores_to_sort) \/ 2)]\n            desc = ('%s %s' % (name, genre_list[label]))\n            plot_pr(pr_scores[label][median], desc, precisions[label][median], recalls[label][median], label=('%s vs rest' % genre_list[label]))\n            plot_roc(roc_scores[label][median], desc, tprs[label][median], fprs[label][median], label=('%s vs rest' % genre_list[label]))\n    all_pr_scores = np.asarray(pr_scores.values()).flatten()\n    summary = (np.mean(scores), np.std(scores), np.mean(all_pr_scores), np.std(all_pr_scores))\n    print(('%.3f\\t%.3f\\t%.3f\\t%.3f\\t' % summary))\n    return (np.mean(train_errors), np.mean(test_errors), np.asarray(cms))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    '\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>\/\\n        \u251c\u2500\u2500 <person1>\/\\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\\n        \u2502   \u251c\u2500\u2500 ...\\n        \u251c\u2500\u2500 <person2>\/\\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\\n        \u2514\u2500\u2500 ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    '\n    X = []\n    y = []\n    for class_dir in os.listdir(train_dir):\n        if (not os.path.isdir(os.path.join(train_dir, class_dir))):\n            continue\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n            if (len(face_bounding_boxes) != 1):\n                if verbose:\n                    print('Image {} not suitable for training: {}'.format(img_path, (\"Didn't find a face\" if (len(face_bounding_boxes) < 1) else 'Found more than one face')))\n            else:\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n    if (n_neighbors is None):\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print('Chose n_neighbors automatically:', n_neighbors)\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n    if (model_save_path is not None):\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n    return knn_clf\n"}
{"label_name":"process","label":2,"method_name":"process_file","method":"\n\ndef process_file(source):\n    lines = resolve_includes(source)\n    return process_str(''.join(lines))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(train_story, train_questions, train_qstory, memory, model, loss, general_config):\n    train_config = general_config.train_config\n    dictionary = general_config.dictionary\n    nepochs = general_config.nepochs\n    nhops = general_config.nhops\n    batch_size = general_config.batch_size\n    enable_time = general_config.enable_time\n    randomize_time = general_config.randomize_time\n    lrate_decay_step = general_config.lrate_decay_step\n    train_range = general_config.train_range\n    val_range = general_config.val_range\n    train_len = len(train_range)\n    val_len = len(val_range)\n    params = {'lrate': train_config['init_lrate'], 'max_grad_norm': train_config['max_grad_norm']}\n    for ep in range(nepochs):\n        if (((ep + 1) % lrate_decay_step) == 0):\n            params['lrate'] *= 0.5\n        total_err = 0.0\n        total_cost = 0.0\n        total_num = 0\n        for _ in Progress(range(int(math.floor((train_len \/ batch_size))))):\n            batch = train_range[np.random.randint(train_len, size=batch_size)]\n            input_data = np.zeros((train_story.shape[0], batch_size), np.float32)\n            target_data = train_questions[(2, batch)]\n            memory[0].data[:] = dictionary['nil']\n            for b in range(batch_size):\n                d = train_story[:, :(1 + train_questions[(1, batch[b])]), train_questions[(0, batch[b])]]\n                offset = max(0, (d.shape[1] - train_config['sz']))\n                d = d[:, offset:]\n                memory[0].data[:d.shape[0], :d.shape[1], b] = d\n                if enable_time:\n                    if (randomize_time > 0):\n                        nblank = np.random.randint(int(math.ceil((d.shape[1] * randomize_time))))\n                        rt = np.random.permutation((d.shape[1] + nblank))\n                        rt[(rt >= train_config['sz'])] = (train_config['sz'] - 1)\n                        memory[0].data[(- 1), :d.shape[1], b] = (np.sort(rt[:d.shape[1]])[::(- 1)] + len(dictionary))\n                    else:\n                        memory[0].data[(- 1), :d.shape[1], b] = (np.arange(d.shape[1])[::(- 1)] + len(dictionary))\n                input_data[:, b] = train_qstory[:, batch[b]]\n            for i in range(1, nhops):\n                memory[i].data = memory[0].data\n            out = model.fprop(input_data)\n            total_cost += loss.fprop(out, target_data)\n            total_err += loss.get_error(out, target_data)\n            total_num += batch_size\n            grad = loss.bprop(out, target_data)\n            model.bprop(input_data, grad)\n            model.update(params)\n            for i in range(nhops):\n                memory[i].emb_query.weight.D[:, 0] = 0\n        total_val_err = 0.0\n        total_val_cost = 0.0\n        total_val_num = 0\n        for k in range(int(math.floor((val_len \/ batch_size)))):\n            batch = val_range[np.arange((k * batch_size), ((k + 1) * batch_size))]\n            input_data = np.zeros((train_story.shape[0], batch_size), np.float32)\n            target_data = train_questions[(2, batch)]\n            memory[0].data[:] = dictionary['nil']\n            for b in range(batch_size):\n                d = train_story[:, :(1 + train_questions[(1, batch[b])]), train_questions[(0, batch[b])]]\n                offset = max(0, (d.shape[1] - train_config['sz']))\n                d = d[:, offset:]\n                memory[0].data[:d.shape[0], :d.shape[1], b] = d\n                if enable_time:\n                    memory[0].data[(- 1), :d.shape[1], b] = (np.arange(d.shape[1])[::(- 1)] + len(dictionary))\n                input_data[:, b] = train_qstory[:, batch[b]]\n            for i in range(1, nhops):\n                memory[i].data = memory[0].data\n            out = model.fprop(input_data)\n            total_val_cost += loss.fprop(out, target_data)\n            total_val_err += loss.get_error(out, target_data)\n            total_val_num += batch_size\n        train_error = (total_err \/ total_num)\n        val_error = (total_val_err \/ total_val_num)\n        print(('%d | train error: %g | val error: %g' % ((ep + 1), train_error, val_error)))\n"}
{"label_name":"process","label":2,"method_name":"centralized_critic_postprocessing","method":"\n\ndef centralized_critic_postprocessing(policy, sample_batch, other_agent_batches=None, episode=None):\n    pytorch = (policy.config['framework'] == 'torch')\n    if ((pytorch and hasattr(policy, 'compute_central_vf')) or ((not pytorch) and policy.loss_initialized())):\n        assert (other_agent_batches is not None)\n        [(_, opponent_batch)] = list(other_agent_batches.values())\n        sample_batch[OPPONENT_OBS] = opponent_batch[SampleBatch.CUR_OBS]\n        sample_batch[OPPONENT_ACTION] = opponent_batch[SampleBatch.ACTIONS]\n        if args.torch:\n            sample_batch[SampleBatch.VF_PREDS] = policy.compute_central_vf(convert_to_torch_tensor(sample_batch[SampleBatch.CUR_OBS], policy.device), convert_to_torch_tensor(sample_batch[OPPONENT_OBS], policy.device), convert_to_torch_tensor(sample_batch[OPPONENT_ACTION], policy.device)).cpu().detach().numpy()\n        else:\n            sample_batch[SampleBatch.VF_PREDS] = policy.compute_central_vf(sample_batch[SampleBatch.CUR_OBS], sample_batch[OPPONENT_OBS], sample_batch[OPPONENT_ACTION])\n    else:\n        sample_batch[OPPONENT_OBS] = np.zeros_like(sample_batch[SampleBatch.CUR_OBS])\n        sample_batch[OPPONENT_ACTION] = np.zeros_like(sample_batch[SampleBatch.ACTIONS])\n        sample_batch[SampleBatch.VF_PREDS] = np.zeros_like(sample_batch[SampleBatch.REWARDS], dtype=np.float32)\n    completed = sample_batch['dones'][(- 1)]\n    if completed:\n        last_r = 0.0\n    else:\n        last_r = sample_batch[SampleBatch.VF_PREDS][(- 1)]\n    train_batch = compute_advantages(sample_batch, last_r, policy.config['gamma'], policy.config['lambda'], use_gae=policy.config['use_gae'])\n    return train_batch\n"}
{"label_name":"train","label":0,"method_name":"validate_dependency_constraints","method":"\n\ndef validate_dependency_constraints(N, Cd, Ci):\n    'Validates Cd and Ci constraints on N columns.'\n    if (N is None):\n        N = 10000000000.0\n    counts = {}\n    for block in Cd:\n        if (len(block) == 1):\n            raise ValueError('Single customer in dependency constraint.')\n        for col in block:\n            if (N <= col):\n                raise ValueError('Dependence customer out of range.')\n            if (col not in counts):\n                counts[col] = 0\n            counts[col] += 1\n            if (counts[col] > 1):\n                raise ValueError('Multiple customer dependencies.')\n        for pair in Ci:\n            if ((pair[0] in block) and (pair[1] in block)):\n                raise ValueError('Contradictory customer independence.')\n    for pair in Ci:\n        if (len(pair) != 2):\n            raise ValueError('Independencies require two customers.')\n        if ((N <= pair[0]) or (N <= pair[1])):\n            raise ValueError('Independence customer of out range.')\n        if (pair[0] == pair[1]):\n            raise ValueError('Independency specified for same customer.')\n    return True\n"}
{"label_name":"predict","label":4,"method_name":"action_predictions_remove","method":"\n\ndef action_predictions_remove(label):\n    '\\n    try to remove the prediction result with the label used as argument\\n    returns \\n        - (False, message) if it there is no directory or the removal failed \\n        - (True, OK) removal succeeded\\n    '\n    predictions_path = pathlib.Path(utils.predictions_repository_path())\n    label_path = predictions_path.joinpath(label)\n    if (not os.path.isdir(label_path)):\n        return (False, f'directory {label_path} not found')\n    try:\n        shutil.rmtree(label_path)\n    except Exception as e:\n        return (False, f'failed to remove {label_path} with error: {e}')\n    return (True, 'OK')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(model, generated_image, initial_image):\n    \" Train your model.\\n    Don't forget to create folders for checkpoints and outputs.\\n    \"\n    skip_step = 1\n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        sess.run(generated_image.assign(initial_image))\n        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints\/checkpoint'))\n        if (ckpt and ckpt.model_checkpoint_path):\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        initial_step = model['global_step'].eval()\n        start_time = time.time()\n        for index in range(initial_step, ITERS):\n            if ((index >= 5) and (index < 20)):\n                skip_step = 10\n            elif (index >= 20):\n                skip_step = 20\n            sess.run(model['optimizer'])\n            if (((index + 1) % skip_step) == 0):\n                gen_image = (gen_image + MEAN_PIXELS)\n                writer.add_summary(summary, global_step=index)\n                print('Step {}\\n   Sum: {:5.1f}'.format((index + 1), np.sum(gen_image)))\n                print('   Loss: {:5.1f}'.format(total_loss))\n                print('   Time: {}'.format((time.time() - start_time)))\n                start_time = time.time()\n                filename = ('outputs\/%d.png' % index)\n                utils.save_image(filename, gen_image)\n                if (((index + 1) % SAVE_EVERY) == 0):\n                    saver.save(sess, 'checkpoints\/style_transfer', index)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n    print(('Training with %s neurons, alpha:%s, dropout:%s %s' % (hidden_neurons, str(alpha), dropout, (dropout_percent if dropout else ''))))\n    print(('Input matrix: %sx%s    Output matrix: %sx%s' % (len(X), len(X[0]), 1, len(classes))))\n    np.random.seed(1)\n    last_mean_error = 1\n    synapse_0 = ((2 * np.random.random((len(X[0]), hidden_neurons))) - 1)\n    synapse_1 = ((2 * np.random.random((hidden_neurons, len(classes)))) - 1)\n    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n    synapse_0_direction_count = np.zeros_like(synapse_0)\n    synapse_1_direction_count = np.zeros_like(synapse_1)\n    for j in iter(range((epochs + 1))):\n        layer_0 = X\n        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n        if dropout:\n            layer_1 *= (np.random.binomial([np.ones((len(X), hidden_neurons))], (1 - dropout_percent))[0] * (1.0 \/ (1 - dropout_percent)))\n        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n        layer_2_error = (y - layer_2)\n        if (((j % 100) == 0) and (j > 50)):\n            if (np.mean(np.abs(layer_2_error)) < last_mean_error):\n                print(((('delta after ' + str(j)) + ' iterations:') + str(np.mean(np.abs(layer_2_error)))))\n                last_mean_error = np.mean(np.abs(layer_2_error))\n            else:\n                print('break:', np.mean(np.abs(layer_2_error)), '>', last_mean_error)\n        layer_2_delta = (layer_2_error * sigmoid_output_to_derivative(layer_2))\n        layer_1_error = layer_2_delta.dot(synapse_1.T)\n        layer_1_delta = (layer_1_error * sigmoid_output_to_derivative(layer_1))\n        synapse_1_weight_update = layer_1.T.dot(layer_2_delta)\n        synapse_0_weight_update = layer_0.T.dot(layer_1_delta)\n        if (j > 0):\n            synapse_0_direction_count += np.abs((((synapse_0_weight_update > 0) + 0) - ((prev_synapse_0_weight_update > 0) + 0)))\n            synapse_1_direction_count += np.abs((((synapse_1_weight_update > 0) + 0) - ((prev_synapse_1_weight_update > 0) + 0)))\n        synapse_1 += (alpha * synapse_1_weight_update)\n        synapse_0 += (alpha * synapse_0_weight_update)\n        prev_synapse_0_weight_update = synapse_0_weight_update\n        prev_synapse_1_weight_update = synapse_1_weight_update\n    now = datetime.datetime.now()\n    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(), 'datetime': now.strftime('%Y-%m-%d %H:%M'), 'words': words, 'classes': classes}\n    synapse_file = '.\/include\/synapsesOrder.json'\n    synapse_file2 = (((('.\/include\/synapses' + str(hidden_neurons)) + '_') + str(alpha)) + '.json')\n    with open(synapse_file, 'w') as outfile:\n        json.dump(synapse, outfile, indent=4, sort_keys=True)\n    with open(synapse_file2, 'w') as outfile:\n        json.dump(synapse, outfile, indent=4, sort_keys=True)\n    print('saved synapses to:', synapse_file)\n    return last_mean_error\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(args):\n    (_, num_workers, rank, local_rank, is_master_node, ctx_l) = init_comm(args.comm_backend, args.gpus)\n    level = (logging.DEBUG if args.verbose else logging.INFO)\n    logging_config(args.ckpt_dir, name=('pretrain_bert_' + str(rank)), level=level, console=(local_rank == 0))\n    logging.info(args)\n    logging.debug('Random seed set to {}'.format(args.seed))\n    set_seed(args.seed)\n    logging.info('Training info: num_buckets: {}, num_workers: {}, rank: {}'.format(args.num_buckets, num_workers, rank))\n    (cfg, tokenizer, model) = get_pretraining_model(args.model_name, ctx_l)\n    if args.start_step:\n        logging.info('Restart training from {}'.format(args.start_step))\n        parameters_option(args.start_step, model, args.ckpt_dir, 'Loading', ctx_l)\n    else:\n        model.initialize(ctx=ctx_l)\n    model.hybridize()\n    if args.raw:\n        get_dataset_fn = functools.partial(get_pretrain_data_text, max_seq_length=args.max_seq_length, short_seq_prob=args.short_seq_prob, masked_lm_prob=args.masked_lm_prob, max_predictions_per_seq=args.max_predictions_per_seq, whole_word_mask=args.whole_word_mask, random_next_sentence=args.random_next_sentence, tokenizer=tokenizer, circle_length=args.circle_length, repeat=args.repeat, dataset_cached=args.dataset_cached, num_max_dataset_cached=args.num_max_dataset_cached)\n    else:\n        get_dataset_fn = get_pretrain_data_npz\n    if args.data_dir:\n        tmp = []\n        for dataset in args.data_dir.split(','):\n            names = os.listdir(dataset)\n            for i in range(len(names)):\n                names[i] = os.path.join(dataset, names[i])\n            tmp.append(','.join(names))\n        args.data = ','.join(tmp)\n    data_train = get_dataset_fn(args.data, args.batch_size, shuffle=True, num_buckets=args.num_buckets, vocab=tokenizer.vocab, num_parts=num_workers, part_idx=rank, num_dataset_workers=args.num_dataset_workers, num_batch_workers=args.num_batch_workers)\n    param_dict = model.collect_params()\n    for (_, v) in model.collect_params('.*beta|.*gamma|.*bias').items():\n        v.wd_mult = 0.0\n    params = [p for p in param_dict.values() if (p.grad_req != 'null')]\n    num_accumulated = args.num_accumulated\n    if (num_accumulated > 1):\n        logging.info('Using gradient accumulation. Effective global batch size = {}'.format((((num_accumulated * args.batch_size) * len(ctx_l)) * num_workers)))\n        for p in params:\n            p.grad_req = 'add'\n    num_steps = args.num_steps\n    warmup_steps = int((num_steps * args.warmup_ratio))\n    log_interval = args.log_interval\n    save_interval = args.ckpt_interval\n    logging.info('#Total Training Steps={}, Warmup Steps={}, Save Interval={}'.format(num_steps, warmup_steps, save_interval))\n    optimizer_params = {'learning_rate': args.lr, 'wd': args.wd}\n    if (args.optimizer == 'adamw'):\n        optimizer_params.update({'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-06, 'correct_bias': False})\n    if args.use_amp:\n        optimizer_params.update({'multi_precision': True})\n    if (args.comm_backend == 'horovod'):\n        trainer = hvd.DistributedTrainer(param_dict, args.optimizer, optimizer_params)\n    elif (args.comm_backend == 'byteps'):\n        trainer = bps.DistributedTrainer(param_dict, args.optimizer, optimizer_params)\n    else:\n        trainer = mx.gluon.Trainer(param_dict, args.optimizer, optimizer_params, update_on_kvstore=False)\n    if args.start_step:\n        logging.info('Restart training from {}'.format(args.start_step))\n        states_option(args.start_step, trainer, args.ckpt_dir, local_rank, 'Loading')\n    if args.use_amp:\n        amp.init_trainer(trainer)\n    if (args.comm_backend == 'byteps'):\n        trainer._init_params()\n    if (args.comm_backend == 'horovod'):\n        hvd.broadcast_parameters(param_dict, root_rank=0)\n    nsp_loss_fn = mx.gluon.loss.SoftmaxCELoss()\n    mlm_loss_fn = mx.gluon.loss.SoftmaxCELoss()\n    nsp_loss_fn.hybridize()\n    mlm_loss_fn.hybridize()\n    mlm_metric = MaskedAccuracy()\n    nsp_metric = MaskedAccuracy()\n    mlm_metric.reset()\n    nsp_metric.reset()\n    step_num = args.start_step\n    if args.phase2:\n        step_num -= args.phase1_num_steps\n    (running_mlm_loss, running_nsp_loss) = (0.0, 0.0)\n    running_num_tks = 0\n    train_start_time = time.time()\n    tic = time.time()\n    train_loop_dataloader = grouper(repeat(data_train), len(ctx_l))\n    while (step_num < num_steps):\n        step_num += 1\n        for _ in range(num_accumulated):\n            sample_l = next(train_loop_dataloader)\n            mlm_loss_l = []\n            nsp_loss_l = []\n            loss_l = []\n            (ns_label_list, ns_pred_list) = ([], [])\n            (mask_label_list, mask_pred_list, mask_weight_list) = ([], [], [])\n            for (sample, ctx) in zip(sample_l, ctx_l):\n                (input_id, masked_id, masked_position, masked_weight, next_sentence_label, segment_id, valid_length) = sample\n                input_id = input_id.as_in_ctx(ctx)\n                masked_id = masked_id.as_in_ctx(ctx)\n                masked_position = masked_position.as_in_ctx(ctx)\n                masked_weight = masked_weight.as_in_ctx(ctx)\n                next_sentence_label = next_sentence_label.as_in_ctx(ctx)\n                segment_id = segment_id.as_in_ctx(ctx)\n                valid_length = valid_length.as_in_ctx(ctx)\n                with mx.autograd.record():\n                    (_, _, nsp_score, mlm_scores) = model(input_id, segment_id, valid_length, masked_position)\n                    denominator = (((masked_weight.sum() + 1e-08) * num_accumulated) * len(ctx_l))\n                    mlm_scores_r = mx.npx.reshape(mlm_scores, ((- 5), (- 1)))\n                    masked_id_r = masked_id.reshape(((- 1),))\n                    mlm_loss = (mlm_loss_fn(mlm_scores_r, masked_id_r, masked_weight.reshape(((- 1), 1))).sum() \/ denominator)\n                    denominator = (num_accumulated * len(ctx_l))\n                    nsp_loss = (nsp_loss_fn(nsp_score, next_sentence_label).mean() \/ denominator)\n                    mlm_loss_l.append(mlm_loss)\n                    nsp_loss_l.append(nsp_loss)\n                    loss_l.append((mlm_loss + nsp_loss))\n                    mask_label_list.append(masked_id_r)\n                    mask_pred_list.append(mlm_scores_r)\n                    mask_weight_list.append(masked_weight.reshape(((- 1),)))\n                    ns_label_list.append(next_sentence_label)\n                    ns_pred_list.append(nsp_score)\n                running_num_tks += valid_length.sum().as_in_ctx(mx.cpu())\n            if args.use_amp:\n                with mx.autograd.record():\n                    with amp.scale_loss(loss_l, trainer) as loss_l:\n                        for loss in loss_l:\n                            loss.backward()\n                norm_clip_mult = (num_workers * trainer.amp_loss_scale)\n            else:\n                with mx.autograd.record():\n                    for loss in loss_l:\n                        loss.backward()\n                norm_clip_mult = num_workers\n            running_mlm_loss += sum([ele.as_in_ctx(mx.cpu()) for ele in mlm_loss_l]).asnumpy().item()\n            running_nsp_loss += sum([ele.as_in_ctx(mx.cpu()) for ele in nsp_loss_l]).asnumpy().item()\n            mlm_metric.update(mask_label_list, mask_pred_list, mask_weight_list)\n            nsp_metric.update(ns_label_list, ns_pred_list)\n        trainer.allreduce_grads()\n        (total_norm, ratio, is_finite) = clip_grad_global_norm(params, (args.max_grad_norm * norm_clip_mult))\n        total_norm = (total_norm \/ norm_clip_mult)\n        scheduled_lr = args.lr\n        if (step_num <= warmup_steps):\n            scheduled_lr *= (step_num \/ warmup_steps)\n        else:\n            offset = ((num_steps - step_num) \/ (num_steps - warmup_steps))\n            scheduled_lr *= max(offset, 0)\n        trainer.set_learning_rate(scheduled_lr)\n        if ((args.comm_backend == 'horovod') or (args.comm_backend == 'byteps')):\n            trainer.update(1, ignore_stale_grad=True)\n        else:\n            trainer.update(num_workers, ignore_stale_grad=True)\n        if (num_accumulated > 1):\n            model.zero_grad()\n        if (((step_num % save_interval) == 0) or (step_num >= num_steps)):\n            states_option(step_num, trainer, args.ckpt_dir, local_rank, 'Saving')\n            if (local_rank == 0):\n                parameters_option(step_num, model, args.ckpt_dir, 'Saving')\n        if ((step_num % log_interval) == 0):\n            running_mlm_loss \/= log_interval\n            running_nsp_loss \/= log_interval\n            toc = time.time()\n            logging.info('[step {}], Loss mlm\/nsp={:.5f}\/{:.3f}, Acc mlm\/nsp={:.3f}\/{:.3f},  LR={:.7f}, grad_norm={:.4f}. Time cost={:.2f} s, Throughput={:.1f}K tks\/s, ETA={:.2f} h'.format(step_num, running_mlm_loss, running_nsp_loss, mlm_metric.get()[1], nsp_metric.get()[1], trainer.learning_rate, total_norm, (toc - tic), ((running_num_tks.asnumpy().item() \/ (toc - tic)) \/ 1000), (((num_steps - step_num) \/ (step_num \/ (toc - train_start_time))) \/ 3600)))\n            mlm_metric.reset()\n            nsp_metric.reset()\n            tic = time.time()\n            running_mlm_loss = 0\n            running_nsp_loss = 0\n            running_num_tks = 0\n    logging.info('Finish training step: %d', step_num)\n    mx.npx.waitall()\n    train_end_time = time.time()\n    logging.info('Train cost={:.1f} s'.format((train_end_time - train_start_time)))\n    if (local_rank == 0):\n        model_name = args.model_name.replace('google', 'gluon')\n        save_dir = os.path.join(args.ckpt_dir, model_name)\n        final_save(model, save_dir, tokenizer, cfg)\n"}
{"label_name":"predict","label":4,"method_name":"get_predictions","method":"\n\ndef get_predictions(model, texts):\n    docs = [model.tokenizer(text) for text in texts]\n    textcat = model.get_pipe('textcat')\n    (scores, _) = textcat.predict(docs)\n    predicted_labels = scores.argmax(axis=1)\n    predicted_class = [textcat.labels[label] for label in predicted_labels]\n    return predicted_class\n"}
{"label_name":"process","label":2,"method_name":"_process_shot_sequence","method":"\n\ndef _process_shot_sequence(shot_list):\n    'Process the shot sequence, to determine the total\\n    number of shots and the shot vector.\\n\\n    Args:\\n        shot_list (Sequence[int, tuple[int]]): sequence of non-negative shot integers\\n\\n    Returns:\\n        tuple[int, list[.ShotTuple[int]]]: A tuple containing the total number\\n        of shots, as well as a list of shot tuples.\\n\\n    **Example**\\n\\n    >>> shot_list = [3, 1, 2, 2, 2, 2, 6, 1, 1, 5, 12, 10, 10]\\n    >>> _process_shot_sequence(shot_list)\\n    (57,\\n     [ShotTuple(shots=3, copies=1),\\n      ShotTuple(shots=1, copies=1),\\n      ShotTuple(shots=2, copies=4),\\n      ShotTuple(shots=6, copies=1),\\n      ShotTuple(shots=1, copies=2),\\n      ShotTuple(shots=5, copies=1),\\n      ShotTuple(shots=12, copies=1),\\n      ShotTuple(shots=10, copies=2)])\\n\\n    The total number of shots (57), and a sparse representation of the shot\\n    sequence is returned, where tuples indicate the number of times a shot\\n    integer is repeated.\\n    '\n    if all((isinstance(s, int) for s in shot_list)):\n        if (len(set(shot_list)) == 1):\n            shot_vector = [ShotTuple(shots=shot_list[0], copies=len(shot_list))]\n        else:\n            split_at_repeated = np.split(shot_list, (np.diff(shot_list).nonzero()[0] + 1))\n            shot_vector = [ShotTuple(shots=i[0], copies=len(i)) for i in split_at_repeated]\n    elif all((isinstance(s, (int, tuple)) for s in shot_list)):\n        shot_vector = [(ShotTuple(*i) if isinstance(i, tuple) else ShotTuple(i, 1)) for i in shot_list]\n    else:\n        raise ValueError(f'Unknown shot sequence format {shot_list}')\n    total_shots = np.sum(np.prod(shot_vector, axis=1))\n    return (total_shots, shot_vector)\n"}
{"label_name":"process","label":2,"method_name":"image_preprocessing","method":"\n\ndef image_preprocessing(image_buffer, bbox, train, thread_id=0):\n    'Decode and preprocess one image for evaluation or training.\\n\\n  Args:\\n    image_buffer: JPEG encoded string Tensor\\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\\n      where each coordinate is [0, 1) and the coordinates are arranged as\\n      [ymin, xmin, ymax, xmax].\\n    train: boolean\\n    thread_id: integer indicating preprocessing thread\\n\\n  Returns:\\n    3-D float Tensor containing an appropriately scaled image\\n\\n  Raises:\\n    ValueError: if user does not provide bounding box\\n  '\n    if (bbox is None):\n        raise ValueError('Please supply a bounding box.')\n    image = decode_jpeg(image_buffer)\n    height = FLAGS.image_size\n    width = FLAGS.image_size\n    if train:\n        image = distort_image(image, height, width, bbox, thread_id)\n    else:\n        image = eval_image(image, height, width)\n    image = tf.sub(image, 0.5)\n    image = tf.mul(image, 2.0)\n    return image\n"}
{"label_name":"process","label":2,"method_name":"_process_dataset","method":"\n\ndef _process_dataset(name, directory, num_shards, labels_file):\n    'Process a complete data set and save it as a TFRecord.\\n\\n  Args:\\n    name: string, unique identifier specifying the data set.\\n    directory: string, root path to the data set.\\n    num_shards: integer number of shards for this data set.\\n    labels_file: string, path to the labels file.\\n  '\n    (filenames, texts, labels) = _find_image_files(directory, labels_file)\n    _process_image_files(name, filenames, texts, labels, num_shards)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(total_loss, global_step):\n    'Train CIFAR-10 model.\\n\\n  Create an optimizer and apply to all trainable variables. Add moving\\n  average for all trainable variables.\\n\\n  Args:\\n    total_loss: Total loss from loss().\\n    global_step: Integer Variable counting the number of training steps\\n      processed.\\n  Returns:\\n    train_op: op for training.\\n  '\n    num_batches_per_epoch = (NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN \/ FLAGS.batch_size)\n    decay_steps = int((num_batches_per_epoch * NUM_EPOCHS_PER_DECAY))\n    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, decay_steps, LEARNING_RATE_DECAY_FACTOR, staircase=True)\n    tf.summary.scalar('learning_rate', lr)\n    loss_averages_op = _add_loss_summaries(total_loss)\n    with tf.control_dependencies([loss_averages_op]):\n        opt = tf.train.GradientDescentOptimizer(lr)\n        grads = opt.compute_gradients(total_loss)\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n    for var in tf.trainable_variables():\n        tf.summary.histogram(var.op.name, var)\n    for (grad, var) in grads:\n        if (grad is not None):\n            tf.summary.histogram((var.op.name + '\/gradients'), grad)\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n        train_op = tf.no_op(name='train')\n    return train_op\n"}
{"label_name":"save","label":1,"method_name":"save_dict","method":"\n\ndef save_dict(dict_data, dict_path):\n    with open(dict_path, 'w', encoding='utf-8') as f:\n        count = 0\n        for (k, v) in dict_data.items():\n            v_str = map(str, v)\n            f.write((((k + '\\t') + ' '.join(v_str)) + '\\n'))\n            count += 1\n        print('save dict size:', count, 'dict_path:', dict_path)\n"}
{"label_name":"predict","label":4,"method_name":"predict_dygraph","method":"\n\ndef predict_dygraph(args, batch_generator):\n    with fluid.dygraph.guard(place):\n        paddle.seed(SEED)\n        paddle.framework.random._manual_program_seed(SEED)\n        test_loader = fluid.io.DataLoader.from_generator(capacity=10)\n        test_loader.set_batch_generator(batch_generator, places=place)\n        transformer = Transformer(args.src_vocab_size, args.trg_vocab_size, (args.max_length + 1), args.n_layer, args.n_head, args.d_key, args.d_value, args.d_model, args.d_inner_hid, args.prepostprocess_dropout, args.attention_dropout, args.relu_dropout, args.preprocess_cmd, args.postprocess_cmd, args.weight_sharing, args.bos_idx, args.eos_idx)\n        (model_dict, _) = util.load_dygraph(os.path.join(args.save_dygraph_model_path, 'transformer'))\n        model_dict['encoder.pos_encoder.weight'] = position_encoding_init((args.max_length + 1), args.d_model)\n        model_dict['decoder.pos_encoder.weight'] = position_encoding_init((args.max_length + 1), args.d_model)\n        transformer.load_dict(model_dict)\n        transformer.eval()\n        step_idx = 0\n        speed_list = []\n        for input_data in test_loader():\n            (src_word, src_pos, src_slf_attn_bias, trg_word, trg_src_attn_bias) = input_data\n            (seq_ids, seq_scores) = transformer.beam_search(src_word, src_pos, src_slf_attn_bias, trg_word, trg_src_attn_bias, bos_id=args.bos_idx, eos_id=args.eos_idx, beam_size=args.beam_size, max_len=args.max_out_len)\n            seq_ids = seq_ids.numpy()\n            seq_scores = seq_scores.numpy()\n            if ((step_idx % args.print_step) == 0):\n                if (step_idx == 0):\n                    logging.info(('Dygraph Predict: step_idx: %d, 1st seq_id: %d, 1st seq_score: %.2f' % (step_idx, seq_ids[0][0][0], seq_scores[0][0])))\n                    avg_batch_time = time.time()\n                else:\n                    speed = (args.print_step \/ (time.time() - avg_batch_time))\n                    speed_list.append(speed)\n                    logging.info(('Dygraph Predict: step_idx: %d, 1st seq_id: %d, 1st seq_score: %.2f, speed: %.3f steps\/s' % (step_idx, seq_ids[0][0][0], seq_scores[0][0], speed)))\n                    avg_batch_time = time.time()\n            step_idx += 1\n            if (step_idx == STEP_NUM):\n                break\n        logging.info(('Dygraph Predict:  avg_speed: %.4f steps\/s' % np.mean(speed_list)))\n        return (seq_ids, seq_scores)\n"}
{"label_name":"train","label":0,"method_name":"count_number_trainable_params","method":"\n\ndef count_number_trainable_params():\n    tot_nb_params = 0\n    for trainable_variable in slim.get_trainable_variables():\n        print(trainable_variable.name, trainable_variable.shape)\n        shape = trainable_variable.get_shape()\n        current_nb_params = get_nb_params_shape(shape)\n        tot_nb_params = (tot_nb_params + current_nb_params)\n    print('Total number of trainable params: ', tot_nb_params)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(image_input, roi_input, dims_input, loss, pred_error, lr_per_sample, mm_schedule, l2_reg_weight, epochs_to_train, rpn_rois_input=None, buffered_rpn_proposals=None):\n    if isinstance(loss, cntk.Variable):\n        loss = combine([loss])\n    params = loss.parameters\n    biases = [p for p in params if (('.b' in p.name) or ('b' == p.name))]\n    others = [p for p in params if (not (p in biases))]\n    bias_lr_mult = cfg['CNTK'].BIAS_LR_MULT\n    if cfg['CNTK'].DEBUG_OUTPUT:\n        print('biases')\n        for p in biases:\n            print(p)\n        print('others')\n        for p in others:\n            print(p)\n        print('bias_lr_mult: {}'.format(bias_lr_mult))\n    lr_schedule = learning_rate_schedule(lr_per_sample, unit=UnitType.sample)\n    learner = momentum_sgd(others, lr_schedule, mm_schedule, l2_regularization_weight=l2_reg_weight, unit_gain=False, use_mean_gradient=cfg['CNTK'].USE_MEAN_GRADIENT)\n    bias_lr_per_sample = [(v * bias_lr_mult) for v in lr_per_sample]\n    bias_lr_schedule = learning_rate_schedule(bias_lr_per_sample, unit=UnitType.sample)\n    bias_learner = momentum_sgd(biases, bias_lr_schedule, mm_schedule, l2_regularization_weight=l2_reg_weight, unit_gain=False, use_mean_gradient=cfg['CNTK'].USE_MEAN_GRADIENT)\n    trainer = Trainer(None, (loss, pred_error), [learner, bias_learner])\n    print(('Training model for %s epochs.' % epochs_to_train))\n    log_number_of_parameters(loss)\n    od_minibatch_source = ObjectDetectionMinibatchSource(globalvars['train_map_file'], globalvars['train_roi_file'], max_annotations_per_image=cfg['CNTK'].INPUT_ROIS_PER_IMAGE, pad_width=image_width, pad_height=image_height, pad_value=img_pad_value, randomize=True, use_flipping=cfg['TRAIN'].USE_FLIPPED, max_images=cfg['CNTK'].NUM_TRAIN_IMAGES, buffered_rpn_proposals=buffered_rpn_proposals)\n    input_map = {od_minibatch_source.image_si: image_input, od_minibatch_source.roi_si: roi_input, od_minibatch_source.dims_si: dims_input}\n    use_buffered_proposals = (buffered_rpn_proposals is not None)\n    progress_printer = ProgressPrinter(tag='Training', num_epochs=epochs_to_train, gen_heartbeat=True)\n    for epoch in range(epochs_to_train):\n        sample_count = 0\n        while (sample_count < epoch_size):\n            (data, proposals) = od_minibatch_source.next_minibatch_with_proposals(min(mb_size, (epoch_size - sample_count)), input_map=input_map)\n            if use_buffered_proposals:\n                data[rpn_rois_input] = MinibatchData(Value(batch=np.asarray(proposals, dtype=np.float32)), 1, 1, False)\n                del data[[k for k in data if ('[6]' in str(k))][0]]\n            trainer.train_minibatch(data)\n            sample_count += trainer.previous_minibatch_sample_count\n            progress_printer.update_with_trainer(trainer, with_metric=True)\n            if ((sample_count % 100) == 0):\n                print('Processed {} samples'.format(sample_count))\n        progress_printer.epoch_summary(with_metric=True)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(args):\n    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n    args.vocab_size = data_loader.vocab_size\n    if (args.init_from is not None):\n        assert os.path.isdir(args.init_from), (' %s must be a a path' % args.init_from)\n        assert os.path.isfile(os.path.join(args.init_from, 'config.pkl')), ('config.pkl file does not exist in path %s' % args.init_from)\n        assert os.path.isfile(os.path.join(args.init_from, 'chars_vocab.pkl')), ('chars_vocab.pkl.pkl file does not exist in path %s' % args.init_from)\n        ckpt = tf.train.get_checkpoint_state(args.init_from)\n        assert ckpt, 'No checkpoint found'\n        assert ckpt.model_checkpoint_path, 'No model path found in checkpoint'\n        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n            saved_model_args = cPickle.load(f)\n        need_be_same = ['model', 'rnn_size', 'num_layers', 'seq_length']\n        for checkme in need_be_same:\n            assert (vars(saved_model_args)[checkme] == vars(args)[checkme]), (\"Command line argument and saved model disagree on '%s' \" % checkme)\n        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n            (saved_chars, saved_vocab) = cPickle.load(f)\n        assert (saved_chars == data_loader.chars), 'Data and loaded model disagree on character set!'\n        assert (saved_vocab == data_loader.vocab), 'Data and loaded model disagree on dictionary mappings!'\n    save_dir = os.path.join(args.data_dir, 'save')\n    if (not os.path.isdir(save_dir)):\n        os.makedirs(save_dir)\n    with open(os.path.join(save_dir, 'config.pkl'), 'wb') as f:\n        cPickle.dump(args, f)\n    with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'wb') as f:\n        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n    model = Model(args)\n    with tf.Session() as sess:\n        summaries = tf.summary.merge_all()\n        writer = tf.summary.FileWriter(os.path.join(os.path.join(args.data_dir, 'logs'), time.strftime('%Y-%m-%d-%H-%M-%S')))\n        writer.add_graph(sess.graph)\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver(tf.global_variables())\n        if (args.init_from is not None):\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        for e in range(args.num_epochs):\n            sess.run(tf.assign(model.lr, (args.learning_rate * (args.decay_rate ** e))))\n            data_loader.reset_batch_pointer()\n            state = sess.run(model.initial_state)\n            for b in range(data_loader.num_batches):\n                start = time.time()\n                (x, y) = data_loader.next_batch()\n                feed = {model.input_data: x, model.targets: y}\n                for (i, (c, h)) in enumerate(model.initial_state):\n                    feed[c] = state[i].c\n                    feed[h] = state[i].h\n                (summ, train_loss, state, _) = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)\n                writer.add_summary(summ, ((e * data_loader.num_batches) + b))\n                end = time.time()\n                print('{}\/{} (epoch {}), train_loss = {:.3f}, time\/batch = {:.3f}'.format(((e * data_loader.num_batches) + b), (args.num_epochs * data_loader.num_batches), e, train_loss, (end - start)))\n                if (((((e * data_loader.num_batches) + b) % args.save_every) == 0) or ((e == (args.num_epochs - 1)) and (b == (data_loader.num_batches - 1)))):\n                    checkpoint_path = os.path.join(args.data_dir, 'save\/', 'model.ckpt')\n                    saver.save(sess, checkpoint_path, global_step=((e * data_loader.num_batches) + b))\n                    print('model saved to {}'.format(checkpoint_path))\n"}
{"label_name":"predict","label":4,"method_name":"predictOneEstimator","method":"\n\ndef predictOneEstimator(num=0, validCosts=validCosts, clfs=clfs, X=X, y=y, dataTest=dataTest):\n    [no_runs, no_ests, no_vers] = validCosts.shape\n    costs = validCosts[:, num, :]\n    costs = np.reshape(costs, (no_runs, no_vers))\n    costsMean = np.mean(costs, axis=0)\n    costsStd = np.std(costs, axis=0)\n    costsDecide = (costsMean * costsStd)\n    clf_best = clfs[num][np.argmin(np.abs(costsDecide))]\n    estTrained = clf_best\n    estTrained = estTrained.fit(X[np.argmin(np.abs(costsDecide))], y)\n    dataTest_optVer = dataTest[np.argmin(np.abs(costsDecide))]\n    noBins = numsBins[np.argmin(np.abs(costsDecide))]\n    y_pred = estTrained.predict(dataTest_optVer)\n    saveresult(y_pred, estimatorsStr[num])\n    logging.info('{}'.format(estTrained))\n    logging.info('No. bins = {}'.format(noBins))\n    plotPerformance(estimatorsStr, costs, numsBins, num=num)\n    return (y_pred, estTrained)\n"}
{"label_name":"predict","label":4,"method_name":"predict_paid_or_unpaid","method":"\n\ndef predict_paid_or_unpaid(years_experience):\n    if (years_experience < 3.0):\n        return 'paid'\n    elif (years_experience < 8.5):\n        return 'unpaid'\n    else:\n        return 'paid'\n"}
{"label_name":"save","label":1,"method_name":"save_results","method":"\n\ndef save_results(sess, folder, name, results_rows=None):\n    if (results_rows is not None):\n        df = pd.DataFrame(results_rows)\n        df.to_csv('{}_results.csv'.format(folder), index=False)\n    model_saver = tf.train.Saver()\n    ckpt_dir = os.path.join(params['CHK_PATH'], folder)\n    if (not os.path.exists(ckpt_dir)):\n        os.makedirs(ckpt_dir)\n    ckpt_file = os.path.join(ckpt_dir, '{}.ckpt'.format(name))\n    path = model_saver.save(sess, ckpt_file)\n    print('Model saved at {}'.format(path))\n    return\n"}
{"label_name":"train","label":0,"method_name":"train_one_safe","method":"\n\ndef train_one_safe(data, loss, C, verbose, max_iter, threshold, dual, tol):\n\n    def _get_features(obj):\n        if (obj['ind'] is None):\n            return obj['data']\n        else:\n            return obj['data'].take(obj['ind'], axis=0)\n    (X, y) = (_get_features(data), data['Y'])\n    clf = LinearSVC(tol=tol, loss=loss, dual=dual, C=C, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=verbose, random_state=0, max_iter=max_iter)\n    try:\n        clf.fit(X, y)\n        (weight, bias) = (clf.coef_, clf.intercept_)\n    except ValueError:\n        (weight, bias) = (np.zeros((1, X.shape[1]), dtype=np.float32), np.zeros(1, dtype=np.float32))\n    del clf\n    apply_threshold(weight, threshold)\n    return (weight, bias)\n"}
{"label_name":"predict","label":4,"method_name":"create_masked_lm_predictions","method":"\n\ndef create_masked_lm_predictions(*, args, tokens, cls_token_id, sep_token_id, mask_token_id, non_special_ids):\n    'Creates the predictions for the masked LM objective.'\n    cand_indexes = [i for (i, tok) in enumerate(tokens) if (tok not in (cls_token_id, sep_token_id))]\n    output_tokens = list(tokens)\n    random.shuffle(cand_indexes)\n    num_to_predict = min(args.max_predictions_per_seq, max(1, int(round((len(tokens) * args.masked_lm_prob)))))\n    mlm_positions = []\n    mlm_labels = []\n    covered_indexes = set()\n    for index in cand_indexes:\n        if (len(mlm_positions) >= num_to_predict):\n            break\n        if (index in covered_indexes):\n            continue\n        covered_indexes.add(index)\n        masked_token = None\n        if (random.random() < 0.8):\n            masked_token = mask_token_id\n        elif (random.random() < 0.5):\n            masked_token = tokens[index]\n        else:\n            masked_token = random.choice(non_special_ids)\n        output_tokens[index] = masked_token\n        mlm_positions.append(index)\n        mlm_labels.append(tokens[index])\n    assert (len(mlm_positions) <= num_to_predict)\n    assert (len(mlm_positions) == len(mlm_labels))\n    return (output_tokens, mlm_positions, mlm_labels)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(data, theta):\n    predictions = []\n    count = 1\n    for row in data:\n        hypothesis = []\n        multiclass_ans = ([0] * len(theta))\n        for c in range(len(theta)):\n            z = ThetaTX(row, theta[c])\n            hypothesis.append(sigmoid(z))\n        index = hypothesis.index(max(hypothesis))\n        multiclass_ans[index] = 1\n        predictions.append(multiclass_ans)\n        count += 1\n    return predictions\n"}
{"label_name":"predict","label":4,"method_name":"predict_fn","method":"\n\ndef predict_fn(classifier, data, batchsize):\n    ' Return features from classifier '\n    out = np.zeros((len(data), RESNET_FEATURES), np.float32)\n    for (idx, dta) in yield_mb_X(data, batchsize):\n        out[(idx * batchsize):((idx + 1) * batchsize)] = classifier.predict_on_batch(dta).squeeze()\n    return out\n"}
{"label_name":"process","label":2,"method_name":"process_user_behaviors","method":"\n\ndef process_user_behaviors(data_path='data\/behavior_info.txt'):\n    with open(data_path, 'r') as f:\n        behavior_lines = f.readlines()[:(- 1)]\n    behavior_elements = [map(int, split_t(x)) for x in behavior_lines]\n    del behavior_lines\n    return behavior_elements\n"}
{"label_name":"train","label":0,"method_name":"check_classifiers_train","method":"\n\n@ignore_warnings\ndef check_classifiers_train(name, Classifier):\n    (X_m, y_m) = make_blobs(n_samples=300, random_state=0)\n    (X_m, y_m) = shuffle(X_m, y_m, random_state=7)\n    X_m = StandardScaler().fit_transform(X_m)\n    y_b = y_m[(y_m != 2)]\n    X_b = X_m[(y_m != 2)]\n    for (X, y) in [(X_m, y_m), (X_b, y_b)]:\n        classes = np.unique(y)\n        n_classes = len(classes)\n        (n_samples, n_features) = X.shape\n        classifier = Classifier()\n        if (name in ['BernoulliNB', 'MultinomialNB']):\n            X -= X.min()\n        set_testing_parameters(classifier)\n        set_random_state(classifier)\n        assert_raises(ValueError, classifier.fit, X, y[:(- 1)])\n        classifier.fit(X, y)\n        classifier.fit(X.tolist(), y.tolist())\n        assert_true(hasattr(classifier, 'classes_'))\n        y_pred = classifier.predict(X)\n        assert_equal(y_pred.shape, (n_samples,))\n        if (name not in ['BernoulliNB', 'MultinomialNB']):\n            assert_greater(accuracy_score(y, y_pred), 0.83)\n        assert_raises(ValueError, classifier.predict, X.T)\n        if hasattr(classifier, 'decision_function'):\n            try:\n                decision = classifier.decision_function(X)\n                if (n_classes is 2):\n                    assert_equal(decision.shape, (n_samples,))\n                    dec_pred = (decision.ravel() > 0).astype(np.int)\n                    assert_array_equal(dec_pred, y_pred)\n                if ((n_classes is 3) and (not isinstance(classifier, BaseLibSVM))):\n                    assert_equal(decision.shape, (n_samples, n_classes))\n                    assert_array_equal(np.argmax(decision, axis=1), y_pred)\n                assert_raises(ValueError, classifier.decision_function, X.T)\n                assert_raises(ValueError, classifier.decision_function, X.T)\n            except NotImplementedError:\n                pass\n        if hasattr(classifier, 'predict_proba'):\n            y_prob = classifier.predict_proba(X)\n            assert_equal(y_prob.shape, (n_samples, n_classes))\n            assert_array_equal(np.argmax(y_prob, axis=1), y_pred)\n            assert_array_almost_equal(np.sum(y_prob, axis=1), np.ones(n_samples))\n            assert_raises(ValueError, classifier.predict_proba, X.T)\n            assert_raises(ValueError, classifier.predict_proba, X.T)\n            if hasattr(classifier, 'predict_log_proba'):\n                y_log_prob = classifier.predict_log_proba(X)\n                assert_array_almost_equal(y_log_prob, np.log(y_prob), 8)\n                assert_array_equal(np.argsort(y_log_prob), np.argsort(y_prob))\n"}
{"label_name":"train","label":0,"method_name":"train_agent","method":"\n\ndef train_agent(config, agent):\n    torch.manual_seed(config.SEED)\n    env = BinaryActionEncodingWrapper(environment_name=config.ENVIRONMENT_NAME, connect_to_running=config.CONNECT_TO_RUNNING)\n    env.seed(config.SEED)\n    agent.build(env)\n    listener = add_early_stopping_key_combination(agent.stop_procedure)\n    if listener:\n        listener.start()\n    try:\n        (trained_model, running_signals, running_lengths, *training_statistics) = agent.train(env, config.ROLLOUTS, render=config.RENDER_ENVIRONMENT)\n    finally:\n        if listener:\n            listener.stop()\n    draugr.save_statistic(running_signals, stat_name='running_signals', config_name=C.CONFIG_NAME, project_name=C.PROJECT, directory=C.LOG_DIRECTORY)\n    draugr.save_statistic(running_lengths, stat_name='running_lengths', directory=C.LOG_DIRECTORY, config_name=C.CONFIG_NAME, project_name=C.PROJECT)\n    U.save_model(trained_model, **config)\n    env.close()\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train():\n    with tf.Graph().as_default():\n        global_step = tf.contrib.framework.get_or_create_global_step()\n    with tf.device('\/cpu:0'):\n        (images, labels) = cifar10.distorted_inputs()\n    logits = cifar10.inference(images)\n    loss = cifar10.loss(logits, lables)\n    train_op = cifar10.train(loss, global_step)\n\n    class _LoggerHook(tf.train.SessionRunHook):\n\n        def begin(self):\n            self._step = (- 1)\n            self._start_time = time.time()\n\n        def before_run(self, run_context):\n            self._step += 1\n            return tf.train.SessionRunArgs(loss)\n\n        def after_run(self, run_context, run_values):\n            if ((self._step % FLAGS.log_frequency) == 0):\n                current_time = time.time()\n                duration = (current_time - self._start_time)\n                self._start_time = current_time\n                loss_value = run_values.results\n                examples_per_sec = ((FLAGS.log_frequency * FLAGS.batch_size) \/ duration)\n                sec_per_batch = float((duration \/ FLAGS.log_frequency))\n                format_str = '%s: step %d, loss = %.2f (%.1f examples\/sec; %.3f sec\/batch'\n                print((format_str % (datetime.now(), self._step, loss_value, examples_per_sec, sec_per_batch)))\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir, hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps), tf.train.NanTensorHook(loss), _LoggerHook()], config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n        mon_sess.run(train_op)\n"}
{"label_name":"predict","label":4,"method_name":"evaluate_predict_pipeline","method":"\n\n@action.command()\n@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\ndef evaluate_predict_pipeline(pipeline_name):\n    logger.info('evaluating')\n    _evaluate_pipeline(pipeline_name)\n    logger.info('predicting')\n    _predict_pipeline(pipeline_name)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(data_dir, checkpoint_path, config):\n    'Trains the model with the given data\\n\\n    Args:\\n        data_dir: path to the data for the model (see data_utils for data\\n            format)\\n        checkpoint_path: the path to save the trained model checkpoints\\n        config: one of the above configs that specify the model and how it\\n            should be run and trained\\n    Returns:\\n        None\\n    '\n    print(('Reading Name data in %s' % data_dir))\n    (names, counts) = data_utils.read_names(data_dir)\n    with tf.Graph().as_default(), tf.Session() as session:\n        initializer = tf.random_uniform_initializer((- config.init_scale), config.init_scale)\n        with tf.variable_scope('model', reuse=None, initializer=initializer):\n            m = NamignizerModel(is_training=True, config=config)\n        tf.global_variables_initializer().run()\n        for i in range(config.max_max_epoch):\n            lr_decay = (config.lr_decay ** max((i - config.max_epoch), 0.0))\n            m.assign_lr(session, (config.learning_rate * lr_decay))\n            print(('Epoch: %d Learning rate: %.3f' % ((i + 1), session.run(m.lr))))\n            train_perplexity = run_epoch(session, m, names, counts, config.epoch_size, m.train_op, verbose=True)\n            print(('Epoch: %d Train Perplexity: %.3f' % ((i + 1), train_perplexity)))\n            m.saver.save(session, checkpoint_path, global_step=i)\n"}
{"label_name":"train","label":0,"method_name":"load_yt_train_base_recent","method":"\n\ndef load_yt_train_base_recent(dir, t, o, l):\n    name = (((('yt_train_base_recent_' + str(t)) + str(o)) + str(l)) + '.txt')\n    with open(os.path.join(dir, name)) as fp:\n        dataSet = []\n        for line in fp:\n            line = line.strip('\\n')\n            features = line.split(',')\n            features = [float(x) for x in features]\n            dataSet.append(features)\n    yt_train_base_recent = np.array(dataSet)\n    return yt_train_base_recent\n"}
{"label_name":"train","label":0,"method_name":"create_dis_train_op","method":"\n\ndef create_dis_train_op(hparams, dis_loss, global_step):\n    'Create Discriminator train op.'\n    with tf.name_scope('train_discriminator'):\n        dis_optimizer = tf.train.AdamOptimizer(hparams.dis_learning_rate)\n        dis_vars = [v for v in tf.trainable_variables() if v.op.name.startswith('dis')]\n        if (FLAGS.dis_update_share_embedding and FLAGS.dis_share_embedding):\n            shared_embedding = [v for v in tf.trainable_variables() if (v.op.name == 'gen\/decoder\/rnn\/embedding')][0]\n            dis_vars.append(shared_embedding)\n        print('\\nOptimizing Discriminator vars:')\n        for v in dis_vars:\n            print(v)\n        dis_grads = tf.gradients(dis_loss, dis_vars)\n        (dis_grads_clipped, _) = tf.clip_by_global_norm(dis_grads, FLAGS.grad_clipping)\n        dis_train_op = dis_optimizer.apply_gradients(zip(dis_grads_clipped, dis_vars), global_step=global_step)\n        return (dis_train_op, dis_grads_clipped, dis_vars)\n"}
{"label_name":"train","label":0,"method_name":"check_weighted_decision_path_train","method":"\n\ndef check_weighted_decision_path_train(est, X):\n    leaf_nodes = est.apply(X)\n    weights_sparse = est.weighted_decision_path(X)\n    assert_array_equal(weights_sparse.data, np.ones(X.shape[0]))\n    assert_array_equal(weights_sparse.indices, leaf_nodes)\n    assert_array_equal(weights_sparse.indptr, np.arange((X.shape[0] + 1)))\n"}
{"label_name":"train","label":0,"method_name":"train_local","method":"\n\ndef train_local(name, src_dir, data_dir, model_dir, run_config, train_spec, eval_spec, params):\n    sys.path.append(src_dir)\n    from doodle.inputs import train_input_fn, eval_input_fn, serving_input_fn\n    from doodle.model import model_fn\n    _model_dir = os.path.join(model_dir, name)\n    _run_config = tf.estimator.RunConfig(model_dir=_model_dir, **run_config)\n    _train_spec = tf.estimator.TrainSpec(input_fn=(lambda : train_input_fn(data_dir, params)), **train_spec)\n    _eval_spec = tf.estimator.EvalSpec(input_fn=(lambda : eval_input_fn(data_dir, params)), exporters=[tf.estimator.LatestExporter('savedmodel', serving_input_fn(params)), tfhub.LatestModuleExporter('hub', serving_input_fn(params))], **eval_spec)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, config=_run_config, params=params)\n    tf.estimator.train_and_evaluate(estimator, _train_spec, _eval_spec)\n    metrics = estimator.evaluate(_eval_spec.input_fn, steps=_eval_spec.steps)\n    print(('###### metrics ' + ('#' * 65)))\n    for (name, value) in sorted(six.iteritems(metrics)):\n        print('{:<30}: {}'.format(name, value))\n"}
{"label_name":"predict","label":4,"method_name":"get_prediction_from_url","method":"\n\ndef get_prediction_from_url(test_url):\n    features_test = features_extraction.main(test_url)\n    features_test = np.array(features_test).reshape((1, (- 1)))\n    clf = joblib.load(((LOCALHOST_PATH + DIRECTORY_NAME) + '\/classifier\/random_forest.pkl'))\n    pred = clf.predict(features_test)\n    return int(pred[0])\n"}
{"label_name":"process","label":2,"method_name":"im_process","method":"\n\ndef im_process(im_cell, im_nuc, im_structures):\n    seg_nuc = binary_fill_holes(im_nuc)\n    seg_cell = binary_fill_holes(im_cell)\n    seg_cell[seg_nuc] = 1\n    im_nuc = ((im_nuc * 255).astype('uint16') * seg_nuc)\n    im_cell = ((im_cell * 255).astype('uint16') * seg_cell)\n    im_structures = [(im_ch * 255).astype('uint16') for im_ch in im_structures]\n    im_structures_seg = list()\n    for (i, im_structure) in enumerate(im_structures):\n        im_blur = gaussian(im_structure, 1)\n        im_pix = im_structure[(im_cell > 0)]\n        if np.all((im_pix == 0)):\n            im_structures_seg.append(im_structure)\n            continue\n        im_structures_seg.append((im_structure * (im_blur > threshold_otsu(im_blur[(im_cell > 0)]))))\n    return (im_cell, im_nuc, im_structures, im_structures_seg, seg_cell, seg_nuc)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(X, Y, P, rate=0.0001, theta=0.1):\n    time = 0\n    while (cost(X, Y, P) > theta):\n        time += 1\n        P = (P + (X.T.dot((Y.T - softmax(X.dot(P)))) * rate))\n    print(time, cost(X, Y, P))\n    return P\n"}
{"label_name":"predict","label":4,"method_name":"_predict_chunk_res","method":"\n\ndef _predict_chunk_res(q_res, *_):\n    chunk_res = defaultdict(list)\n    res_rows_path = ['results', 'bindings']\n    bindings = sparql_json_result_bindings_to_rdflib(get_path(q_res, res_rows_path, default=[]))\n    for row in bindings:\n        s = get_path(row, [SOURCE_VAR])\n        t = get_path(row, [TARGET_VAR])\n        chunk_res[s].append(t)\n    return chunk_res\n"}
{"label_name":"train","label":0,"method_name":"create_train_masks_data","method":"\n\ndef create_train_masks_data():\n    train_masks_data_path = os.path.join(data_path, 'train_masks')\n    images = os.listdir(train_masks_data_path)\n    total = len(images)\n    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n    i = 0\n    print(('-' * 30))\n    print('Creating train masks images...')\n    print(('-' * 30))\n    for image_name in images:\n        print(image_name)\n        img = cv2.imread(os.path.join(train_masks_data_path, image_name))\n        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        resized_image = cv2.resize(gray_image, (image_cols, image_rows))\n        img = np.array([resized_image])\n        img = np.array([img])\n        imgs[i] = img\n        print('Done: {0}\/{1} images'.format(i, total))\n        i += 1\n    print('Loading done.')\n    np.save('imgs_mask_train.npy', imgs)\n    print('Saving to .npy files done.')\n"}
{"label_name":"process","label":2,"method_name":"download_and_process","method":"\n\ndef download_and_process(data_root):\n    if (not os.path.isdir(data_root)):\n        os.makedirs(data_root)\n        os.makedirs(os.path.join(data_root, 'raw'))\n        tx.data.maybe_download(urls='https:\/\/drive.google.com\/file\/d\/1Gytd-SSetUkIY6aVVKNrBOxkHjAlSGeU\/view?usp=sharing', path='.\/', filenames=os.path.join(data_root, 'sw1c2r.tar.gz'), extract=True)\n        os.system('mv {} {}'.format(os.path.join(data_root, 'sw1c2r.tar.gz'), os.path.join(data_root, 'raw\/sw1c2r.tar.gz')))\n        os.system('mv {}\/* {}'.format(os.path.join(data_root, 'switchboard'), data_root))\n        datasets = sw1c2r(os.path.join(data_root, 'json_data'))\n        for stage in ['train', 'val', 'test']:\n            dts = datasets[stage]\n            (spk, src, tgt, meta) = list(zip(*[dts[i] for i in range(len(dts))]))\n            src_txt = '\\n'.join(src)\n            tgt_txt = '\\n'.join(tgt)\n            spk = list(zip(*spk))\n            for i in range(len(spk)):\n                with open(os.path.join(data_root, '{}-source-spk-{}.txt'.format(stage, i)), 'w') as f:\n                    f.write('\\n'.join([str(a) for a in spk[i]]))\n            spk_tgt = meta\n            with open(os.path.join(data_root, '{}-target-spk.txt'.format(stage)), 'w') as f:\n                f.write('\\n'.join([str(a) for a in spk_tgt]))\n            with open(os.path.join(data_root, '{}-source.txt'.format(stage)), 'w') as f:\n                f.write(src_txt)\n            with open(os.path.join(data_root, '{}-target.txt'.format(stage)), 'w') as f:\n                f.write(tgt_txt)\n            with open(os.path.join(data_root, '{}-target-refs.txt'.format(stage)), 'w') as f:\n                f.write('\\n'.join(['|||'.join(v) for v in dts.refs]))\n"}
{"label_name":"forward","label":3,"method_name":"_list_forward","method":"\n\ndef _list_forward(model: Model[(List2d, List2d)], Xs: List2d, is_train: bool) -> Tuple[(List2d, Callable)]:\n    layer = model.layers[0]\n    pad = model.attrs['pad']\n    lengths = layer.ops.asarray1i([len(seq) for seq in Xs])\n    Xf = layer.ops.flatten(Xs, pad=pad)\n    (Yf, get_dXf) = layer(Xf, is_train)\n\n    def backprop(dYs: List2d) -> List2d:\n        dYf = layer.ops.flatten(dYs, pad=pad)\n        dXf = get_dXf(dYf)\n        return layer.ops.unflatten(dXf, lengths, pad=pad)\n    return (layer.ops.unflatten(Yf, lengths, pad=pad), backprop)\n"}
{"label_name":"predict","label":4,"method_name":"cross_val_predict","method":"\n\ndef cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', method='predict'):\n    \"Generate cross-validated estimates for each input data point\\n\\n    Read more in the :ref:`User Guide <cross_validation>`.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit' and 'predict'\\n        The object to use to fit the data.\\n\\n    X : array-like\\n        The data to fit. Can be, for example a list, or an array at least 2d.\\n\\n    y : array-like, optional, default: None\\n        The target variable to try to predict in the case of\\n        supervised learning.\\n\\n    groups : array-like, with shape (n_samples,), optional\\n        Group labels for the samples used while splitting the dataset into\\n        train\/test set.\\n\\n    cv : int, cross-validation generator or an iterable, optional\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 3-fold cross validation,\\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\\n        - An object to be used as a cross-validation generator.\\n        - An iterable yielding train, test splits.\\n\\n        For integer\/None inputs, if the estimator is a classifier and ``y`` is\\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\\n        other cases, :class:`KFold` is used.\\n\\n        Refer :ref:`User Guide <cross_validation>` for the various\\n        cross-validation strategies that can be used here.\\n\\n    n_jobs : integer, optional\\n        The number of CPUs to use to do the computation. -1 means\\n        'all CPUs'.\\n\\n    verbose : integer, optional\\n        The verbosity level.\\n\\n    fit_params : dict, optional\\n        Parameters to pass to the fit method of the estimator.\\n\\n    pre_dispatch : int, or string, optional\\n        Controls the number of jobs that get dispatched during parallel\\n        execution. Reducing this number can be useful to avoid an\\n        explosion of memory consumption when more jobs get dispatched\\n        than CPUs can process. This parameter can be:\\n\\n            - None, in which case all the jobs are immediately\\n              created and spawned. Use this for lightweight and\\n              fast-running jobs, to avoid delays due to on-demand\\n              spawning of the jobs\\n\\n            - An int, giving the exact number of total jobs that are\\n              spawned\\n\\n            - A string, giving an expression as a function of n_jobs,\\n              as in '2*n_jobs'\\n\\n    method : string, optional, default: 'predict'\\n        Invokes the passed method name of the passed estimator. For\\n        method='predict_proba', the columns correspond to the classes\\n        in sorted order.\\n\\n    Returns\\n    -------\\n    predictions : ndarray\\n        This is the result of calling ``method``\\n\\n    Notes\\n    -----\\n    In the case that one or more classes are absent in a training portion, a\\n    default score needs to be assigned to all instances for that class if\\n    ``method`` produces columns per class, as in {'decision_function',\\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\\n    0.  In order to ensure finite output, we approximate negative infinity by\\n    the minimum finite float value for the dtype in other cases.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import datasets, linear_model\\n    >>> from sklearn.model_selection import cross_val_predict\\n    >>> diabetes = datasets.load_diabetes()\\n    >>> X = diabetes.data[:150]\\n    >>> y = diabetes.target[:150]\\n    >>> lasso = linear_model.Lasso()\\n    >>> y_pred = cross_val_predict(lasso, X, y)\\n    \"\n    (X, y, groups) = indexable(X, y, groups)\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    if (method in ['decision_function', 'predict_proba', 'predict_log_proba']):\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    prediction_blocks = parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params, method) for (train, test) in cv.split(X, y, groups)))\n    predictions = [pred_block_i for (pred_block_i, _) in prediction_blocks]\n    test_indices = np.concatenate([indices_i for (_, indices_i) in prediction_blocks])\n    if (not _check_is_permutation(test_indices, _num_samples(X))):\n        raise ValueError('cross_val_predict only works for partitions')\n    inv_test_indices = np.empty(len(test_indices), dtype=int)\n    inv_test_indices[test_indices] = np.arange(len(test_indices))\n    if sp.issparse(predictions[0]):\n        predictions = sp.vstack(predictions, format=predictions[0].format)\n    else:\n        predictions = np.concatenate(predictions)\n    return predictions[inv_test_indices]\n"}
{"label_name":"process","label":2,"method_name":"image_read_and_process","method":"\n\ndef image_read_and_process(image_file):\n    '\\n    *Dataset Specific\\n    Read and pre-process image\\n    '\n    image = np.array(Image.open(image_file))\n    image = image[:1200, :1200, :]\n    return image\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(model, x_train, y_train, x_test, y_test, gen=False):\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    ' original:\\n    model.fit(x_train, y_train,\\n              batch_size=300,\\n              epochs=40,\\n              validation_data=(x_test, y_test),\\n              shuffle=True)\\n    '\n    LOG_DIR = '.\/training_logs'\n    LOG_FILE_PATH = (LOG_DIR + '\/checkpoint-{epoch:02d}-{val_loss:.4f}.hdf5')\n    tensorboard = TensorBoard(log_dir=LOG_DIR, write_images=True)\n    checkpoint = ModelCheckpoint(filepath=LOG_FILE_PATH, monitor='val_loss', verbose=1, save_best_only=True)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=40, verbose=1)\n    batch_size = 256\n    if gen:\n        datagen = ImageDataGenerator(rotation_range=0.2, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n        datagen.fit(x_train)\n        model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=round((len(x_train) \/ batch_size)), epochs=40, validation_data=(x_test, y_test), callbacks=[tensorboard, checkpoint, early_stopping])\n    else:\n        model.fit(x_train, y_train, batch_size=256, epochs=40, validation_data=(x_test, y_test), shuffle=True, callbacks=[tensorboard, checkpoint, early_stopping])\n    return model\n"}
{"label_name":"process","label":2,"method_name":"preprocess_for_eval","method":"\n\ndef preprocess_for_eval(image, height, width, central_fraction=0.875, scope=None):\n    'Prepare one image for evaluation.\\n\\n  If height and width are specified it would output an image with that size by\\n  applying resize_bilinear.\\n\\n  If central_fraction is specified it would cropt the central fraction of the\\n  input image.\\n\\n  Args:\\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\\n      is [0, MAX], where MAX is largest positive representable number for\\n      int(8\/16\/32) data type (see `tf.image.convert_image_dtype` for details)\\n    height: integer\\n    width: integer\\n    central_fraction: Optional Float, fraction of the image to crop.\\n    scope: Optional scope for name_scope.\\n  Returns:\\n    3-D float Tensor of prepared image.\\n  '\n    with tf.name_scope(scope, 'eval_image', [image, height, width]):\n        if (image.dtype != tf.float32):\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        if central_fraction:\n            image = tf.image.central_crop(image, central_fraction=central_fraction)\n        if (height and width):\n            image = tf.expand_dims(image, 0)\n            image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n            image = tf.squeeze(image, [0])\n        image = tf.subtract(image, 0.5)\n        image = tf.multiply(image, 2.0)\n        return image\n"}
{"label_name":"forward","label":3,"method_name":"adaptive_pool2d_forward","method":"\n\ndef adaptive_pool2d_forward(x, output_size, data_format='NCHW', pool_type='max'):\n    N = x.shape[0]\n    (C, H, W) = ([x.shape[1], x.shape[2], x.shape[3]] if (data_format == 'NCHW') else [x.shape[3], x.shape[1], x.shape[2]])\n    if (isinstance(output_size, int) or (output_size == None)):\n        H_out = output_size\n        W_out = output_size\n        output_size = [H_out, W_out]\n    else:\n        (H_out, W_out) = output_size\n    if (output_size[0] == None):\n        output_size[0] = H\n        H_out = H\n    if (output_size[1] == None):\n        output_size[1] = W\n        W_out = W\n    out = (np.zeros((N, C, H_out, W_out)) if (data_format == 'NCHW') else np.zeros((N, H_out, W_out, C)))\n    for i in range(H_out):\n        in_h_start = adaptive_start_index(i, H, output_size[0])\n        in_h_end = adaptive_end_index(i, H, output_size[0])\n        for j in range(W_out):\n            in_w_start = adaptive_start_index(j, W, output_size[1])\n            in_w_end = adaptive_end_index(j, W, output_size[1])\n            if (data_format == 'NCHW'):\n                x_masked = x[:, :, in_h_start:in_h_end, in_w_start:in_w_end]\n                if (pool_type == 'avg'):\n                    field_size = ((in_h_end - in_h_start) * (in_w_end - in_w_start))\n                    out[:, :, i, j] = (np.sum(x_masked, axis=(2, 3)) \/ field_size)\n                elif (pool_type == 'max'):\n                    out[:, :, i, j] = np.max(x_masked, axis=(2, 3))\n            elif (data_format == 'NHWC'):\n                x_masked = x[:, in_h_start:in_h_end, in_w_start:in_w_end, :]\n                if (pool_type == 'avg'):\n                    field_size = ((in_h_end - in_h_start) * (in_w_end - in_w_start))\n                    out[:, i, j, :] = (np.sum(x_masked, axis=(1, 2)) \/ field_size)\n                elif (pool_type == 'max'):\n                    out[:, i, j, :] = np.max(x_masked, axis=(1, 2))\n    return out\n"}
{"label_name":"process","label":2,"method_name":"process_raw_data","method":"\n\ndef process_raw_data():\n    lines = read_lines('D:\\\\workspace\\\\chatdata\\\\chatdata')\n    qlines = []\n    alines = []\n    print('loaded')\n    for i in range(len(lines)):\n        if (lines[i] == '\\n'):\n            continue\n        session = json.loads(lines[i])\n        for j in range(0, len(session), 2):\n            if ((j + 1) == len(session)):\n                break\n            if ((session[j] != '') and (session[(j + 1)] != '')):\n                session[j] = filter(session[j])\n                session[(j + 1)] = filter(session[(j + 1)])\n                if ((session[j] == '\\n') or (session[j] == '') or (session[j] == ' ') or (session[(j + 1)] == '\\n') or (session[(j + 1)] == '') or (session[(j + 1)] == ' ')):\n                    continue\n                qlines.append(session[j])\n                alines.append(session[(j + 1)])\n    print('filtered')\n    questions = [[w for w in jb.cut(wordlist)] for wordlist in qlines]\n    answers = [[w for w in jb.cut(wordlist)] for wordlist in alines]\n    vocab = build_vocab({}, questions)\n    vocab = build_vocab(vocab, answers)\n    id2word = (([' '] + [config.UNK_ID]) + [x for x in vocab])\n    word2id = dict([(w, i) for (i, w) in enumerate(id2word)])\n    with open('config.py', 'a') as cf:\n        cf.write((('DEC_VOCAB = ' + str(len(id2word))) + '\\n'))\n        cf.write((('ENC_VOCAB = ' + str(len(id2word))) + '\\n'))\n    print('wrote config')\n    questions_ids = [line_ids(word, word2id, config.SENTENCE_MAX_LEN) for word in questions]\n    answers_ids = [line_ids(word, word2id, config.SENTENCE_MAX_LEN) for word in answers]\n    np.save((config.DATA_PATH + '\/idx_q.npy'), questions_ids)\n    np.save((config.DATA_PATH + '\/idx_a.npy'), answers_ids)\n    print('wrote qa')\n    metadata = {'w2idx': word2id, 'idx2w': id2word}\n    with open((config.DATA_PATH + '\/metadata.pkl'), 'wb') as f:\n        pickle.dump(metadata, f)\n    print('wrote metadata')\n    return (id2word, word2id, questions_ids, answers_ids)\n"}
{"label_name":"train","label":0,"method_name":"load_pretrained","method":"\n\ndef load_pretrained(model, num_classes, settings):\n    assert (num_classes == settings['num_classes']), 'num_classes should be {}, but is {}'.format(settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n    return model\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(i, m):\n    numerator = 0\n    denominator = 0\n    for (neg_w, j) in neighbors[i]:\n        try:\n            numerator += ((- neg_w) * deviations[j][m])\n            denominator += abs(neg_w)\n        except KeyError:\n            pass\n    if (denominator == 0):\n        prediction = averages[i]\n    else:\n        prediction = ((numerator \/ denominator) + averages[i])\n    prediction = min(5, prediction)\n    prediction = max(0.5, prediction)\n    return prediction\n"}
{"label_name":"process","label":2,"method_name":"new_img_preprocess","method":"\n\ndef new_img_preprocess(img_path, size=224, augment=False):\n    mean = [103.939, 116.779, 123.68]\n    img = imread(img_path)\n    if (img == None):\n        img = np.zeros((1, 299, 299, 3))\n    if augment:\n        img = randomizer(img)\n    if (len(img.shape) == 2):\n        img = np.dstack([img, img, img])\n    resFac = (256.0 \/ min(img.shape[:2]))\n    newSize = list(map(int, ((img.shape[0] * resFac), (img.shape[1] * resFac))))\n    img = resize(img, newSize, mode='constant', preserve_range=True)\n    offset = [((newSize[0] \/ 2.0) - np.floor((size \/ 2.0))), ((newSize[1] \/ 2.0) - np.floor((size \/ 2.0)))]\n    img = img[int(offset[0]):(int(offset[0]) + size), int(offset[1]):(int(offset[1]) + size), :]\n    img[:, :, 0] -= mean[2]\n    img[:, :, 1] -= mean[1]\n    img[:, :, 2] -= mean[0]\n    img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n    img = np.reshape(img, [1, size, size, 3])\n    return img\n"}
{"label_name":"predict","label":4,"method_name":"predict_sr","method":"\n\ndef predict_sr(model, player):\n    stats_vector = np.array([get_vector_gamestats(player, 'us')])\n    X = scaler_X.transform(stats_vector)\n    y_matrix = model.predict(X)\n    sr = np.squeeze(scaler_y.inverse_transform(y_matrix))\n    return int(sr)\n"}
{"label_name":"train","label":0,"method_name":"load_number_of_epochs_trained","method":"\n\ndef load_number_of_epochs_trained(model, run_id=None, early_stopping=False, best_model=False):\n    n_epoch = None\n    data_set_kind = 'training'\n    loss = 'log_likelihood'\n    loss_prefix = 'losses\/'\n    if ('VAE' in model.type):\n        loss = 'lower_bound'\n    loss = (loss_prefix + loss)\n    log_directory = model.log_directory(run_id=run_id, early_stopping=early_stopping, best_model=best_model)\n    scalar_sets = _summary_reader(log_directory, data_set_kind, loss)\n    if (scalar_sets and (data_set_kind in scalar_sets)):\n        data_set_scalars = scalar_sets[data_set_kind]\n    else:\n        data_set_scalars = None\n    if (data_set_scalars and (loss in data_set_scalars)):\n        scalars = data_set_scalars[loss]\n    else:\n        scalars = None\n    if scalars:\n        n_epoch = max([scalar.step for scalar in scalars])\n    return n_epoch\n"}
{"label_name":"save","label":1,"method_name":"save_smi","method":"\n\ndef save_smi(name, smiles):\n    if (not os.path.exists('epoch_data')):\n        os.makedirs('epoch_data')\n    smi_file = os.path.join('epoch_data', '{}.smi'.format(name))\n    with open(smi_file, 'w') as afile:\n        afile.write('\\n'.join(smiles))\n    return\n"}
{"label_name":"predict","label":4,"method_name":"_predict_res_init","method":"\n\ndef _predict_res_init(sources):\n    return OrderedDict([(s, []) for s in sources])\n"}
{"label_name":"predict","label":4,"method_name":"predict_regression","method":"\n\ndef predict_regression(x_test, trained_estimator):\n    '\\n    Given feature data and a trained estimator, return a regression prediction\\n\\n    Args:\\n        x_test: \\n        trained_estimator (sklearn.base.BaseEstimator): a trained scikit-learn estimator\\n\\n    Returns:\\n        a prediction\\n    '\n    validate_estimator(trained_estimator)\n    prediction = trained_estimator.predict(x_test)\n    return prediction\n"}
{"label_name":"save","label":1,"method_name":"_fig_show_save_or_axes","method":"\n\ndef _fig_show_save_or_axes(plot_obj, return_fig, show, save):\n    '\\n    Decides what to return\\n    '\n    if return_fig:\n        return plot_obj\n    else:\n        plot_obj.make_figure()\n        savefig_or_show(plot_obj.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = (settings.autoshow if (show is None) else show)\n        if (show is False):\n            return plot_obj.get_axes()\n"}
{"label_name":"process","label":2,"method_name":"start_ray_process","method":"\n\ndef start_ray_process(command, process_type, fate_share, env_updates=None, cwd=None, use_valgrind=False, use_gdb=False, use_valgrind_profiler=False, use_perftools_profiler=False, use_tmux=False, stdout_file=None, stderr_file=None, pipe_stdin=False):\n    'Start one of the Ray processes.\\n\\n    TODO(rkn): We need to figure out how these commands interact. For example,\\n    it may only make sense to start a process in gdb if we also start it in\\n    tmux. Similarly, certain combinations probably don\\'t make sense, like\\n    simultaneously running the process in valgrind and the profiler.\\n\\n    Args:\\n        command (List[str]): The command to use to start the Ray process.\\n        process_type (str): The type of the process that is being started\\n            (e.g., \"raylet\").\\n        fate_share: If true, the child will be killed if its parent (us) dies.\\n            True must only be passed after detection of this functionality.\\n        env_updates (dict): A dictionary of additional environment variables to\\n            run the command with (in addition to the caller\\'s environment\\n            variables).\\n        cwd (str): The directory to run the process in.\\n        use_valgrind (bool): True if we should start the process in valgrind.\\n        use_gdb (bool): True if we should start the process in gdb.\\n        use_valgrind_profiler (bool): True if we should start the process in\\n            the valgrind profiler.\\n        use_perftools_profiler (bool): True if we should profile the process\\n            using perftools.\\n        use_tmux (bool): True if we should start the process in tmux.\\n        stdout_file: A file handle opened for writing to redirect stdout to. If\\n            no redirection should happen, then this should be None.\\n        stderr_file: A file handle opened for writing to redirect stderr to. If\\n            no redirection should happen, then this should be None.\\n        pipe_stdin: If true, subprocess.PIPE will be passed to the process as\\n            stdin.\\n\\n    Returns:\\n        Information about the process that was started including a handle to\\n            the process that was started.\\n    '\n    valgrind_env_var = f'RAY_{process_type.upper()}_VALGRIND'\n    if (os.environ.get(valgrind_env_var) == '1'):\n        logger.info(\"Detected environment variable '%s'.\", valgrind_env_var)\n        use_valgrind = True\n    valgrind_profiler_env_var = f'RAY_{process_type.upper()}_VALGRIND_PROFILER'\n    if (os.environ.get(valgrind_profiler_env_var) == '1'):\n        logger.info(\"Detected environment variable '%s'.\", valgrind_profiler_env_var)\n        use_valgrind_profiler = True\n    perftools_profiler_env_var = f'RAY_{process_type.upper()}_PERFTOOLS_PROFILER'\n    if (os.environ.get(perftools_profiler_env_var) == '1'):\n        logger.info(\"Detected environment variable '%s'.\", perftools_profiler_env_var)\n        use_perftools_profiler = True\n    tmux_env_var = f'RAY_{process_type.upper()}_TMUX'\n    if (os.environ.get(tmux_env_var) == '1'):\n        logger.info(\"Detected environment variable '%s'.\", tmux_env_var)\n        use_tmux = True\n    gdb_env_var = f'RAY_{process_type.upper()}_GDB'\n    if (os.environ.get(gdb_env_var) == '1'):\n        logger.info(\"Detected environment variable '%s'.\", gdb_env_var)\n        use_gdb = True\n    if (sum([use_gdb, use_valgrind, use_valgrind_profiler, use_perftools_profiler]) > 1):\n        raise ValueError(\"At most one of the 'use_gdb', 'use_valgrind', 'use_valgrind_profiler', and 'use_perftools_profiler' flags can be used at a time.\")\n    if (env_updates is None):\n        env_updates = {}\n    if (not isinstance(env_updates, dict)):\n        raise ValueError(\"The 'env_updates' argument must be a dictionary.\")\n    modified_env = os.environ.copy()\n    modified_env.update(env_updates)\n    if use_gdb:\n        if (not use_tmux):\n            raise ValueError(\"If 'use_gdb' is true, then 'use_tmux' must be true as well.\")\n        gdb_init_path = os.path.join(ray._private.utils.get_ray_temp_dir(), f'gdb_init_{process_type}_{time.time()}')\n        ray_process_path = command[0]\n        ray_process_args = command[1:]\n        run_args = ' '.join([\"'{}'\".format(arg) for arg in ray_process_args])\n        with open(gdb_init_path, 'w') as gdb_init_file:\n            gdb_init_file.write(f'run {run_args}')\n        command = ['gdb', ray_process_path, '-x', gdb_init_path]\n    if use_valgrind:\n        command = (['valgrind', '--track-origins=yes', '--leak-check=full', '--show-leak-kinds=all', '--leak-check-heuristics=stdstring', '--error-exitcode=1'] + command)\n    if use_valgrind_profiler:\n        command = (['valgrind', '--tool=callgrind'] + command)\n    if use_perftools_profiler:\n        modified_env['LD_PRELOAD'] = os.environ['PERFTOOLS_PATH']\n        modified_env['CPUPROFILE'] = os.environ['PERFTOOLS_LOGFILE']\n    if use_tmux:\n        command = ['tmux', 'new-session', '-d', f\"{' '.join(command)}\"]\n    if fate_share:\n        assert ray._private.utils.detect_fate_sharing_support(), 'kernel-level fate-sharing must only be specified if detect_fate_sharing_support() has returned True'\n\n    def preexec_fn():\n        import signal\n        signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGINT})\n        if (fate_share and sys.platform.startswith('linux')):\n            ray._private.utils.set_kill_on_parent_death_linux()\n    win32_fate_sharing = (fate_share and (sys.platform == 'win32'))\n    CREATE_SUSPENDED = 4\n    process = ConsolePopen(command, env=modified_env, cwd=cwd, stdout=stdout_file, stderr=stderr_file, stdin=(subprocess.PIPE if pipe_stdin else None), preexec_fn=(preexec_fn if (sys.platform != 'win32') else None), creationflags=(CREATE_SUSPENDED if win32_fate_sharing else 0))\n    if win32_fate_sharing:\n        try:\n            ray._private.utils.set_kill_child_on_death_win32(process)\n            psutil.Process(process.pid).resume()\n        except (psutil.Error, OSError):\n            process.kill()\n            raise\n\n    def _get_stream_name(stream):\n        if (stream is not None):\n            try:\n                return stream.name\n            except AttributeError:\n                return str(stream)\n        return None\n    return ProcessInfo(process=process, stdout_file=_get_stream_name(stdout_file), stderr_file=_get_stream_name(stderr_file), use_valgrind=use_valgrind, use_gdb=use_gdb, use_valgrind_profiler=use_valgrind_profiler, use_perftools_profiler=use_perftools_profiler, use_tmux=use_tmux)\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\n\ndef save_model(sims_model):\n    logging.debug('saving sims model')\n    dir_name = os.path.dirname(configs.SIMS_FILE_PATH)\n    os.makedirs(dir_name)\n    sims_model.save(configs.SIMS_FILE_PATH)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train():\n    'Train a en->fr translation model using WMT data.'\n    from_train = None\n    to_train = None\n    from_dev = None\n    to_dev = None\n    if (FLAGS.from_train_data and FLAGS.to_train_data):\n        from_train_data = FLAGS.from_train_data\n        to_train_data = FLAGS.to_train_data\n        from_dev_data = from_train_data\n        to_dev_data = to_train_data\n        if (FLAGS.from_dev_data and FLAGS.to_dev_data):\n            from_dev_data = FLAGS.from_dev_data\n            to_dev_data = FLAGS.to_dev_data\n        (from_train, to_train, from_dev, to_dev, _, _) = data_utils.prepare_data(FLAGS.data_dir, from_train_data, to_train_data, from_dev_data, to_dev_data, FLAGS.from_vocab_size, FLAGS.to_vocab_size)\n    else:\n        print(('Preparing WMT data in %s' % FLAGS.data_dir))\n        (from_train, to_train, from_dev, to_dev, _, _) = data_utils.prepare_wmt_data(FLAGS.data_dir, FLAGS.from_vocab_size, FLAGS.to_vocab_size)\n    with tf.Session() as sess:\n        print(('Creating %d layers of %d units.' % (FLAGS.num_layers, FLAGS.size)))\n        model = create_model(sess, False)\n        print(('Reading development and training data (limit: %d).' % FLAGS.max_train_data_size))\n        dev_set = read_data(from_dev, to_dev)\n        train_set = read_data(from_train, to_train, FLAGS.max_train_data_size)\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n        train_total_size = float(sum(train_bucket_sizes))\n        train_buckets_scale = [(sum(train_bucket_sizes[:(i + 1)]) \/ train_total_size) for i in xrange(len(train_bucket_sizes))]\n        (step_time, loss) = (0.0, 0.0)\n        current_step = 0\n        previous_losses = []\n        while True:\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if (train_buckets_scale[i] > random_number_01)])\n            start_time = time.time()\n            (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(train_set, bucket_id)\n            (_, step_loss, _) = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n            step_time += ((time.time() - start_time) \/ FLAGS.steps_per_checkpoint)\n            loss += (step_loss \/ FLAGS.steps_per_checkpoint)\n            current_step += 1\n            if ((current_step % FLAGS.steps_per_checkpoint) == 0):\n                perplexity = (math.exp(float(loss)) if (loss < 300) else float('inf'))\n                print(('global step %d learning rate %.4f step-time %.2f perplexity %.2f' % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity)))\n                if ((len(previous_losses) > 2) and (loss > max(previous_losses[(- 3):]))):\n                    sess.run(model.learning_rate_decay_op)\n                previous_losses.append(loss)\n                checkpoint_path = os.path.join(FLAGS.train_dir, 'translate.ckpt')\n                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n                (step_time, loss) = (0.0, 0.0)\n                for bucket_id in xrange(len(_buckets)):\n                    if (len(dev_set[bucket_id]) == 0):\n                        print(('  eval: empty bucket %d' % bucket_id))\n                        continue\n                    (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(dev_set, bucket_id)\n                    (_, eval_loss, _) = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n                    eval_ppx = (math.exp(float(eval_loss)) if (eval_loss < 300) else float('inf'))\n                    print(('  eval: bucket %d perplexity %.2f' % (bucket_id, eval_ppx)))\n                sys.stdout.flush()\n"}
{"label_name":"predict","label":4,"method_name":"score_predictions","method":"\n\ndef score_predictions(y, p, scorer, name, inst_name):\n    'Try-Except wrapper around Learner scoring'\n    s = None\n    if (scorer is not None):\n        try:\n            s = scorer(y, p)\n        except Exception as exc:\n            warnings.warn(('[%s] Could not score %s. Details:\\n%r' % (name, inst_name, exc)), MetricWarning)\n    return s\n"}
{"label_name":"predict","label":4,"method_name":"predictOneVsAll","method":"\n\ndef predictOneVsAll(all_theta, X):\n    m = X.shape[0]\n    X = np.vstack((np.ones(m), X.T)).T\n    return np.argmax(sigmoid(np.dot(all_theta, X.T)), axis=0)\n"}
{"label_name":"process","label":2,"method_name":"process_urlencoded","method":"\n\ndef process_urlencoded(entity):\n    'Read application\/x-www-form-urlencoded data into entity.params.'\n    qs = entity.fp.read()\n    for charset in entity.attempt_charsets:\n        try:\n            params = {}\n            for aparam in qs.split(b'&'):\n                for pair in aparam.split(b';'):\n                    if (not pair):\n                        continue\n                    atoms = pair.split(b'=', 1)\n                    if (len(atoms) == 1):\n                        atoms.append(b'')\n                    key = unquote_plus(atoms[0]).decode(charset)\n                    value = unquote_plus(atoms[1]).decode(charset)\n                    if (key in params):\n                        if (not isinstance(params[key], list)):\n                            params[key] = [params[key]]\n                        params[key].append(value)\n                    else:\n                        params[key] = value\n        except UnicodeDecodeError:\n            pass\n        else:\n            entity.charset = charset\n            break\n    else:\n        raise cherrypy.HTTPError(400, ('The request entity could not be decoded. The following charsets were attempted: %s' % repr(entity.attempt_charsets)))\n    for (key, value) in params.items():\n        if (key in entity.params):\n            if (not isinstance(entity.params[key], list)):\n                entity.params[key] = [entity.params[key]]\n            entity.params[key].append(value)\n        else:\n            entity.params[key] = value\n"}
{"label_name":"predict","label":4,"method_name":"predict_patients","method":"\n\ndef predict_patients(patients_dir, model_path, holdout, patient_predictions, model_type):\n    model = get_unet(0.001)\n    model.load_weights(model_path)\n    for item_name in os.listdir(patients_dir):\n        if (not os.path.isdir((patients_dir + item_name))):\n            continue\n        patient_id = item_name\n        if (holdout >= 0):\n            patient_fold = helpers.get_patient_fold(patient_id, submission_set_neg=True)\n            if (patient_fold < 0):\n                if (holdout != 0):\n                    continue\n            else:\n                patient_fold %= 3\n                if (patient_fold != holdout):\n                    continue\n        print(patient_id)\n        patient_dir = ((patients_dir + patient_id) + '\/')\n        mass = 0\n        img_type = ('_i' if (model_type == 'masses') else '_c')\n        slices = glob.glob((((patient_dir + '*') + img_type) + '.png'))\n        if (model_type == 'emphysema'):\n            slices = slices[int((len(slices) \/ 2)):]\n        for img_path in slices:\n            src_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            src_img = cv2.resize(src_img, dsize=(settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n            src_img = prepare_image_for_net(src_img)\n            p = model.predict(src_img, batch_size=1)\n            p[(p < 0.5)] = 0\n            mass += p.sum()\n            p = (p[0, :, :, 0] * 255)\n            src_img = src_img.reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n            src_img *= 255\n            src_img = cv2.addWeighted(p.astype(numpy.uint8), 0.2, src_img.astype(numpy.uint8), (1 - 0.2), 0)\n            cv2.imwrite(img_path.replace((img_type + '.png'), (('_' + model_type) + 'o.png')), src_img)\n        if (mass > 1):\n            print((model_type + ': '), mass)\n        patient_predictions.append((patient_id, mass))\n        df = pandas.DataFrame(patient_predictions, columns=['patient_id', 'prediction'])\n        df.to_csv(((settings.BASE_DIR + model_type) + '_predictions.csv'), index=False)\n"}
{"label_name":"train","label":0,"method_name":"train_catdog_dataset_json_path","method":"\n\n@pytest.fixture('session')\ndef train_catdog_dataset_json_path():\n    return os.path.abspath(os.path.join('tests', 'files', 'catdog', 'train_data.json'))\n"}
{"label_name":"train","label":0,"method_name":"train_evaluate_model_from_config","method":"\n\ndef train_evaluate_model_from_config(config: Union[(str, Path, dict)], iterator: Union[(DataLearningIterator, DataFittingIterator)]=None, *, to_train: bool=True, evaluation_targets: Optional[Iterable[str]]=None, to_validate: Optional[bool]=None, download: bool=False, start_epoch_num: Optional[int]=None, recursive: bool=False) -> Dict[(str, Dict[(str, float)])]:\n    'Make training and evaluation of the model described in corresponding configuration file.'\n    config = parse_config(config)\n    if download:\n        deep_download(config)\n    if (to_train and recursive):\n        for subconfig in get_all_elems_from_json(config['chainer'], 'config_path'):\n            log.info(f'Training \"{subconfig}\"')\n            train_evaluate_model_from_config(subconfig, download=False, recursive=True)\n    import_packages(config.get('metadata', {}).get('imports', []))\n    if (iterator is None):\n        try:\n            data = read_data_by_config(config)\n        except ConfigError as e:\n            to_train = False\n            log.warning(f'Skipping training. {e.message}')\n        else:\n            iterator = get_iterator_from_config(config, data)\n    if ('train' not in config):\n        log.warning('Train config is missing. Populating with default values')\n    train_config = config.get('train')\n    if (start_epoch_num is not None):\n        train_config['start_epoch_num'] = start_epoch_num\n    if (('evaluation_targets' not in train_config) and (('validate_best' in train_config) or ('test_best' in train_config))):\n        log.warning('\"validate_best\" and \"test_best\" parameters are deprecated. Please, use \"evaluation_targets\" list instead')\n        train_config['evaluation_targets'] = []\n        if train_config.pop('validate_best', True):\n            train_config['evaluation_targets'].append('valid')\n        if train_config.pop('test_best', True):\n            train_config['evaluation_targets'].append('test')\n    trainer_class = get_model(train_config.pop('class_name', 'nn_trainer'))\n    trainer = trainer_class(config['chainer'], **train_config)\n    if to_train:\n        trainer.train(iterator)\n    res = {}\n    if (iterator is not None):\n        if (to_validate is not None):\n            if (evaluation_targets is None):\n                log.warning('\"to_validate\" parameter is deprecated and will be removed in future versions. Please, use \"evaluation_targets\" list instead')\n                evaluation_targets = ['test']\n                if to_validate:\n                    evaluation_targets.append('valid')\n            else:\n                log.warning('Both \"evaluation_targets\" and \"to_validate\" parameters are specified. \"to_validate\" is deprecated and will be ignored')\n        res = trainer.evaluate(iterator, evaluation_targets, print_reports=True)\n        trainer.get_chainer().destroy()\n    res = {k: v['metrics'] for (k, v) in res.items()}\n    return res\n"}
{"label_name":"predict","label":4,"method_name":"_fit_and_predict","method":"\n\ndef _fit_and_predict(estimator, X, y, train, test, verbose, fit_params, method):\n    \"Fit estimator and predict values for a given dataset split.\\n\\n    Read more in the :ref:`User Guide <cross_validation>`.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit' and 'predict'\\n        The object to use to fit the data.\\n\\n    X : array-like of shape at least 2D\\n        The data to fit.\\n\\n    y : array-like, optional, default: None\\n        The target variable to try to predict in the case of\\n        supervised learning.\\n\\n    train : array-like, shape (n_train_samples,)\\n        Indices of training samples.\\n\\n    test : array-like, shape (n_test_samples,)\\n        Indices of test samples.\\n\\n    verbose : integer\\n        The verbosity level.\\n\\n    fit_params : dict or None\\n        Parameters that will be passed to ``estimator.fit``.\\n\\n    method : string\\n        Invokes the passed method name of the passed estimator.\\n\\n    Returns\\n    -------\\n    predictions : sequence\\n        Result of calling 'estimator.method'\\n\\n    test : array-like\\n        This is the value of the test parameter\\n    \"\n    fit_params = (fit_params if (fit_params is not None) else {})\n    fit_params = dict([(k, _index_param_value(X, v, train)) for (k, v) in fit_params.items()])\n    (X_train, y_train) = _safe_split(estimator, X, y, train)\n    (X_test, _) = _safe_split(estimator, X, y, test, train)\n    if (y_train is None):\n        estimator.fit(X_train, **fit_params)\n    else:\n        estimator.fit(X_train, y_train, **fit_params)\n    func = getattr(estimator, method)\n    predictions = func(X_test)\n    if (method in ['decision_function', 'predict_proba', 'predict_log_proba']):\n        n_classes = len(set(y))\n        if (n_classes != len(estimator.classes_)):\n            recommendation = 'To fix this, use a cross-validation technique resulting in properly stratified folds'\n            warnings.warn('Number of classes in training fold ({}) does not match total number of classes ({}). Results may not be appropriate for your use case. {}'.format(len(estimator.classes_), n_classes, recommendation), RuntimeWarning)\n            if (method == 'decision_function'):\n                if ((predictions.ndim == 2) and (predictions.shape[1] != len(estimator.classes_))):\n                    raise ValueError('Output shape {} of {} does not match number of classes ({}) in fold. Irregular decision_function outputs are not currently supported by cross_val_predict'.format(predictions.shape, method, len(estimator.classes_), recommendation))\n                if (len(estimator.classes_) <= 2):\n                    raise ValueError('Only {} class\/es in training fold, this is not supported for decision_function with imbalanced folds. {}'.format(len(estimator.classes_), recommendation))\n            float_min = np.finfo(predictions.dtype).min\n            default_values = {'decision_function': float_min, 'predict_log_proba': float_min, 'predict_proba': 0}\n            predictions_for_all_classes = np.full((_num_samples(predictions), n_classes), default_values[method])\n            predictions_for_all_classes[:, estimator.classes_] = predictions\n            predictions = predictions_for_all_classes\n    return (predictions, test)\n"}
{"label_name":"process","label":2,"method_name":"process_sub","method":"\n\ndef process_sub(subspace):\n    '\\n    Apply ROD on a 3D subSpace\\n    '\n    mad_subspace = np.nan_to_num(np.array(rod_3D(subspace)))\n    return sigmoid(mad_subspace)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(session, model, questions):\n    'Runs the model on the given data.'\n    with open(FLAGS.p_path, 'w') as f:\n        q_id = 1\n        f.write('id,answer\\n')\n        for q in questions:\n            left = q.left\n            if (len(left) < 30):\n                for i in range((30 - len(left))):\n                    left += [0]\n            start_time = time.time()\n            costs = 0.0\n            iters = 0\n            state = session.run(model.initial_state)\n            fetches = {'logits': model.logits, 'final_state': model.final_state}\n            feed_dict = {}\n            for (i, (c, h)) in enumerate(model.initial_state):\n                feed_dict[c] = state[i].c\n                feed_dict[h] = state[i].h\n            feed_dict[model.question] = np.array(left).reshape((1, (- 1)))\n            vals = session.run(fetches, feed_dict)\n            p_opt = []\n            for o in q.options:\n                p_opt.append(np.log(vals['logits'][((q.pos - 1), o)]))\n            f.write('{},{}\\n'.format(q_id, ['a', 'b', 'c', 'd', 'e'][np.argmax(p_opt)]))\n            q_id += 1\n    return None\n"}
{"label_name":"save","label":1,"method_name":"save_image","method":"\n\ndef save_image(filename, data):\n    img = data.clone().clamp(0, 255).numpy()\n    img = img.transpose(1, 2, 0).astype('uint8')\n    img = Image.fromarray(img)\n    img.save(filename)\n"}
{"label_name":"process","label":2,"method_name":"preprocess_filter_rescale_t1","method":"\n\ndef preprocess_filter_rescale_t1(image_dict, new_min_val, new_max_val):\n\n    class MinMaxRescaleFilterParams(fltr.FilterParams):\n\n        def __init__(self, min_, max_) -> None:\n            super().__init__()\n            self.min = min_\n            self.max = max_\n\n    class MinMaxRescaleFilter(fltr.Filter):\n\n        def execute(self, img: sitk.Image, params: MinMaxRescaleFilterParams=None) -> sitk.Image:\n            resacaled_img = sitk.RescaleIntensity(img, params.min, params.max)\n            return resacaled_img\n    filter = None\n    filter_params = None\n    minmax_rescaled_img = None\n    return minmax_rescaled_img\n"}
{"label_name":"save","label":1,"method_name":"save_obj","method":"\n\ndef save_obj(pickle_file, obj):\n    try:\n        f = open(pickle_file, 'wb')\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n        f.close()\n    except Exception as e:\n        print('Unable to save data to', pickle_file, ':', e)\n        raise\n    statinfo = os.stat(pickle_file)\n    print('Compressed pickle size:', statinfo.st_size)\n"}
{"label_name":"save","label":1,"method_name":"save_results","method":"\n\ndef save_results(sess, folder, name, results_rows=None):\n    if (results_rows is not None):\n        df = pd.DataFrame(results_rows)\n        df.to_csv('{}_results.csv'.format(folder), index=False)\n    model_saver = tf.train.Saver()\n    ckpt_dir = os.path.join(params['CHK_PATH'], folder)\n    if (not os.path.exists(ckpt_dir)):\n        os.makedirs(ckpt_dir)\n    ckpt_file = os.path.join(ckpt_dir, '{}.ckpt'.format(name))\n    path = model_saver.save(sess, ckpt_file)\n    print('Model saved at {}'.format(path))\n    return\n"}
{"label_name":"train","label":0,"method_name":"trained_optimizer","method":"\n\n@pytest.fixture\ndef trained_optimizer():\n    'Returns a trained optimizer instance with 100 iterations'\n    options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n    optimizer = GlobalBestPSO(n_particles=10, dimensions=2, options=options)\n    optimizer.optimize(sphere, iters=100)\n    return optimizer\n"}
{"label_name":"process","label":2,"method_name":"process_arff","method":"\n\ndef process_arff(feature_path, subjects, classes, out_dir):\n    'Processes the given dataset to return a clean name and path.'\n    loaded_dataset = load_arff_dataset(feature_path)\n    if (len(loaded_dataset.description) > 1):\n        method_name = loaded_dataset.description\n    else:\n        method_name = basename(feature_path)\n    out_name = make_dataset_filename(method_name)\n    out_path_cur_dataset = pjoin(out_dir, out_name)\n    loaded_dataset.save(out_path_cur_dataset)\n    if (not saved_dataset_matches(loaded_dataset, subjects, classes)):\n        raise ValueError('supplied ARFF dataset does not match samples in the meta data.')\n    return (method_name, out_path_cur_dataset)\n"}
{"label_name":"predict","label":4,"method_name":"predict_fn","method":"\n\ndef predict_fn(input_object, model):\n    return (input_object + model)\n"}
{"label_name":"predict","label":4,"method_name":"predict_columns","method":"\n\ndef predict_columns():\n    'Predict transaction column labels.'\n    result = []\n    transactions = session['uploaded_transactions']\n    for _ in range(len(transactions[0])):\n        result.append('ignore')\n    header_row = False\n    for (i, value) in enumerate(transactions[0]):\n        if ('date' in value.lower()):\n            result[i] = 'date'\n            header_row = True\n        elif (('description' in value.lower()) or ('narration' in value.lower())):\n            result[i] = 'description'\n            header_row = True\n        elif (('dr' in value.lower()) and ('cr' in value.lower()) and (len(value) < 6)):\n            result[i] = 'drcr'\n            header_row = True\n        elif ('debit' in value.lower()):\n            result[i] = 'dr'\n            header_row = True\n        elif (('dr' in value.lower()) and (len(value) < 5)):\n            result[i] = 'dr'\n            header_row = True\n        elif ('credit' in value.lower()):\n            result[i] = 'cr'\n            header_row = True\n        elif (('cr' in value.lower()) and (len(value) < 5)):\n            result[i] = 'cr'\n            header_row = True\n    if (not header_row):\n        for (i, value) in enumerate(transactions[1]):\n            if is_number(value):\n                result[i] = 'drcr'\n            elif is_date(value):\n                result[i] = 'date'\n            elif (len(value) > 10):\n                result[i] = 'description'\n    return (result, header_row)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train():\n    '\\n    Train the word2Vec model\\n    '\n    sentences = build_sentences()\n    model = word2vec.Word2Vec(sentences=sentences, min_count=3, workers=multiprocessing.cpu_count())\n    model.build_vocab(sentences)\n    model.train(sentences=sentences)\n    if (not os.path.exists(os.path.join(DATASET_PATH, 'trained'))):\n        os.mkdir(os.path.join(DATASET_PATH, 'trained'))\n    model.save(trained_model)\n"}
{"label_name":"forward","label":3,"method_name":"conv2d_forward_naive","method":"\n\ndef conv2d_forward_naive(input, filter, group, conv_param, padding_algorithm='EXPLICIT', data_format='NCHW'):\n    if (padding_algorithm not in ['SAME', 'VALID', 'EXPLICIT']):\n        raise ValueError((\"Unknown Attr(padding_algorithm): '%s'. It can only be 'SAME' or 'VALID'.\" % str(padding_algorithm)))\n    if (data_format not in ['NCHW', 'NHWC']):\n        raise ValueError((\"Unknown Attr(data_format): '%s' .It can only be 'NCHW' or 'NHWC'.\" % str(data_format)))\n    channel_last = (data_format == 'NHWC')\n    if channel_last:\n        input = np.transpose(input, [0, 3, 1, 2])\n    (in_n, in_c, in_h, in_w) = input.shape\n    (f_n, f_c, f_h, f_w) = filter.shape\n    out_n = in_n\n    out_c = f_n\n    assert ((f_c * group) == in_c)\n    assert (np.mod(out_c, group) == 0)\n    sub_out_c = (out_c \/\/ group)\n    sub_f_n = (f_n \/\/ group)\n    (stride, pad, dilation) = (conv_param['stride'], conv_param['pad'], conv_param['dilation'])\n\n    def _get_padding_with_SAME(input_shape, pool_size, pool_stride):\n        padding = []\n        for (input_size, filter_size, stride_size) in zip(input_shape, pool_size, pool_stride):\n            out_size = int((((input_size + stride_size) - 1) \/ stride_size))\n            pad_sum = np.max((((((out_size - 1) * stride_size) + filter_size) - input_size), 0))\n            pad_0 = int((pad_sum \/ 2))\n            pad_1 = int((pad_sum - pad_0))\n            padding.append(pad_0)\n            padding.append(pad_1)\n        return padding\n    ksize = filter.shape[2:4]\n    if (padding_algorithm == 'VALID'):\n        pad = [0, 0, 0, 0]\n    elif (padding_algorithm == 'SAME'):\n        dilation = [1, 1]\n        input_data_shape = input.shape[2:4]\n        pad = _get_padding_with_SAME(input_data_shape, ksize, stride)\n    (pad_h_0, pad_h_1) = (pad[0], pad[0])\n    (pad_w_0, pad_w_1) = (pad[1], pad[1])\n    if (len(pad) == 4):\n        (pad_h_0, pad_h_1) = (pad[0], pad[1])\n        (pad_w_0, pad_w_1) = (pad[2], pad[3])\n    out_h = (1 + ((((in_h + pad_h_0) + pad_h_1) - ((dilation[0] * (f_h - 1)) + 1)) \/\/ stride[0]))\n    out_w = (1 + ((((in_w + pad_w_0) + pad_w_1) - ((dilation[1] * (f_w - 1)) + 1)) \/\/ stride[1]))\n    out = np.zeros((out_n, out_c, out_h, out_w))\n    d_bolck_h = ((dilation[0] * (f_h - 1)) + 1)\n    d_bolck_w = ((dilation[1] * (f_w - 1)) + 1)\n    input_pad = np.pad(input, ((0, 0), (0, 0), (pad_h_0, pad_h_1), (pad_w_0, pad_w_1)), mode='constant', constant_values=0)\n    filter_dilation = np.zeros((f_n, f_c, d_bolck_h, d_bolck_w))\n    filter_dilation[:, :, 0:d_bolck_h:dilation[0], 0:d_bolck_w:dilation[1]] = filter\n    for i in range(out_h):\n        for j in range(out_w):\n            for g in range(group):\n                input_pad_masked = input_pad[:, (g * f_c):((g + 1) * f_c), (i * stride[0]):((i * stride[0]) + d_bolck_h), (j * stride[1]):((j * stride[1]) + d_bolck_w)]\n                f_sub = filter_dilation[(g * sub_f_n):((g + 1) * sub_f_n), :, :, :]\n                for k in range(sub_out_c):\n                    out[:, ((g * sub_out_c) + k), i, j] = np.sum((input_pad_masked * f_sub[k, :, :, :]), axis=(1, 2, 3))\n    if channel_last:\n        out = np.transpose(out, [0, 2, 3, 1])\n    return (out, in_n, out_h, out_w, out_c)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(num_epochs):\n    hvd.init()\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    if gpus:\n        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n    ((mnist_images, mnist_labels), _) = tf.keras.datasets.mnist.load_data(path=('mnist-%d.npz' % hvd.rank()))\n    dataset = tf.data.Dataset.from_tensor_slices((tf.cast((mnist_images[(..., tf.newaxis)] \/ 255.0), tf.float32), tf.cast(mnist_labels, tf.int64)))\n    dataset = dataset.repeat().shuffle(10000).batch(128)\n    mnist_model = tf.keras.Sequential([tf.keras.layers.Conv2D(32, [3, 3], activation='relu'), tf.keras.layers.Conv2D(64, [3, 3], activation='relu'), tf.keras.layers.MaxPooling2D(pool_size=(2, 2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(10, activation='softmax')])\n    scaled_lr = (0.001 * hvd.size())\n    opt = tf.optimizers.Adam(scaled_lr)\n    opt = hvd.DistributedOptimizer(opt)\n    mnist_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'], experimental_run_tf_function=False)\n    callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0), hvd.callbacks.MetricAverageCallback(), hvd.callbacks.LearningRateWarmupCallback(initial_lr=scaled_lr, warmup_epochs=3, verbose=1)]\n    if (hvd.rank() == 0):\n        callbacks.append(tf.keras.callbacks.ModelCheckpoint('.\/checkpoint-{epoch}.h5'))\n    verbose = (1 if (hvd.rank() == 0) else 0)\n    mnist_model.fit(dataset, steps_per_epoch=(500 \/\/ hvd.size()), callbacks=callbacks, epochs=num_epochs, verbose=verbose)\n"}
{"label_name":"forward","label":3,"method_name":"_list_forward","method":"\n\ndef _list_forward(layer, Xs, is_train):\n    list2padded = layer.ops.list2padded\n    padded2list = layer.ops.padded2list\n    (Yp, get_dXp) = layer(list2padded(Xs), is_train)\n\n    def backprop(dYs):\n        return padded2list(get_dXp(list2padded(dYs)))\n    return (padded2list(Yp), backprop)\n"}
{"label_name":"train","label":0,"method_name":"train_word2vec","method":"\n\ndef train_word2vec():\n    '\\n    train word2vec model\\n    :return:\\n    '\n\n    class MyCorpus(object):\n\n        def __init__(self):\n            pass\n\n        def __iter__(self):\n            for fname in os.listdir(DOCUMENT_DIR):\n                text = read_document_from_text(os.path.join(DOCUMENT_DIR, fname))\n                segmented_words = '\/'.join(cut_words(''.join(text))).split('\/')\n                (yield segmented_words)\n    sentences = MyCorpus()\n    model = gensim.models.Word2Vec(sentences, workers=8)\n    model.save(WORD2VEC_SAVE_PATH)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(x, y, **kwargs):\n    N = np.shape(x)[0]\n    C = np.size(np.unique(y))\n    max_its = 100\n    alpha_choice = 1\n    cost_name = 'softmax'\n    w = (0.1 * np.random.randn((N + 1), 1))\n    optimizer = 'gradient_descent'\n    if ('max_its' in kwargs):\n        max_its = kwargs['max_its']\n    if ('alpha_choice' in kwargs):\n        alpha_choice = kwargs['alpha_choice']\n    if ('cost_name' in kwargs):\n        cost_name = kwargs['cost_name']\n    if ('w' in kwargs):\n        w = kwargs['w']\n    if ('optimizer' in kwargs):\n        optimizer = kwargs['optimizer']\n    epsilon = (10 ** (- 7))\n    if ('epsilon' in kwargs):\n        epsilon = kwargs['epsilon']\n    weight_histories = []\n    for c in range(0, C):\n        y_temp = copy.deepcopy(y)\n        ind = np.argwhere((y_temp.astype(int) == c))\n        ind = ind[:, 1]\n        ind2 = np.argwhere((y_temp.astype(int) != c))\n        ind2 = ind2[:, 1]\n        y_temp[(0, ind)] = 1\n        y_temp[(0, ind2)] = (- 1)\n        cost = cost_lib.choose_cost(x, y_temp, cost_name)\n        weight_history = 0\n        cost_history = 0\n        if (optimizer == 'gradient_descent'):\n            (weight_history, cost_history) = optimizers.gradient_descent(cost, alpha_choice, max_its, w)\n        if (optimizer == 'newtons_method'):\n            (weight_history, cost_history) = optimizers.newtons_method(cost, max_its, w=w, epsilon=epsilon)\n        weight_histories.append(copy.deepcopy(weight_history))\n    R = len(weight_histories[0])\n    combined_weights = []\n    for r in range(R):\n        a = []\n        for c in range(C):\n            a.append(weight_histories[c][r])\n        a = np.array(a).T\n        a = a[0, :, :]\n        combined_weights.append(a)\n    counter = cost_lib.choose_cost(x, y, 'multiclass_counter')\n    count_history = [counter(v) for v in combined_weights]\n    return (combined_weights, count_history)\n"}
{"label_name":"save","label":1,"method_name":"save_model","method":"\n\ndef save_model(nameprefix, model):\n    ' Save a keras sequential model into files.\\n\\n    Given a keras sequential model, save the model with the given file path prefix.\\n    It saves the model into a JSON file, and an HDF5 file (.h5).\\n\\n    :param nameprefix: Prefix of the paths of the model files\\n    :param model: keras sequential model to be saved\\n    :return: None\\n    :type nameprefix: str\\n    :type model: keras.models.Model\\n    '\n    model_json = model.to_json()\n    open((nameprefix + '.json'), 'w').write(model_json)\n    model.save_weights((nameprefix + '.h5'))\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input, label):\n    optimizer.zero_grad()\n    output = model(input)\n    loss = criterion(output, label)\n    loss.backward()\n    optimizer.step()\n    return (output, loss.data[0])\n"}
{"label_name":"process","label":2,"method_name":"process_body","method":"\n\ndef process_body():\n    'Return (params, method) from request body.'\n    try:\n        return xmlrpc_loads(cherrypy.request.body.read())\n    except Exception:\n        return (('ERROR PARAMS',), 'ERRORMETHOD')\n"}
{"label_name":"train","label":0,"method_name":"read_train","method":"\n\ndef read_train(filename):\n    (X, Y) = ([], [])\n    with open(filename, 'r', encoding='big5') as f:\n        count = 0\n        for line in list(csv.reader(f))[1:]:\n            Y.append(float(line[0]))\n            X.append([float(x) for x in line[1].split()])\n            count += 1\n            print(('\\rX_train: ' + repr(count)), end='', flush=True)\n        print('', flush=True)\n    return (np.array(X), np_utils.to_categorical(Y, CATEGORY))\n"}
{"label_name":"train","label":0,"method_name":"announce_training","method":"\n\ndef announce_training(args, dataset_len, t_total):\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', dataset_len)\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per device = %d', args.per_device_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accum) = %d', ((args.per_device_train_batch_size * args.gradient_accumulation_steps) * args.num_workers))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n"}
{"label_name":"train","label":0,"method_name":"train_maxent_classifier_with_gis","method":"\n\ndef train_maxent_classifier_with_gis(train_toks, trace=3, encoding=None, labels=None, **cutoffs):\n    '\\n    Train a new ``ConditionalExponentialClassifier``, using the given\\n    training samples, using the Generalized Iterative Scaling\\n    algorithm.  This ``ConditionalExponentialClassifier`` will encode\\n    the model that maximizes entropy from all the models that are\\n    empirically consistent with ``train_toks``.\\n\\n    :see: ``train_maxent_classifier()`` for parameter descriptions.\\n    '\n    cutoffs.setdefault('max_iter', 100)\n    cutoffchecker = CutoffChecker(cutoffs)\n    if (encoding is None):\n        encoding = GISEncoding.train(train_toks, labels=labels)\n    if (not hasattr(encoding, 'C')):\n        raise TypeError('The GIS algorithm requires an encoding that defines C (e.g., GISEncoding).')\n    Cinv = (1.0 \/ encoding.C)\n    empirical_fcount = calculate_empirical_fcount(train_toks, encoding)\n    unattested = set(numpy.nonzero((empirical_fcount == 0))[0])\n    weights = numpy.zeros(len(empirical_fcount), 'd')\n    for fid in unattested:\n        weights[fid] = numpy.NINF\n    classifier = ConditionalExponentialClassifier(encoding, weights)\n    log_empirical_fcount = numpy.log2(empirical_fcount)\n    del empirical_fcount\n    if (trace > 0):\n        print(('  ==> Training (%d iterations)' % cutoffs['max_iter']))\n    if (trace > 2):\n        print()\n        print('      Iteration    Log Likelihood    Accuracy')\n        print('      ---------------------------------------')\n    try:\n        while True:\n            if (trace > 2):\n                ll = (cutoffchecker.ll or log_likelihood(classifier, train_toks))\n                acc = (cutoffchecker.acc or accuracy(classifier, train_toks))\n                iternum = cutoffchecker.iter\n                print(('     %9d    %14.5f    %9.3f' % (iternum, ll, acc)))\n            estimated_fcount = calculate_estimated_fcount(classifier, train_toks, encoding)\n            for fid in unattested:\n                estimated_fcount[fid] += 1\n            log_estimated_fcount = numpy.log2(estimated_fcount)\n            del estimated_fcount\n            weights = classifier.weights()\n            weights += ((log_empirical_fcount - log_estimated_fcount) * Cinv)\n            classifier.set_weights(weights)\n            if cutoffchecker.check(classifier, train_toks):\n                break\n    except KeyboardInterrupt:\n        print('      Training stopped: keyboard interrupt')\n    except:\n        raise\n    if (trace > 2):\n        ll = log_likelihood(classifier, train_toks)\n        acc = accuracy(classifier, train_toks)\n        print(('         Final    %14.5f    %9.3f' % (ll, acc)))\n    return classifier\n"}
{"label_name":"process","label":2,"method_name":"reprocess_row","method":"\n\n@numba.njit()\ndef reprocess_row(probabilities, k=15, n_iters=32):\n    target = np.log2(k)\n    lo = 0.0\n    hi = NPY_INFINITY\n    mid = 1.0\n    for n in range(n_iters):\n        psum = 0.0\n        for j in range(probabilities.shape[0]):\n            psum += pow(probabilities[j], mid)\n        if (np.fabs((psum - target)) < SMOOTH_K_TOLERANCE):\n            break\n        if (psum < target):\n            hi = mid\n            mid = ((lo + hi) \/ 2.0)\n        else:\n            lo = mid\n            if (hi == NPY_INFINITY):\n                mid *= 2\n            else:\n                mid = ((lo + hi) \/ 2.0)\n    return np.power(probabilities, mid)\n"}
{"label_name":"save","label":1,"method_name":"save_signature","method":"\n\ndef save_signature(filename, selected, threshold=0.75):\n    'Save signature summary.'\n    with open(filename, 'w') as f:\n        line_drawn = False\n        for k in reversed(sorted(selected, key=selected.__getitem__)):\n            if ((not line_drawn) and (float(selected[k]) < threshold)):\n                line_drawn = True\n                f.write(('=' * 40))\n                f.write('\\n')\n            f.write('{} : {}\\n'.format(k, (selected[k] * 100.0)))\n"}
{"label_name":"forward","label":3,"method_name":"_apply_forwards","method":"\n\ndef _apply_forwards(fstruct, forward, fs_class, visited):\n    '\\n    Replace any feature structure that has a forward pointer with\\n    the target of its forward pointer (to preserve reentrancy).\\n    '\n    while (id(fstruct) in forward):\n        fstruct = forward[id(fstruct)]\n    if (id(fstruct) in visited):\n        return\n    visited.add(id(fstruct))\n    if _is_mapping(fstruct):\n        items = fstruct.items()\n    elif _is_sequence(fstruct):\n        items = enumerate(fstruct)\n    else:\n        raise ValueError('Expected mapping or sequence')\n    for (fname, fval) in items:\n        if isinstance(fval, fs_class):\n            while (id(fval) in forward):\n                fval = forward[id(fval)]\n            fstruct[fname] = fval\n            _apply_forwards(fval, forward, fs_class, visited)\n    return fstruct\n"}
{"label_name":"predict","label":4,"method_name":"predict_dt","method":"\n\ndef predict_dt():\n    data_with_idx = data_dt.zipWithIndex().map((lambda k, v: (v, k)))\n    test = data_with_idx.sample(False, 0.2, 42)\n    train = data_with_idx.subtractByKey(test)\n    test_data = test.map((lambda idx, p: p))\n    train_data = train.map((lambda idx, p: p))\n    maxDepths = [1, 2, 3, 4, 5, 10, 20]\n    maxBins = [2, 4, 8, 16, 32, 64, 100]\n    m = {}\n    for maxDepth in maxDepths:\n        for maxBin in maxBins:\n            metrics = evaluate_dt(train_data, test_data, maxDepth, maxBin)\n            print(('metrics in maxDepth: %d; maxBins: %d' % (maxDepth, maxBin)))\n            print(metrics)\n            m[('maxDepth:%d;maxBins:%d' % (maxDepth, maxBin))] = metrics[2]\n    mSort = sorted(m.iteritems(), key=operator.itemgetter(1), reverse=True)\n    print(mSort)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(trainset=trainset, valset=valset, weight_file=NB_WEIGHTS):\n    'trains classifier on name->gender\\n    \\n    Args:\\n        trainset: list of name->gender tuple pairs for training\\n        valset (opt): list of name->gender tuple pairs to validation\\n        weight_file: filename to save classifer weights\\n\\n    '\n    start = time.time()\n    print(('Training Naive Bayes Classifer on %d examples (%s)' % (len(trainset), time_since(start))))\n    trainset = apply_features(_gender_features, trainset, labeled=True)\n    classifier = nltk.NaiveBayesClassifier.train(trainset)\n    with open(weight_file, 'wb') as f:\n        pickle.dump(classifier, f)\n        f.close()\n    print(('Training complete. (%s)' % time_since(start)))\n    if ((valset is not None) and (len(valset) > 0)):\n        valset = apply_features(_gender_features, valset, labeled=True)\n        acc = nltk.classify.accuracy(classifier, valset)\n        print(('Validation accuracy is %.2f%% on %d examples (%s)' % (acc, len(valset), time_since(start))))\n"}
{"label_name":"save","label":1,"method_name":"save_configfile","method":"\n\ndef save_configfile():\n    with open(CONFIG_FILE, 'w') as configfile:\n        config.write(configfile)\n"}
{"label_name":"train","label":0,"method_name":"read_train_data","method":"\n\ndef read_train_data(path):\n    file_path = os.path.normpath(path)\n    reader = Reader(line_format='timestamp user item rating', sep=',')\n    data = Dataset.load_from_file(file_path, reader=reader)\n    return data\n"}
{"label_name":"process","label":2,"method_name":"load_preprocess","method":"\n\ndef load_preprocess():\n    '\\n    Load the Preprocessed Training data and return them in batches of <batch_size> or less\\n    '\n    return pickle.load(open('preprocess.p', mode='rb'))\n"}
{"label_name":"train","label":0,"method_name":"make_train_batch","method":"\n\ndef make_train_batch(env, batch_size, target_type):\n    obs_batch = []\n    target_batch = []\n    for i in range(batch_size):\n        env.index = np.random.randint(high=env.data_length, low=env.obs_steps)\n        env.portfolio_df = pd.DataFrame()\n        env.balance = env.init_balance\n        obs = env.get_observation(True).astype(np.float32).values\n        xp = chainer.cuda.get_array_module(obs)\n        obs_batch.append(obs[:(- 1)])\n        target_batch.append(get_target(obs[(- 1)], target_type))\n    obs_batch = batch_states(obs_batch, xp, phi)\n    target_batch = np.swapaxes(batch_states(target_batch, xp, phi), 3, 2)\n    return (obs_batch, target_batch)\n"}
{"label_name":"process","label":2,"method_name":"process_dollars","method":"\n\ndef process_dollars(app, docname, source):\n    dollars_to_math(source)\n"}
{"label_name":"save","label":1,"method_name":"save_data","method":"\n\ndef save_data(df, category):\n    store = pd.HDFStore(((processed_path + category) + '.h5'))\n    store['data'] = df\n    store.close()\n"}
{"label_name":"train","label":0,"method_name":"load_train_data","method":"\n\ndef load_train_data(filename):\n    ext = filename.split('.')[(- 1)]\n    if (ext == 'csv'):\n        return read_smiles_csv(filename)\n    if (ext == 'smi'):\n        return read_smi(filename)\n    else:\n        raise ValueError('data is not smi or csv!')\n    return\n"}
{"label_name":"forward","label":3,"method_name":"crf_decode_forward","method":"\n\ndef crf_decode_forward(inputs: TensorLike, state: TensorLike, transition_params: TensorLike, sequence_lengths: TensorLike) -> Tuple[(tf.Tensor, tf.Tensor)]:\n    \"Computes forward decoding in a linear-chain CRF.\\n\\n    Args:\\n      inputs: A [batch_size, num_tags] matrix of unary potentials.\\n      state: A [batch_size, num_tags] matrix containing the previous step's\\n            score values.\\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\\n\\n    Returns:\\n      output: A [batch_size, num_tags * 2] matrix of backpointers and scores.\\n      new_state: A [batch_size, num_tags] matrix of new score values.\\n    \"\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    mask = tf.sequence_mask(sequence_lengths, tf.shape(inputs)[1])\n    crf_fwd_cell = CrfDecodeForwardRnnCell(transition_params)\n    crf_fwd_layer = tf.keras.layers.RNN(crf_fwd_cell, return_sequences=True, return_state=True)\n    return crf_fwd_layer(inputs, state, mask=mask)\n"}
{"label_name":"predict","label":4,"method_name":"plot_prediction","method":"\n\ndef plot_prediction(fig, ax):\n    Xnew = np.linspace((X.min() - 0.5), (X.max() + 0.5), 100).reshape((- 1), 1)\n    Ypred = model.predict_f_samples(Xnew, full_cov=True, num_samples=20)\n    ax.plot(Xnew.flatten(), np.squeeze(Ypred).T, 'C1', alpha=0.2)\n    ax.plot(X, Y, 'o')\n"}
{"label_name":"process","label":2,"method_name":"process_config","method":"\n\ndef process_config(jsonfile):\n    'Processing the configuration JSON for the creating the directory paths.\\n\\n    Args:\\n        jsonfile (JSON object): The JSON configuration file.\\n\\n    Returns:\\n        Bunch object: the JSON configuration Bunch object.\\n\\n    '\n    (config, _) = _get_config_from_json(jsonfile)\n    config.summary_dir = os.path.join('experiments', config.exp_name, 'summary')\n    config.checkpoint_dir = os.path.join('experiments', config.exp_name, 'checkpoint')\n    return config\n"}
{"label_name":"train","label":0,"method_name":"run_train","method":"\n\ndef run_train():\n    dictionary = DP.read_dict(dict_file)\n    train_label = DP.read_train(train_label_file)\n    train = DataSet(train_path, train_label, len(dictionary), dictionary[EOS_tag])\n    N_input = train.datalen\n    N_iter = ((N_input * N_epoch) \/\/ batch_size)\n    print(('Total training steps: %d' % N_iter))\n    model = AttentionModel(image_dim=train.feat_dim, vocab_size=train.vocab_size, N_hidden=N_hidden, N_video_step=train.feat_timestep, N_caption_step=train.maxseqlen, batch_size=batch_size)\n    (tf_loss, tf_video_train, tf_caption_train, tf_caption_mask_train, _) = model.build_train_model(dictionary)\n    tf_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n    (valid_feat, ID, inv_dictionary, ref) = load_valid(valid_id_file, valid_path)\n    (tf_video, tf_caption, _) = model.build_valid_model(dictionary)\n    init = tf.global_variables_initializer()\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, device_count={'GPU': 1})) as sess:\n        sess.run(init)\n        step = 0\n        t = time.time()\n        while (step < N_iter):\n            (batch_x, batch_y) = train.next_batch(batch_size=batch_size)\n            y = np.full((batch_size, train.maxseqlen), dictionary[EOS_tag])\n            y_mask = np.zeros(y.shape)\n            for (i, caption) in enumerate(batch_y):\n                y[i, :len(caption)] = caption\n                y_mask[i, :len(caption)] = 1\n            sess.run(tf_optimizer, feed_dict={tf_video_train: batch_x, tf_caption_train: y, tf_caption_mask_train: y_mask})\n            if ((step % display_step) == 0):\n                used_time = (time.time() - t)\n                t = time.time()\n                loss = sess.run(tf_loss, feed_dict={tf_video_train: batch_x, tf_caption_train: y, tf_caption_mask_train: y_mask})\n                print((((((((str(step) + '\/') + str(N_iter)) + ' step: loss = ') + str(loss)) + ' time = ') + str(used_time)) + ' secs'))\n                model.save_model(sess, model_file)\n            if (((step % valid_step) == 0) and (step > 0)):\n                result = predict(sess, valid_feat, ID, inv_dictionary, tf_caption, tf_video)\n                print(result)\n                bleu = evaluate_list(result, ref)\n                print(((((str(step) + '\/') + str(N_iter)) + ' step: bleu = ') + str(bleu)))\n                with open('seq2seq_random_b256.csv', 'a') as f:\n                    writer = csv.writer(f)\n                    writer.writerow([step, bleu])\n            step += 1\n        model.save_model(sess, model_file)\n    return\n"}
{"label_name":"process","label":2,"method_name":"preprocess_review","method":"\n\ndef preprocess_review(review):\n    review_text = BeautifulSoup(review).get_text()\n    review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n    return review_text.lower()\n"}
{"label_name":"save","label":1,"method_name":"save","method":"\n\ndef save(filename_audio, filename_jam, jam, strict=True, fmt='auto', **kwargs):\n    'Save a muda jam to disk\\n\\n    Parameters\\n    ----------\\n    filename_audio: str\\n        The path to store the audio file\\n\\n    filename_jam: str\\n        The path to store the jams object\\n\\n    strict: bool\\n        Strict safety checking for jams output\\n\\n    fmt : str\\n        Output format parameter for `jams.JAMS.save`\\n\\n    kwargs\\n        Additional parameters to `soundfile.write`\\n    '\n    y = jam.sandbox.muda._audio['y']\n    sr = jam.sandbox.muda._audio['sr']\n    psf.write(filename_audio, y, sr, **kwargs)\n    jam.save(filename_jam, strict=strict, fmt=fmt)\n"}
{"label_name":"process","label":2,"method_name":"preprocessing_inference_tta","method":"\n\ndef preprocessing_inference_tta(config, model_name='unet'):\n    if (config.execution.loader_mode == 'crop_and_pad'):\n        Loader = loaders.ImageSegmentationLoaderCropPadTTA\n    elif (config.execution.loader_mode == 'resize'):\n        Loader = loaders.ImageSegmentationLoaderResizeTTA\n    else:\n        raise NotImplementedError\n    if (config.loader.dataset_params.image_source == 'memory'):\n        reader_inference = Step(name='reader_inference', transformer=loaders.ImageReader(**config.reader[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)\n        tta_generator = Step(name='tta_generator', transformer=loaders.TestTimeAugmentationGenerator(**config.tta_generator), input_steps=[reader_inference], adapter={'X': [('reader_inference', 'X')]}, cache_dirpath=config.env.cache_dirpath)\n    elif (config.loader.dataset_params.image_source == 'disk'):\n        reader_inference = Step(name='reader_inference', transformer=XYSplit(**config.xy_splitter[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)\n        tta_generator = Step(name='tta_generator', transformer=loaders.MetaTestTimeAugmentationGenerator(**config.tta_generator), input_steps=[reader_inference], adapter={'X': [('reader_inference', 'X')]}, cache_dirpath=config.env.cache_dirpath)\n    else:\n        raise NotImplementedError\n    loader = Step(name='loader', transformer=Loader(**config.loader), input_data=['specs'], input_steps=[tta_generator], adapter={'X': ([(tta_generator.name, 'X_tta')], squeeze_inputs_if_needed), 'tta_params': [(tta_generator.name, 'tta_params')]}, cache_dirpath=config.env.cache_dirpath, cache_output=True)\n    return (loader, tta_generator)\n"}
{"label_name":"process","label":2,"method_name":"postprocess_fn_add_next_actions_for_sarsa","method":"\n\ndef postprocess_fn_add_next_actions_for_sarsa(policy: Policy, batch: SampleBatch, other_agent=None, episode=None) -> SampleBatch:\n    'Add next_actions to SampleBatch for SARSA training'\n    if (policy.config['slateq_strategy'] == 'SARSA'):\n        if (not batch['dones'][(- 1)]):\n            raise RuntimeError(f'Expected a complete episode in each sample batch. But this batch is not: {batch}.')\n        batch['next_actions'] = np.roll(batch['actions'], (- 1), axis=0)\n    return batch\n"}
{"label_name":"train","label":0,"method_name":"watch_local_trainers","method":"\n\ndef watch_local_trainers(procs, nranks):\n    try:\n        error = False\n        error_rank = []\n        alive = False\n        for p in procs:\n            if (p.log_fn and (p.local_rank == 0)):\n                pull_worker_log(p)\n            ret = p.proc.poll()\n            if (ret is None):\n                alive = True\n            elif (ret != 0):\n                error = True\n                error_rank.append(p.rank)\n        if error:\n            terminate_local_procs(procs)\n            exit(1)\n    except KeyboardInterrupt:\n        logger.warning('KeyboardInterrupt, exit')\n        terminate_local_procs(procs)\n        raise\n    except SystemExit:\n        logger.error('ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.'.format(nranks, error_rank))\n        terminate_local_procs(procs)\n        raise\n    except:\n        logger.error('ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.'.format(nranks, error_rank))\n        terminate_local_procs(procs)\n        raise\n    return alive\n"}
{"label_name":"predict","label":4,"method_name":"evaluate_predictions","method":"\n\ndef evaluate_predictions(sparql, gps, gtps, gtp_predicted_fused_targets=None, fusion_methods=None):\n    recall = 0\n    method_idxs = defaultdict(list)\n    method_order = []\n    res_lens = []\n    timeout = calibrate_query_timeout(sparql)\n    for (i, (source, target)) in enumerate(gtps, 1):\n        print(('%d\/%d: predicting target for %s (ground truth: %s):' % (i, len(gtps), source.n3(), target.n3())))\n        if gtp_predicted_fused_targets:\n            method_res = gtp_predicted_fused_targets[(i - 1)]\n        else:\n            method_res = predict_fused_targets(sparql, timeout, gps, source, fusion_methods=fusion_methods)\n        once = False\n        if (not method_order):\n            method_order = method_res.keys()\n        for (method, res) in method_res.items():\n            idx = find_in_prediction(res, target)\n            if (not once):\n                once = True\n                if (idx < 0):\n                    print('  target not found')\n                else:\n                    recall += 1\n                n = len(res)\n                res_lens.append(n)\n                print(('  result list length: %d' % n))\n            method_idxs[method].append(idx)\n            print_prediction_results(method, res, target, idx)\n    recall \/= len(gtps)\n    print(('Ground Truth Pairs: %s' % gtps))\n    print(('Result list lenghts: %s' % res_lens))\n    print(('Recall of test set: %.5f' % recall))\n    for (method, indices) in [(m, method_idxs[m]) for m in method_order]:\n        print((\"\\nIndices for method %s:\\n'%s': %s\" % (method, method, indices)))\n        avg_idx = np.average([i for i in indices if (i >= 0)])\n        median_idx = np.median([i for i in indices if (i >= 0)])\n        ranks = (np.array(indices, dtype='f8') + 1)\n        mrr = (np.sum((1 \/ ranks[(ranks > 0)])) \/ len(indices))\n        ndcg = (np.sum((1 \/ np.log2((1 + ranks[(ranks > 0)])))) \/ len(indices))\n        print(('  Avg. index %s: %.3f, Median index: %.3f\\n  MAP (MRR): %.3f, NDCG: %.3f' % (method, avg_idx, median_idx, mrr, ndcg)))\n        recalls_at = [(k, (len([True for i in indices if (k > i >= 0)]) \/ len(indices))) for k in (1, 2, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 75, 100)]\n        print(('         k:\\t%s' % '\\t'.join((('% 5d' % k) for (k, r) in recalls_at))))\n        print(('  recall@k:\\t%s' % '\\t'.join((('%.3f' % r) for (k, r) in recalls_at))))\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(imagepath, target_x, target_y, name, model):\n    if (imagepath.startswith('http:\/\/') or imagepath.startswith('https:\/\/') or imagepath.startswith('ftp:\/\/')):\n        response = requests.get(imagepath)\n        img = Image.open(BytesIO(response.content))\n        img = img.resize((target_x, target_y))\n    else:\n        if (not os.path.exists(imagepath)):\n            raise Exception('Input image file does not exist')\n        img = image.load_img(imagepath, target_size=(target_x, target_y))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = processInputImage(name, x)\n    preds = decodePrediction(name, model.predict(x))\n    result = []\n    for p in preds[0]:\n        result.append({'synset': p[0], 'text': p[1], 'prediction': float('{0:.2f}'.format((p[2] * 100)))})\n    return json.loads(jsonpickle.encode(result, unpicklable=False))\n"}
{"label_name":"save","label":1,"method_name":"save_image","method":"\n\ndef save_image(image, filename):\n    '\\n    Saves unscaled Tensor Images.\\n    Args:\\n      image: 3D image tensor. [height, width, channels]\\n      filename: Name of the file to save to.\\n  '\n    if (not isinstance(image, Image.Image)):\n        image = tf.clip_by_value(image, 0, 255)\n        image = Image.fromarray(tf.cast(image, tf.uint8).numpy())\n    image.save(('%s.jpg' % filename))\n    print(('Saved as %s.jpg' % filename))\n"}
{"label_name":"process","label":2,"method_name":"_preprocess_image","method":"\n\ndef _preprocess_image(image):\n    return ((tf.to_float(image) \/ 255) - 0.5)\n"}
{"label_name":"predict","label":4,"method_name":"get_predictions","method":"\n\ndef get_predictions(p, y):\n    thres = get_best_thresholds(p, y)\n    return (thres, get_predictions_using_thresholds(p, thres))\n"}
{"label_name":"process","label":2,"method_name":"post_process_metrics","method":"\n\ndef post_process_metrics(prefix, workers, metrics):\n    'Update current dataset metrics and filter out specific keys.\\n\\n    Args:\\n        prefix (str): Prefix string to be appended\\n        workers (WorkerSet): Set of workers\\n        metrics (dict): Current metrics dictionary\\n    '\n    res = collect_metrics(remote_workers=workers.remote_workers())\n    for key in METRICS_KEYS:\n        metrics[((prefix + '_') + key)] = res[key]\n    return metrics\n"}
{"label_name":"save","label":1,"method_name":"_save","method":"\n\ndef _save(im, fp, filename):\n    if (im.mode == '1'):\n        (rawmode, head) = ('1;I', b'P4')\n    elif (im.mode == 'L'):\n        (rawmode, head) = ('L', b'P5')\n    elif (im.mode == 'I'):\n        if (im.getextrema()[1] < (2 ** 16)):\n            (rawmode, head) = ('I;16B', b'P5')\n        else:\n            (rawmode, head) = ('I;32B', b'P5')\n    elif (im.mode == 'RGB'):\n        (rawmode, head) = ('RGB', b'P6')\n    elif (im.mode == 'RGBA'):\n        (rawmode, head) = ('RGB', b'P6')\n    else:\n        raise IOError(('cannot write mode %s as PPM' % im.mode))\n    fp.write((head + ('\\n%d %d\\n' % im.size).encode('ascii')))\n    if (head == b'P6'):\n        fp.write(b'255\\n')\n    if (head == b'P5'):\n        if (rawmode == 'L'):\n            fp.write(b'255\\n')\n        elif (rawmode == 'I;16B'):\n            fp.write(b'65535\\n')\n        elif (rawmode == 'I;32B'):\n            fp.write(b'2147483648\\n')\n    ImageFile._save(im, fp, [('raw', ((0, 0) + im.size), 0, (rawmode, 0, 1))])\n"}
{"label_name":"save","label":1,"method_name":"distributed_save","method":"\n\ndef distributed_save():\n    f = open('namuwiki_20190312.json')\n    data = ijson.parse(f)\n    p = open('namu_sentences.txt', 'w', encoding='utf-8')\n    for (prefix, event, value) in data:\n        if (prefix == 'item.text'):\n            sentences = re.compile('[\\n]').split(value)\n            for (i, s) in enumerate(sentences):\n                try:\n                    sentences[i] = get_namuwiki_text(s.strip())\n                except Exception as e:\n                    print(e, 'in sentence', s)\n            new_s = '\\n'.join(sentences)\n            new_s = re.compile('[]]|[[]|[|][|]|[{][{][{]|[}][}][}]').sub('', new_s)\n            new_s = re.compile('[.]+|[?]|[\\n]').split(new_s)\n            new_s = [s.strip() for s in new_s if (len(s) > 2)]\n            p.write(('\\n'.join(new_s) + '\\n'))\n            print(new_s)\n        elif (prefix == 'item.title'):\n            p.write((value + '\\n'))\n    p.close()\n"}
{"label_name":"save","label":1,"method_name":"save_abc","method":"\n\ndef save_abc(name, smiles):\n    folder = 'epoch_data'\n    if (not os.path.exists(folder)):\n        os.makedirs(folder)\n    smi_file = os.path.join(folder, (name + '.abc'))\n    with open(smi_file, 'w') as afile:\n        afile.write('\\n'.join(smiles))\n    return\n"}
{"label_name":"train","label":0,"method_name":"get_train_batch","method":"\n\ndef get_train_batch():\n    labels = []\n    arr = np.zeros([batch_size, max_seq_num])\n    for i in range(batch_size):\n        if ((i % 2) == 0):\n            num = randint(1, 11499)\n            labels.append([1, 0])\n        else:\n            num = randint(13499, 24999)\n            labels.append([0, 1])\n        arr[i] = ids[(num - 1):num]\n    return (arr, labels)\n"}
{"label_name":"train","label":0,"method_name":"trainNB","method":"\n\ndef trainNB(data, labels):\n    print('training\\n-----')\n    Pspam = (sum(labels) \/ len(labels))\n    Pham = (1 - Pspam)\n    SN = np.ones(data.shape[1])\n    HN = np.ones(data.shape[1])\n    for (k, d) in enumerate(data):\n        if labels[k]:\n            SN += d\n        else:\n            HN += d\n    PS = (SN \/ sum(SN))\n    PH = (HN \/ sum(HN))\n    print('\u8bad\u7ec3\u5b8c\u6bd5\uff0c \u7528\u65f6\uff1a{}'.format((time.time() - s)))\n    return (Pspam, Pham, PS, PH)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input_variable, target_variable, cnn, cnn_optimizer, criterion, max_length=T):\n    cnn_optimizer.zero_grad()\n    loss = 0\n    cnn_output = cnn(input_variable)\n    loss = criterion(cnn_output, target_variable)\n    loss.backward()\n    cnn_optimizer.step()\n    return loss.data[0]\n"}
{"label_name":"train","label":0,"method_name":"training_set","method":"\n\n@pytest.fixture\ndef training_set():\n    return datasets.one_p_mnist()\n"}
{"label_name":"train","label":0,"method_name":"get_list_trainer","method":"\n\ndef get_list_trainer(chatbot):\n    return ListTrainer(chatbot, show_training_progress=False)\n"}
{"label_name":"save","label":1,"method_name":"save","method":"\n\ndef save(key, data):\n    'Given a key and a data frame, saves it compressed in LZMA'\n    if (not os.path.exists(DATA_PATH)):\n        os.makedirs(DATA_PATH)\n    prefix = date.today().strftime('%Y-%m-%d')\n    filename = '{}-donations-{}.xz'.format(prefix, key)\n    print('Saving {}\u2026'.format(filename))\n    data.to_csv(os.path.join(DATA_PATH, filename), compression='xz')\n"}
{"label_name":"process","label":2,"method_name":"_process_image_files","method":"\n\ndef _process_image_files(name, filenames, texts, labels, num_shards):\n    \"Process and save list of images as TFRecord of Example protos.\\n\\n  Args:\\n    name: string, unique identifier specifying the data set\\n    filenames: list of strings; each string is a path to an image file\\n    texts: list of strings; each string is human readable, e.g. 'dog'\\n    labels: list of integer; each integer identifies the ground truth\\n    num_shards: integer number of shards for this data set.\\n  \"\n    assert (len(filenames) == len(texts))\n    assert (len(filenames) == len(labels))\n    spacing = np.linspace(0, len(filenames), (FLAGS.num_threads + 1)).astype(np.int)\n    ranges = []\n    for i in range((len(spacing) - 1)):\n        ranges.append([spacing[i], spacing[(i + 1)]])\n    print(('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges)))\n    sys.stdout.flush()\n    coord = tf.train.Coordinator()\n    coder = ImageCoder()\n    threads = []\n    for thread_index in range(len(ranges)):\n        args = (coder, thread_index, ranges, name, filenames, texts, labels, num_shards)\n        t = threading.Thread(target=_process_image_files_batch, args=args)\n        t.start()\n        threads.append(t)\n    coord.join(threads)\n    print(('%s: Finished writing all %d images in data set.' % (datetime.now(), len(filenames))))\n    sys.stdout.flush()\n"}
{"label_name":"process","label":2,"method_name":"postprocessing","method":"\n\ndef postprocessing(im, thre_discard, wid_dilate):\n    im = discard(im, thre_discard)\n    im = dilate_im(im, wid_dilate)\n    im = erode_im(im, wid_dilate)\n    im = watershed_im(im)\n    return im\n"}
{"label_name":"predict","label":4,"method_name":"_compute_importance_via_prediction_variance","method":"\n\ndef _compute_importance_via_prediction_variance(new_predictions, original_predictions, original_x, perturbed_x, scaled=True):\n    'Mean absolute change in predictions given perturbations in a feature'\n    changes_in_predictions = abs((new_predictions - original_predictions))\n    if (len(changes_in_predictions.shape) == 1):\n        changes_in_predictions = changes_in_predictions[:, np.newaxis]\n    if scaled:\n        scales = importance_scaler(original_x, perturbed_x)\n        changes_in_predictions = (np.sum(changes_in_predictions, axis=1) * scales)\n    importance = np.mean(changes_in_predictions)\n    return importance\n"}
{"label_name":"predict","label":4,"method_name":"predict_sliding","method":"\n\ndef predict_sliding(model, x_train, test_len, seq_len, window_length, full=True):\n    train_pred = model.predict(x_train)\n    cur_window = train_pred[(- seq_len):]\n    cur_window = cur_window.reshape(window_length)\n    pred = []\n    for i in range(test_len):\n        pred.append(model.predict(np.expand_dims(cur_window, axis=0))[0][0])\n        cur_window = cur_window[1:]\n        cur_window = np.concatenate((cur_window, np.array([pred[i]])))\n    pred = np.array(pred)\n    if full:\n        result = np.concatenate((train_pred.reshape((- 1)), pred))\n    else:\n        result = pred\n    return result\n"}
{"label_name":"process","label":2,"method_name":"_process_frame42","method":"\n\ndef _process_frame42(frame):\n    frame = frame[34:(34 + 160), :160]\n    frame = cv2.resize(frame, (80, 80))\n    frame = cv2.resize(frame, (42, 42))\n    frame = frame.mean(2)\n    frame = frame.astype(np.float32)\n    frame *= (1.0 \/ 255.0)\n    frame = np.reshape(frame, [42, 42, 1])\n    return frame\n"}
{"label_name":"process","label":2,"method_name":"kill_process","method":"\n\ndef kill_process(pid):\n    ' Kills a child process by PID.\\n\\t'\n    max_timeout = 60\n    logger.debug('Sending Ctrl+C to the child process %d', pid)\n    os.kill(pid, signal.SIGINT)\n    start = time.time()\n    while True:\n        now = time.time()\n        result = os.waitpid(pid, os.WNOHANG)\n        if (result != (0, 0)):\n            break\n        if ((now - start) > max_timeout):\n            os.kill(pid, signal.SIGKILL)\n            break\n        logger.debug('Waiting patiently...')\n        time.sleep(0.5)\n"}
{"label_name":"save","label":1,"method_name":"save","method":"\n\ndef save(net, dic, path):\n    dict_m = net.state_dict()\n    dict_m['word_dic'] = dic\n    dict_m['reviews'] = torch.Tensor()\n    dict_m['word.mask'] = torch.Tensor()\n    dict_m['sent.mask'] = torch.Tensor()\n    torch.save(dict_m, path)\n"}
{"label_name":"process","label":2,"method_name":"_process_worker","method":"\n\ndef _process_worker(call_queue, result_queue):\n    'Evaluates calls from call_queue and places the results in result_queue.\\n\\n    This worker is run in a separate process.\\n\\n    Args:\\n        call_queue: A multiprocessing.Queue of _CallItems that will be read and\\n            evaluated by the worker.\\n        result_queue: A multiprocessing.Queue of _ResultItems that will written\\n            to by the worker.\\n        shutdown: A multiprocessing.Event that will be set as a signal to the\\n            worker that it should exit when call_queue is empty.\\n    '\n    while True:\n        call_item = call_queue.get(block=True)\n        if (call_item is None):\n            result_queue.put(None)\n            return\n        try:\n            r = call_item.fn(*call_item.args, **call_item.kwargs)\n        except BaseException:\n            e = sys.exc_info()[1]\n            result_queue.put(_ResultItem(call_item.work_id, exception=e))\n        else:\n            result_queue.put(_ResultItem(call_item.work_id, result=r))\n"}
{"label_name":"save","label":1,"method_name":"save_csv","method":"\n\ndef save_csv(dataset, data_path):\n    dataset_name = '{}-{}'.format(date.today().strftime('%Y-%m-%d'), 'cities-with-tp-url.xz')\n    dataset_path = os.path.join(data_path, dataset_name)\n    dataset.to_csv(dataset_path, compression='xz', encoding='utf-8', index=False)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(batch_size, epochs, learning_rate, num_gpus, training_channel, testing_channel, hosts, current_host, model_dir):\n    (train_labels, train_images) = load_data(training_channel)\n    (test_labels, test_images) = load_data(testing_channel)\n    shard_size = (len(train_images) \/\/ len(hosts))\n    for (i, host) in enumerate(hosts):\n        if (host == current_host):\n            start = (shard_size * i)\n            end = (start + shard_size)\n            break\n    train_iter = mx.io.NDArrayIter(train_images[start:end], train_labels[start:end], batch_size, shuffle=True)\n    val_iter = mx.io.NDArrayIter(test_images, test_labels, batch_size)\n    logging.getLogger().setLevel(logging.DEBUG)\n    kvstore = ('local' if (len(hosts) == 1) else 'dist_sync')\n    mlp_model = mx.mod.Module(symbol=build_graph(), context=get_train_context(num_gpus))\n    mlp_model.fit(train_iter, eval_data=val_iter, kvstore=kvstore, optimizer='sgd', optimizer_params={'learning_rate': learning_rate}, eval_metric='acc', batch_end_callback=mx.callback.Speedometer(batch_size, 100), num_epoch=epochs)\n    if ((len(hosts) == 1) or (current_host == hosts[0])):\n        save(model_dir, mlp_model)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(n_iter: int=100) -> None:\n    nlp = spacy.load('en_core_web_sm')\n    if ('ner' not in nlp.pipe_names):\n        ner = nlp.create_pipe('ner')\n        nlp.add_pipe(ner)\n    else:\n        ner = nlp.get_pipe('ner')\n    for (_, annotations) in TRAIN_DATA:\n        for ent in annotations.get('entities'):\n            ner.add_label(ent[2])\n    pipe_exceptions = ['ner', 'trf_wordpiecer', 'trf_tok2vec']\n    other_pipes = [pipe for pipe in nlp.pipe_names if (pipe not in pipe_exceptions)]\n    with nlp.disable_pipes(*other_pipes):\n        nlp.begin_training()\n        for step_n in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            batches = minibatch(TRAIN_DATA, size=compounding(1.0, 4.0, 1.001))\n            for batch in batches:\n                (texts, annotations) = zip(*batch)\n                nlp.update(texts, annotations, drop=0.5, losses=losses)\n            print('Losses: ', losses)\n            tracking.log_metrics(step=step_n, **losses)\n    nlp.to_disk('custom_spacy_model')\n"}
{"label_name":"train","label":0,"method_name":"str2int_train","method":"\n\ndef str2int_train(datapath, dictionary, NotExist='NotExist'):\n    filenames = [filename for filename in os.listdir(datapath) if os.path.isfile(os.path.join(datapath, filename))]\n    data = []\n    stemmer = nltk.stem.PorterStemmer()\n    for filename in filenames:\n        with open(os.path.join(datapath, filename), 'r', encoding='utf-8', errors='ignore') as f:\n            raw_text = f.read()\n            for sent in nltk.sent_tokenize(raw_text):\n                words = []\n                for word in nltk.word_tokenize(sent):\n                    try:\n                        words.append(stemmer.stem(word))\n                    except:\n                        pass\n                words = [word.lower() for word in words]\n                s = []\n                for w in words:\n                    if (w in dictionary):\n                        s.append(dictionary[w])\n                    else:\n                        s.append(dictionary[NotExist])\n                s = np.array(s)\n                data.append(s)\n    return np.array(data)\n"}
{"label_name":"process","label":2,"method_name":"process_line","method":"\n\ndef process_line(line, filename, line_number, finder=None, comes_from=None, options=None, session=None, wheel_cache=None, constraint=False):\n    'Process a single requirements line; This can result in creating\/yielding\\n    requirements, or updating the finder.\\n\\n    For lines that contain requirements, the only options that have an effect\\n    are from SUPPORTED_OPTIONS_REQ, and they are scoped to the\\n    requirement. Other options from SUPPORTED_OPTIONS may be present, but are\\n    ignored.\\n\\n    For lines that do not contain requirements, the only options that have an\\n    effect are from SUPPORTED_OPTIONS. Options from SUPPORTED_OPTIONS_REQ may\\n    be present, but are ignored. These lines may contain multiple options\\n    (although our docs imply only one is supported), and all our parsed and\\n    affect the finder.\\n\\n    :param constraint: If True, parsing a constraints file.\\n    :param options: OptionParser options that we may update\\n    '\n    parser = build_parser()\n    defaults = parser.get_default_values()\n    defaults.index_url = None\n    if finder:\n        defaults.format_control = finder.format_control\n    (args_str, options_str) = break_args_options(line)\n    if (sys.version_info < (2, 7, 3)):\n        options_str = options_str.encode('utf8')\n    (opts, _) = parser.parse_args(shlex.split(options_str), defaults)\n    line_comes_from = ('%s %s (line %s)' % (('-c' if constraint else '-r'), filename, line_number))\n    if args_str:\n        isolated = (options.isolated_mode if options else False)\n        if options:\n            cmdoptions.check_install_build_global(options, opts)\n        req_options = {}\n        for dest in SUPPORTED_OPTIONS_REQ_DEST:\n            if ((dest in opts.__dict__) and opts.__dict__[dest]):\n                req_options[dest] = opts.__dict__[dest]\n        (yield InstallRequirement.from_line(args_str, line_comes_from, constraint=constraint, isolated=isolated, options=req_options, wheel_cache=wheel_cache))\n    elif opts.editables:\n        isolated = (options.isolated_mode if options else False)\n        default_vcs = (options.default_vcs if options else None)\n        (yield InstallRequirement.from_editable(opts.editables[0], comes_from=line_comes_from, constraint=constraint, default_vcs=default_vcs, isolated=isolated, wheel_cache=wheel_cache))\n    elif (opts.requirements or opts.constraints):\n        if opts.requirements:\n            req_path = opts.requirements[0]\n            nested_constraint = False\n        else:\n            req_path = opts.constraints[0]\n            nested_constraint = True\n        if SCHEME_RE.search(filename):\n            req_path = urllib_parse.urljoin(filename, req_path)\n        elif (not SCHEME_RE.search(req_path)):\n            req_path = os.path.join(os.path.dirname(filename), req_path)\n        parser = parse_requirements(req_path, finder, comes_from, options, session, constraint=nested_constraint, wheel_cache=wheel_cache)\n        for req in parser:\n            (yield req)\n    elif opts.require_hashes:\n        options.require_hashes = opts.require_hashes\n    elif finder:\n        if opts.allow_external:\n            warnings.warn('--allow-external has been deprecated and will be removed in the future. Due to changes in the repository protocol, it no longer has any effect.', RemovedInPip10Warning)\n        if opts.allow_all_external:\n            warnings.warn('--allow-all-external has been deprecated and will be removed in the future. Due to changes in the repository protocol, it no longer has any effect.', RemovedInPip10Warning)\n        if opts.allow_unverified:\n            warnings.warn('--allow-unverified has been deprecated and will be removed in the future. Due to changes in the repository protocol, it no longer has any effect.', RemovedInPip10Warning)\n        if opts.index_url:\n            finder.index_urls = [opts.index_url]\n        if (opts.use_wheel is False):\n            finder.use_wheel = False\n            pip.index.fmt_ctl_no_use_wheel(finder.format_control)\n        if (opts.no_index is True):\n            finder.index_urls = []\n        if opts.extra_index_urls:\n            finder.index_urls.extend(opts.extra_index_urls)\n        if opts.find_links:\n            value = opts.find_links[0]\n            req_dir = os.path.dirname(os.path.abspath(filename))\n            relative_to_reqs_file = os.path.join(req_dir, value)\n            if os.path.exists(relative_to_reqs_file):\n                value = relative_to_reqs_file\n            finder.find_links.append(value)\n        if opts.pre:\n            finder.allow_all_prereleases = True\n        if opts.process_dependency_links:\n            finder.process_dependency_links = True\n        if opts.trusted_hosts:\n            finder.secure_origins.extend((('*', host, '*') for host in opts.trusted_hosts))\n"}
{"label_name":"train","label":0,"method_name":"loadPretrainedW","method":"\n\ndef loadPretrainedW(dictionarySize, retrainW=False):\n    pretrainedWFilePath = (PRETRAINED_W_PATH_TEMPLATE % dictionarySize)\n    logging.info(('GCCNMFPretraining: Loading pretrained W (size %d): %s' % (dictionarySize, pretrainedWFilePath)))\n    if (exists(pretrainedWFilePath) and (not retrainW)):\n        W = np.load(pretrainedWFilePath)\n    else:\n        if retrainW:\n            logging.info(('GCCNMFPretraining: Retraining W, saving as %s...' % pretrainedWFilePath))\n        else:\n            logging.info(('GCCNMFPretraining: Pretrained W not found at %s, creating...' % pretrainedWFilePath))\n        trainV = np.load(CHIME_DATASET_PATH)\n        (W, _) = performKLNMF(trainV, dictionarySize, numIterations=100, sparsityAlpha=0, epsilon=1e-16, seedValue=0)\n        try:\n            makedirs(PRETRAINED_W_DIR)\n        except:\n            pass\n        np.save(pretrainedWFilePath, W)\n    return W\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\n\ndef forward(model: Model[(InT, OutT)], ids: Ints1d, is_train: bool) -> Tuple[(OutT, Callable)]:\n    vectors = cast(Floats2d, model.get_param('E'))\n    nV = vectors.shape[0]\n    nO = vectors.shape[1]\n    if (len(ids) == 0):\n        output: Floats2d = model.ops.alloc((0, nO), dtype=vectors.dtype)\n    else:\n        ids = model.ops.as_contig(ids, dtype='uint64')\n        nN = ids.shape[0]\n        seed: int = model.attrs['seed']\n        keys = (model.ops.hash(ids, seed) % nV)\n        output = vectors[keys].sum(axis=1)\n        drop_mask = None\n        if is_train:\n            dropout: Optional[float] = model.attrs.get('dropout_rate')\n            drop_mask = cast(Floats1d, model.ops.get_dropout_mask((nO,), dropout))\n            if (drop_mask is not None):\n                output *= drop_mask\n\n    def backprop(d_vectors: OutT) -> Ints1d:\n        if (drop_mask is not None):\n            d_vectors *= drop_mask\n        dE = model.ops.alloc2f(*vectors.shape)\n        keysT = model.ops.as_contig(keys.T, dtype='i')\n        for i in range(keysT.shape[0]):\n            model.ops.scatter_add(dE, keysT[i], d_vectors)\n        model.inc_grad('E', dE)\n        dX = model.ops.alloc1i(nN)\n        return dX\n    return (output, backprop)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\n@app.route('\/predict', methods=['GET'])\ndef predict():\n    if (not request.args.get('thread_url', False)):\n        logger.warning('Predict endpoint used with no thread_url')\n        return (jsonify({'error': 'Must contain thread_url'}), 400)\n    else:\n        thread_url = request.args.get('thread_url', False)\n        result = False\n        if app.config['REDIS_ON']:\n            result = redis.get(thread_url)\n        if result:\n            logger.info('Predict endpoint cache hit', url=thread_url)\n            print(pickle.loads(result))\n            return jsonify(pickle.loads(result))\n        else:\n            (answer, status_code) = pipeline.make_prediction(thread_url)\n            if ((status_code == 200) and app.config['REDIS_ON']):\n                redis.setex(thread_url, app.config['REDIS_DURATION'], pickle.dumps(answer))\n                logger.info('Predict endpoint cached result', url=thread_url)\n            return (jsonify(answer), status_code)\n"}
{"label_name":"train","label":0,"method_name":"lms_train","method":"\n\ndef lms_train(p0, Zi, Data):\n\n    def error(p, y, args):\n        l = len(p)\n        f = p[(l - 1)]\n        for i in range(len(args)):\n            f += (p[i] * args[i])\n        return (f - y)\n    Para = leastsq(error, p0, args=(Zi, Data))\n    return Para[0]\n"}
{"label_name":"forward","label":3,"method_name":"forward","method":"\n\ndef forward(model: Model[(InT, OutT)], X: InT, is_train: bool) -> Tuple[(OutT, Callable)]:\n    column: int = model.attrs['column']\n    layer = model.layers[0]\n    if (X.size < 2):\n        return layer(X, is_train)\n    keys = X[:, column]\n    if (not isinstance(keys, numpy.ndarray)):\n        keys = keys.get()\n    (uniq_keys, ind, inv, counts) = layer.ops.xp.unique(keys, return_index=True, return_inverse=True, return_counts=True)\n    counts = model.ops.reshape2i(counts, (- 1), 1)\n    X_uniq = X[ind]\n    (Y_uniq, bp_Y_uniq) = layer(X_uniq, is_train)\n    Y = Y_uniq[inv].reshape(((X.shape[0],) + Y_uniq.shape[1:]))\n    uniq_shape = tuple(Y_uniq.shape)\n\n    def backprop(dY: OutT) -> InT:\n        dY_uniq = layer.ops.alloc2f(*uniq_shape)\n        layer.ops.scatter_add(dY_uniq, layer.ops.asarray_i(inv), dY)\n        d_uniques = bp_Y_uniq(dY_uniq)\n        return (d_uniques \/ counts)[inv]\n    return (Y, backprop)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(estimator, X_test):\n    return estimator.predict(X_test)\n"}
{"label_name":"train","label":0,"method_name":"trainWithGradientDescent","method":"\n\ndef trainWithGradientDescent(coordiantes, h_slope, h_constant, learning_rate):\n    slope_gd_rate = 0\n    constant_gd_rate = 0\n    updated_h_slope = h_slope\n    updated_h_constant = h_constant\n    for i in range(0, len(coordiantes)):\n        x = coordiantes[i][0]\n        y = coordiantes[i][1]\n        constant_gd_rate += (((- 2) \/ len(coordiantes)) * (y - ((h_slope * x) + h_constant)))\n        slope_gd_rate += ((((- 2) \/ len(coordiantes)) * x) * (y - ((h_slope * x) + h_constant)))\n        print(('Constant error: ' + str(constant_gd_rate)))\n        print(('Slope error: ' + str(slope_gd_rate)))\n    updated_h_constant = (h_constant - (learning_rate * constant_gd_rate))\n    updated_h_slope = (h_slope - (learning_rate * slope_gd_rate))\n    return [updated_h_constant, updated_h_slope]\n"}
{"label_name":"save","label":1,"method_name":"_save_all","method":"\n\ndef _save_all(im, fp, filename):\n    encoderinfo = im.encoderinfo.copy()\n    encoderconfig = im.encoderconfig\n    append_images = list(encoderinfo.get('append_images', []))\n    if ((not hasattr(im, 'n_frames')) and (not append_images)):\n        return _save(im, fp, filename)\n    cur_idx = im.tell()\n    try:\n        with AppendingTiffWriter(fp) as tf:\n            for ims in ([im] + append_images):\n                ims.encoderinfo = encoderinfo\n                ims.encoderconfig = encoderconfig\n                if (not hasattr(ims, 'n_frames')):\n                    nfr = 1\n                else:\n                    nfr = ims.n_frames\n                for idx in range(nfr):\n                    ims.seek(idx)\n                    ims.load()\n                    _save(ims, tf, filename)\n                    tf.newFrame()\n    finally:\n        im.seek(cur_idx)\n"}
{"label_name":"train","label":0,"method_name":"train_main","method":"\n\ndef train_main(use_cuda):\n    if (use_cuda and (not fluid.core.is_compiled_with_cuda())):\n        return\n    place = (fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace())\n    context = encoder()\n    state_cell = decoder_state_cell(context)\n    rnn_out = decoder_train(state_cell)\n    label = layers.data(name='target_next_word', shape=[1], dtype='int64', lod_level=1)\n    cost = layers.cross_entropy(input=rnn_out, label=label)\n    avg_cost = layers.mean(x=cost)\n    optimizer = fluid.optimizer.Adagrad(learning_rate=0.001)\n    optimizer.minimize(avg_cost)\n    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.wmt14.train(dict_size), buf_size=1000), batch_size=batch_size)\n    feed_order = ['src_word', 'target_word', 'target_next_word']\n    exe = Executor(place)\n\n    def train_loop(main_program):\n        exe.run(framework.default_startup_program())\n        feed_list = [main_program.global_block().var(var_name) for var_name in feed_order]\n        feeder = fluid.DataFeeder(feed_list, place)\n        for pass_id in range(1):\n            for (batch_id, data) in enumerate(train_reader()):\n                outs = exe.run(main_program, feed=feeder.feed(data), fetch_list=[avg_cost])\n                avg_cost_val = np.array(outs[0])\n                print(((((('pass_id=' + str(pass_id)) + ' batch=') + str(batch_id)) + ' avg_cost=') + str(avg_cost_val)))\n                if (batch_id > 3):\n                    break\n    train_loop(framework.default_main_program())\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\ndef predict(w, X):\n    X = np.append(np.ones((len(X), 1)), np.array(X), axis=1)\n    yh = (((np.dot(X, w) > 0) * 2) - 1)\n    return yh\n"}
{"label_name":"process","label":2,"method_name":"_process_date_conversion","method":"\n\ndef _process_date_conversion(data_dict, converter, parse_spec, index_col, index_names, columns, keep_date_col=False):\n\n    def _isindex(colspec):\n        return ((isinstance(index_col, list) and (colspec in index_col)) or (isinstance(index_names, list) and (colspec in index_names)))\n    new_cols = []\n    new_data = {}\n    orig_names = columns\n    columns = list(columns)\n    date_cols = set()\n    if ((parse_spec is None) or isinstance(parse_spec, bool)):\n        return (data_dict, columns)\n    if isinstance(parse_spec, list):\n        for colspec in parse_spec:\n            if is_scalar(colspec):\n                if (isinstance(colspec, int) and (colspec not in data_dict)):\n                    colspec = orig_names[colspec]\n                if _isindex(colspec):\n                    continue\n                data_dict[colspec] = converter(data_dict[colspec])\n            else:\n                (new_name, col, old_names) = _try_convert_dates(converter, colspec, data_dict, orig_names)\n                if (new_name in data_dict):\n                    raise ValueError(('New date column already in dict %s' % new_name))\n                new_data[new_name] = col\n                new_cols.append(new_name)\n                date_cols.update(old_names)\n    elif isinstance(parse_spec, dict):\n        for (new_name, colspec) in compat.iteritems(parse_spec):\n            if (new_name in data_dict):\n                raise ValueError(('Date column %s already in dict' % new_name))\n            (_, col, old_names) = _try_convert_dates(converter, colspec, data_dict, orig_names)\n            new_data[new_name] = col\n            new_cols.append(new_name)\n            date_cols.update(old_names)\n    data_dict.update(new_data)\n    new_cols.extend(columns)\n    if (not keep_date_col):\n        for c in list(date_cols):\n            data_dict.pop(c)\n            new_cols.remove(c)\n    return (data_dict, new_cols)\n"}
{"label_name":"train","label":0,"method_name":"_applyConstraints","method":"\n\ndef _applyConstraints(blockVectorV, factYBY, blockVectorBY, blockVectorY):\n    'Changes blockVectorV in place.'\n    gramYBV = np.dot(blockVectorBY.T, blockVectorV)\n    tmp = cho_solve(factYBY, gramYBV)\n    blockVectorV -= np.dot(blockVectorY, tmp)\n"}
{"label_name":"train","label":0,"method_name":"train_support_vector","method":"\n\ndef train_support_vector(Xtrain, Ytrain, Xtest, Yexpected):\n    classifier = svm.SVC(gamma=0.001)\n    classifier.fit(Xtrain, Ytrain)\n    Ypredicted = classifier.predict(Xtest)\n    accuracy = ((np.sum((Yexpected == Ypredicted)) \/ len(Yexpected)) * 100)\n    print(('Accuracy: %.4f' % accuracy))\n    print('Classification report')\n    print(metrics.classification_report(Yexpected, Ypredicted))\n"}
{"label_name":"process","label":2,"method_name":"process","method":"\n\ndef process(num_results):\n    inpt = input('> ')\n    if (inpt == 'exit'):\n        exit()\n    (start_word, minus_words, plus_words) = parse_expression(inpt)\n    (err, results) = word_arithmetic(start_word=start_word, minus_words=minus_words, plus_words=plus_words, word_to_id=word_to_id, id_to_word=id_to_word, df=df, num_results=num_results)\n    if results:\n        print()\n        for res in results:\n            print(res[0].ljust(15), '     {0:.2f}'.format(res[1]))\n        print()\n    else:\n        print('{} not found in the dataset.'.format(err), file=sys.stderr)\n"}
{"label_name":"predict","label":4,"method_name":"create_fast_rcnn_predictor","method":"\n\ndef create_fast_rcnn_predictor(conv_out, rois, fc_layers):\n    roi_out = roipooling(conv_out, rois, cntk.MAX_POOLING, (roi_dim, roi_dim), spatial_scale=(1 \/ 16.0))\n    fc_out = fc_layers(roi_out)\n    W_pred = parameter(shape=(4096, globalvars['num_classes']), init=normal(scale=0.01), name='cls_score.W')\n    b_pred = parameter(shape=globalvars['num_classes'], init=0, name='cls_score.b')\n    cls_score = plus(times(fc_out, W_pred), b_pred, name='cls_score')\n    W_regr = parameter(shape=(4096, (globalvars['num_classes'] * 4)), init=normal(scale=0.001), name='bbox_regr.W')\n    b_regr = parameter(shape=(globalvars['num_classes'] * 4), init=0, name='bbox_regr.b')\n    bbox_pred = plus(times(fc_out, W_regr), b_regr, name='bbox_regr')\n    return (cls_score, bbox_pred)\n"}
{"label_name":"train","label":0,"method_name":"trainingNaiveBayes","method":"\n\ndef trainingNaiveBayes(trainMarkedWords, trainCategory):\n    '\\n    \u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u8bed\u6599\u5e93\u4e2d\u8bcd\u6c47\u7684\\n    Pos\uff1aP\uff08Wi|Pos\u603b\u8bcd\u6570\uff09\\n    Neg\uff1aP\uff08Wi|Neg\u603b\u8bcd\u6570\uff09\\n    Neutral\uff1aP\uff08Wi|Neutral\u603b\u8bcd\u6570\uff09\\n    '\n    numTrainDoc = len(trainMarkedWords)\n    numWords = len(trainMarkedWords[0])\n    (pPos, pNeg, pNeutral) = (0.0, 0.0, 0.0)\n    for i in trainCategory:\n        if (i == 1):\n            pPos = (pPos + 1)\n        elif (i == 2):\n            pNeg = (pNeg + 1)\n        else:\n            pNeutral = (pNeutral + 1)\n    pPos = (pPos \/ float(numTrainDoc))\n    pNeg = (pNeg \/ float(numTrainDoc))\n    pNeutral = (pNeutral \/ float(numTrainDoc))\n    wordsInPosNum = np.ones(numWords)\n    wordsInNegNum = np.ones(numWords)\n    wordsInNeutralNum = np.ones(numWords)\n    PosWordsNum = 2.0\n    NegWordsNum = 2.0\n    NeutralWordsNum = 2.0\n    for i in range(0, numTrainDoc):\n        try:\n            if (trainCategory[i] == 1):\n                wordsInPosNum += trainMarkedWords[i]\n                PosWordsNum += sum(trainMarkedWords[i])\n            elif (trainCategory[i] == 2):\n                wordsInNegNum += trainMarkedWords[i]\n                NegWordsNum += sum(trainMarkedWords[i])\n            else:\n                wordsInNeutralNum += trainMarkedWords[i]\n                NeutralWordsNum += sum(trainMarkedWords[i])\n        except Exception as e:\n            traceback.print_exc(e)\n    pWordsPosicity = np.log((wordsInPosNum \/ PosWordsNum))\n    pWordsNegy = np.log((wordsInNegNum \/ NegWordsNum))\n    pWordsNeutral = np.log((wordsInNeutralNum \/ NeutralWordsNum))\n    return (pWordsPosicity, pWordsNegy, pWordsNeutral, pPos, pNeg, pNeutral)\n"}
{"label_name":"train","label":0,"method_name":"setup_trains_logging","method":"\n\ndef setup_trains_logging(trainer: Engine, optimizers: Optional[Union[(Optimizer, Dict[(str, Optimizer)])]]=None, evaluators: Optional[Union[(Engine, Dict[(str, Engine)])]]=None, log_every_iters: int=100, **kwargs: Any) -> ClearMLLogger:\n    '``setup_trains_logging`` was renamed to :func:`~ignite.contrib.engines.common.setup_clearml_logging`.\\n    '\n    warnings.warn('setup_trains_logging was renamed to setup_clearml_logging.')\n    return setup_clearml_logging(trainer, optimizers, evaluators, log_every_iters, **kwargs)\n"}
{"label_name":"train","label":0,"method_name":"train_model","method":"\n\ndef train_model(name, xml):\n    'requires: the model name, and the path to the xml annotations.\\n  It trains and saves a new model according to the specified \\n  training options and given annotations'\n    options = dlib.shape_predictor_training_options()\n    options.tree_depth = 3\n    options.nu = 0.1\n    options.cascade_depth = 10\n    options.feature_pool_size = 150\n    options.num_test_splits = 350\n    options.oversampling_amount = 5\n    options.oversampling_translation_jitter = 0\n    options.be_verbose = True\n    options.num_threads = 1\n    dlib.train_shape_predictor(xml, name, options)\n"}
{"label_name":"process","label":2,"method_name":"_preprocess_rhs","method":"\n\ndef _preprocess_rhs(x, value):\n    if isinstance(value, chainer.Variable):\n        return value\n    if (not (numpy.isscalar(value) or isinstance(value, chainer.get_array_types()))):\n        raise TypeError('Value must be a scalar, `numpy.ndarray`, `cupy.ndarray` or a `Variable`.\\nActual: {}'.format(type(value)))\n    return value.astype(x.dtype, copy=False)\n"}
{"label_name":"train","label":0,"method_name":"train_worker","method":"\n\ndef train_worker(learning_agent, output_file, experience_file, lr, batch_size):\n    learning_agent = load_agent(learning_agent)\n    with h5py.File(experience_file, 'r') as expf:\n        exp_buffer = rl.load_experience(expf)\n    learning_agent.train(exp_buffer, lr=lr, batch_size=batch_size)\n    with h5py.File(output_file, 'w') as updated_agent_outf:\n        learning_agent.serialize(updated_agent_outf)\n"}
{"label_name":"train","label":0,"method_name":"constrain","method":"\n\ndef constrain(n, min, max):\n    'This returns a number, n constrained to the min and max bounds. '\n    if (n < min):\n        return min\n    if (n > max):\n        return max\n    return n\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(input_variable, target_variable, encoder1, encoder2, encoder3, encoder4, encoder5, encoder6, encoder7, encoder8, encoder9, encoder10, encoder11, encoder12, cnn, encoder1_optimizer, encoder2_optimizer, encoder3_optimizer, encoder4_optimizer, encoder5_optimizer, encoder6_optimizer, encoder7_optimizer, encoder8_optimizer, encoder9_optimizer, encoder10_optimizer, encoder11_optimizer, encoder12_optimizer, cnn_optimizer, criterion, max_length=T):\n    encoder1_optimizer.zero_grad()\n    encoder2_optimizer.zero_grad()\n    encoder3_optimizer.zero_grad()\n    encoder4_optimizer.zero_grad()\n    encoder5_optimizer.zero_grad()\n    encoder6_optimizer.zero_grad()\n    encoder7_optimizer.zero_grad()\n    encoder8_optimizer.zero_grad()\n    encoder9_optimizer.zero_grad()\n    encoder10_optimizer.zero_grad()\n    encoder11_optimizer.zero_grad()\n    encoder12_optimizer.zero_grad()\n    cnn_optimizer.zero_grad()\n    loss = 0\n    lead_output = Variable(torch.Tensor().type('torch.cuda.FloatTensor'))\n    lead_hidden = Variable(torch.Tensor().type('torch.cuda.FloatTensor'))\n    input_part = input_variable[0].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder12(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[1].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder1(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[2].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder2(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[3].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder3(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[4].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder4(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[5].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder5(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[6].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder6(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[7].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder7(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[8].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder8(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[9].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder9(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[10].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder10(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    input_part = input_variable[11].clone()\n    input_lead = input_part.view(N, T, D).transpose(0, 1).clone()\n    (encoder_outputs, encoder_hidden, attn_weights) = encoder11(input_lead)\n    lead_output = Variable(torch.cat((lead_output.data, encoder_outputs.data), 1))\n    lead_output = lead_output.view(N, L, O)\n    (model_output, model_hidden) = cnn(lead_hidden, lead_output)\n    loss = criterion(model_output, target_variable)\n    loss.backward()\n    encoder1_optimizer.step()\n    encoder2_optimizer.step()\n    encoder3_optimizer.step()\n    encoder4_optimizer.step()\n    encoder5_optimizer.step()\n    encoder6_optimizer.step()\n    encoder7_optimizer.step()\n    encoder8_optimizer.step()\n    encoder9_optimizer.step()\n    encoder10_optimizer.step()\n    encoder11_optimizer.step()\n    encoder12_optimizer.step()\n    cnn_optimizer.step()\n    return loss.data[0]\n"}
{"label_name":"predict","label":4,"method_name":"check_new_observation_predict_parts","method":"\n\ndef check_new_observation_predict_parts(new_observation, explainer):\n    new_observation_ = deepcopy(new_observation)\n    error_dim_val = 'Wrong new_observation dimension'\n    if isinstance(new_observation_, pd.Series):\n        new_observation_ = new_observation_.to_frame().T\n        new_observation_.columns = explainer.data.columns\n    elif isinstance(new_observation_, np.ndarray):\n        if (new_observation_.ndim == 1):\n            new_observation_ = new_observation_.reshape((1, (- 1)))\n        elif (new_observation_.ndim > 2):\n            raise ValueError(error_dim_val)\n        elif (new_observation.shape[0] != 1):\n            raise ValueError(error_dim_val)\n        new_observation_ = pd.DataFrame(new_observation_)\n        new_observation_.columns = explainer.data.columns\n    elif isinstance(new_observation_, list):\n        new_observation_ = pd.DataFrame(new_observation_).T\n        new_observation_.columns = explainer.data.columns\n    elif isinstance(new_observation_, pd.DataFrame):\n        if (new_observation.shape[0] != 1):\n            raise ValueError(error_dim_val)\n        new_observation_.columns = explainer.data.columns\n    else:\n        raise TypeError('new_observation must be a numpy.ndarray or pandas.Series or pandas.DataFrame')\n    if pd.api.types.is_bool_dtype(new_observation_.index):\n        raise ValueError('new_observation index is of boolean type')\n    return new_observation_\n"}
{"label_name":"save","label":1,"method_name":"check_saved_vars_try_dump","method":"\n\ndef check_saved_vars_try_dump(dump_dir, dump_prog_fn, is_text_dump_program, feed_config, fetch_config, batch_size=1, save_filename=None):\n    dump_prog = load_program(os.path.join(dump_dir, dump_prog_fn), is_text_dump_program)\n    saved_params = [v for v in dump_prog.list_vars() if fluid.io.is_persistable(v)]\n    logger.info('persistable vars in dump program: {}'.format([v.name for v in saved_params]))\n    check_not_expected_ops(dump_prog)\n    return try_load_model_vars(dump_dir, dump_prog_fn, is_text_dump_program, batch_size, feed_config, fetch_config, save_filename, saved_params)\n"}
{"label_name":"train","label":0,"method_name":"get_simple_training_fn","method":"\n\ndef get_simple_training_fn(mlp_model, learning_rate):\n    inputs = [mlp_model.input, mlp_model.target]\n    params_and_grads = nll_grad(mlp_model)\n    updates = sgd_updates(params_and_grads, learning_rate=lr)\n    return theano.function(inputs=inputs, outputs=[], updates=updates)\n"}
{"label_name":"save","label":1,"method_name":"save_categories_to_csv_file","method":"\n\ndef save_categories_to_csv_file(categories, csv_path):\n    \"Saves categories to a csv file.\\n\\n  Args:\\n    categories: A list of dictionaries representing categories to save to file.\\n                Each category must contain an 'id' and 'name' field.\\n    csv_path: Path to the csv file to be parsed into categories.\\n  \"\n    categories.sort(key=(lambda x: x['id']))\n    with tf.gfile.Open(csv_path, 'w') as csvfile:\n        writer = csv.writer(csvfile, delimiter=',', quotechar='\"')\n        for category in categories:\n            writer.writerow([category['id'], category['name']])\n"}
{"label_name":"predict","label":4,"method_name":"predict_domain","method":"\n\ndef predict_domain(images, hparams, is_training=False, reuse=False, scope='discriminator'):\n    \"Creates a discriminator for a GAN.\\n\\n  Args:\\n    images: A `Tensor` of size [batch_size, height, width, channels]. It is\\n      assumed that the images are centered between -1 and 1.\\n    hparams: hparam object with params for discriminator\\n    is_training: Specifies whether or not we're training or testing.\\n    reuse: Whether to reuse variable scope\\n    scope: An optional variable_scope.\\n\\n  Returns:\\n    [batch size, 1] - logit output of discriminator.\\n  \"\n    with tf.variable_scope(scope, 'discriminator', [images], reuse=reuse):\n        lrelu_partial = functools.partial(lrelu, leakiness=hparams.lrelu_leakiness)\n        with slim.arg_scope([slim.conv2d], kernel_size=([hparams.discriminator_kernel_size] * 2), activation_fn=lrelu_partial, stride=2, normalizer_fn=slim.batch_norm):\n\n            def add_noise(hidden, scope_num=None):\n                if scope_num:\n                    hidden = slim.dropout(hidden, hparams.discriminator_dropout_keep_prob, is_training=is_training, scope=('dropout_%s' % scope_num))\n                if (hparams.discriminator_noise_stddev == 0):\n                    return hidden\n                return (hidden + tf.random_normal(hidden.shape.as_list(), mean=0.0, stddev=hparams.discriminator_noise_stddev))\n            if hparams.discriminator_image_noise:\n                images = add_noise(images)\n            net = slim.conv2d(images, hparams.num_discriminator_filters, normalizer_fn=None, stride=hparams.discriminator_first_stride, scope=('conv1_stride%s' % hparams.discriminator_first_stride))\n            net = add_noise(net, 1)\n            block_id = 2\n            while (net.shape.as_list()[1] > hparams.projection_shape_size):\n                num_filters = int((hparams.num_discriminator_filters * (hparams.discriminator_filter_factor ** (block_id - 1))))\n                for conv_id in range(1, hparams.discriminator_conv_block_size):\n                    net = slim.conv2d(net, num_filters, stride=1, scope=('conv_%s_%s' % (block_id, conv_id)))\n                if hparams.discriminator_do_pooling:\n                    net = slim.conv2d(net, num_filters, scope=('conv_%s_prepool' % block_id))\n                    net = slim.avg_pool2d(net, kernel_size=[2, 2], stride=2, scope=('pool_%s' % block_id))\n                else:\n                    net = slim.conv2d(net, num_filters, scope=('conv_%s_stride2' % block_id))\n                net = add_noise(net, block_id)\n                block_id += 1\n            net = slim.flatten(net)\n            net = slim.fully_connected(net, 1, normalizer_fn=None, activation_fn=None, scope='fc_logit_out')\n    return net\n"}
{"label_name":"process","label":2,"method_name":"process_multipauli_term","method":"\n\ndef process_multipauli_term(term):\n    components = term.split('_')\n    components.remove('pauliSet')\n    core_operators = list(sorted(construct_models.core_operator_dict.keys()))\n    for l in components:\n        if (l[0] == 'd'):\n            dim = int(l.replace('d', ''))\n        elif (l[0] in core_operators):\n            operators = l.split('J')\n        else:\n            sites = l.split('J')\n    sites = [int(s) for s in sites]\n    all_terms = list(zip(sites, operators))\n    term_dict = {'dim': dim, 'terms': [all_terms]}\n    full_mod_str = full_model_string(term_dict)\n    return construct_models.compute(full_mod_str)\n"}
{"label_name":"train","label":0,"method_name":"train_maxent_classifier_with_megam","method":"\n\ndef train_maxent_classifier_with_megam(train_toks, trace=3, encoding=None, labels=None, gaussian_prior_sigma=0, **kwargs):\n    '\\n    Train a new ``ConditionalExponentialClassifier``, using the given\\n    training samples, using the external ``megam`` library.  This\\n    ``ConditionalExponentialClassifier`` will encode the model that\\n    maximizes entropy from all the models that are empirically\\n    consistent with ``train_toks``.\\n\\n    :see: ``train_maxent_classifier()`` for parameter descriptions.\\n    :see: ``nltk.classify.megam``\\n    '\n    explicit = True\n    bernoulli = True\n    if ('explicit' in kwargs):\n        explicit = kwargs['explicit']\n    if ('bernoulli' in kwargs):\n        bernoulli = kwargs['bernoulli']\n    if (encoding is None):\n        count_cutoff = kwargs.get('count_cutoff', 0)\n        encoding = BinaryMaxentFeatureEncoding.train(train_toks, count_cutoff, labels=labels, alwayson_features=True)\n    elif (labels is not None):\n        raise ValueError('Specify encoding or labels, not both')\n    try:\n        (fd, trainfile_name) = tempfile.mkstemp(prefix='nltk-')\n        with open(trainfile_name, 'w') as trainfile:\n            write_megam_file(train_toks, encoding, trainfile, explicit=explicit, bernoulli=bernoulli)\n        os.close(fd)\n    except (OSError, IOError, ValueError) as e:\n        raise ValueError(('Error while creating megam training file: %s' % e))\n    options = []\n    options += ['-nobias', '-repeat', '10']\n    if explicit:\n        options += ['-explicit']\n    if (not bernoulli):\n        options += ['-fvals']\n    if gaussian_prior_sigma:\n        inv_variance = (1.0 \/ (gaussian_prior_sigma ** 2))\n    else:\n        inv_variance = 0\n    options += ['-lambda', ('%.2f' % inv_variance), '-tune']\n    if (trace < 3):\n        options += ['-quiet']\n    if ('max_iter' in kwargs):\n        options += ['-maxi', ('%s' % kwargs['max_iter'])]\n    if ('ll_delta' in kwargs):\n        options += ['-dpp', ('%s' % abs(kwargs['ll_delta']))]\n    if hasattr(encoding, 'cost'):\n        options += ['-multilabel']\n    options += ['multiclass', trainfile_name]\n    stdout = call_megam(options)\n    try:\n        os.remove(trainfile_name)\n    except (OSError, IOError) as e:\n        print(('Warning: unable to delete %s: %s' % (trainfile_name, e)))\n    weights = parse_megam_weights(stdout, encoding.length(), explicit)\n    weights *= numpy.log2(numpy.e)\n    return MaxentClassifier(encoding, weights)\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\n@get('\/playml\/sentiment\/predict\/<statement>')\ndef predict(statement):\n    response.content_type = 'application\/json'\n    test = pd.DataFrame({'statements': statement}, index=[0])\n    sentiment = obj.predict(test)\n    json_sentiment = pd.DataFrame({'sentiments': sentiment}).to_json(orient='records')\n    return json_sentiment\n"}
{"label_name":"process","label":2,"method_name":"preprocess_text","method":"\n\ndef preprocess_text(text):\n    return ' '.join(tokenize_text(text))\n"}
{"label_name":"train","label":0,"method_name":"train_batch_fn","method":"\n\ndef train_batch_fn(data, ctx):\n    'split and load data in GPU'\n    template = split_and_load(data[0], ctx_list=ctx, batch_axis=0)\n    search = split_and_load(data[1], ctx_list=ctx, batch_axis=0)\n    label_cls = split_and_load(data[2], ctx_list=ctx, batch_axis=0)\n    label_loc = split_and_load(data[3], ctx_list=ctx, batch_axis=0)\n    label_loc_weight = split_and_load(data[4], ctx_list=ctx, batch_axis=0)\n    return (template, search, label_cls, label_loc, label_loc_weight)\n"}
{"label_name":"train","label":0,"method_name":"train_input_fn","method":"\n\ndef train_input_fn(training_dir, params):\n    data = tf.data.TFRecordDataset(_resolves(training_dir, params['train_tfrecord_files']), compression_type=params['tfrecord_compression_type'], num_parallel_reads=params['train_parallel_reads_num'])\n    return data.map(parse_example).shuffle(params['train_shuffle_buffer_size']).batch(params['train_batch_size']).prefetch(params['train_prefetch_buffer_size']).repeat(params['train_epochs']).make_one_shot_iterator().get_next()\n"}
{"label_name":"train","label":0,"method_name":"trainModel","method":"\n\ndef trainModel(model):\n    (X_train, X_test, Y_train, Y_test) = initializers()\n    hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_split=0.2)\n    ans = input('Do you want to save the trained weights - y\/n ?')\n    if (ans == 'y'):\n        filename = input('Enter file name - ')\n        fname = ((path + str(filename)) + '.hdf5')\n        model.save_weights(fname, overwrite=True)\n    else:\n        model.save_weights('newWeight.hdf5', overwrite=True)\n    visualizeHis(hist)\n"}
{"label_name":"save","label":1,"method_name":"save_images_to_arrays","method":"\n\ndef save_images_to_arrays():\n    image_paths = [train_data_dir, validation_data_dir]\n    array_dict = {train_data_dir: [[], []], validation_data_dir: [[], []]}\n    for train_or_test_dir in image_paths:\n        for (label, class_dir) in enumerate(list_files(train_or_test_dir)):\n            for image in list_files(os.path.join(train_or_test_dir, class_dir)):\n                image_path = os.path.join(train_or_test_dir, class_dir, image)\n                sample = convert_to_np(image_path)\n                array_dict[train_or_test_dir][0].append(sample)\n                one_hot = ([0.0, 1.0] if (label == 0) else [1.0, 0.0])\n                array_dict[train_or_test_dir][1].append(one_hot)\n    (train_X, train_y) = array_dict[train_data_dir]\n    (test_X, test_y) = array_dict[validation_data_dir]\n    create_dir('bottleneck')\n    np.save('bottleneck\/bottleneck_train_X', np.array(train_X))\n    np.save('bottleneck\/bottleneck_train_y', np.array(train_y))\n    np.save('bottleneck\/bottleneck_test_X', np.array(test_X))\n    np.save('bottleneck\/bottleneck_test_y', np.array(test_y))\n    return (np.array(train_X), np.array(train_y), np.array(test_X), np.array(test_y))\n"}
{"label_name":"save","label":1,"method_name":"save_smi","method":"\n\ndef save_smi(name, smiles):\n    if (not os.path.exists('epoch_data')):\n        os.makedirs('epoch_data')\n    smi_file = os.path.join('epoch_data', '{}.smi'.format(name))\n    with open(smi_file, 'w') as afile:\n        afile.write('\\n'.join(smiles))\n    return\n"}
{"label_name":"process","label":2,"method_name":"process_columns","method":"\n\ndef process_columns(columns):\n    '\\n    Helper function to parse column names from pandas.DataFrame objects\\n    parsed from HTML tables\\n    '\n    if isinstance(columns, pd.MultiIndex):\n        return [process_multi_index_col(name) for name in columns]\n    else:\n        return [None, *columns[1:]]\n"}
{"label_name":"train","label":0,"method_name":"_run_train","method":"\n\ndef _run_train(ppo_hparams, event_dir, model_dir, restarter, train_summary_op, eval_summary_op, initializers, epoch, report_fn=None, model_save_fn=None):\n    'Train.'\n    summary_writer = tf.summary.FileWriter(event_dir, graph=tf.get_default_graph(), flush_secs=60)\n    model_saver = tf.train.Saver(((((tf.global_variables((ppo_hparams.policy_network + '\/.*')) + tf.global_variables((('training\/' + ppo_hparams.policy_network) + '\/.*'))) + tf.global_variables('global_step')) + tf.global_variables('losses_avg.*')) + tf.global_variables('train_stats.*')))\n    global_step = tf.train.get_or_create_global_step()\n    with tf.control_dependencies([tf.assign_add(global_step, 1)]):\n        train_summary_op = tf.identity(train_summary_op)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for initializer in initializers:\n            initializer(sess)\n        trainer_lib.restore_checkpoint(model_dir, model_saver, sess)\n        num_target_iterations = restarter.target_local_step\n        num_completed_iterations = (num_target_iterations - restarter.steps_to_go)\n        with restarter.training_loop():\n            for epoch_index in range(num_completed_iterations, num_target_iterations):\n                summary = sess.run(train_summary_op)\n                if summary_writer:\n                    summary_writer.add_summary(summary, epoch_index)\n                if (ppo_hparams.eval_every_epochs and ((epoch_index % ppo_hparams.eval_every_epochs) == 0)):\n                    eval_summary = sess.run(eval_summary_op)\n                    if summary_writer:\n                        summary_writer.add_summary(eval_summary, epoch_index)\n                    if report_fn:\n                        summary_proto = tf.Summary()\n                        summary_proto.ParseFromString(eval_summary)\n                        for elem in summary_proto.value:\n                            if ('mean_score' in elem.tag):\n                                report_fn(elem.simple_value, epoch_index)\n                                break\n                if (model_saver and ppo_hparams.save_models_every_epochs and (((epoch_index % ppo_hparams.save_models_every_epochs) == 0) or ((epoch_index + 1) == num_target_iterations))):\n                    ckpt_name = 'model.ckpt-{}'.format(tf.train.global_step(sess, global_step))\n                    epoch_dir = os.path.join(model_dir, 'epoch_{}'.format(epoch))\n                    tf.gfile.MakeDirs(epoch_dir)\n                    for ckpt_dir in (model_dir, epoch_dir):\n                        model_saver.save(sess, os.path.join(ckpt_dir, ckpt_name))\n                    if model_save_fn:\n                        model_save_fn(model_dir)\n"}
{"label_name":"train","label":0,"method_name":"train_network","method":"\n\ndef train_network():\n    download_idenprof()\n    print(os.listdir(os.path.join(execution_path, 'idenprof')))\n    optimizer = keras.optimizers.Adam(lr=0.01, decay=0.0001)\n    batch_size = 32\n    num_classes = 10\n    epochs = 200\n    model = ResNet50((224, 224, 3), num_classes=num_classes)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.summary()\n    print('Using real time Data Augmentation')\n    train_datagen = ImageDataGenerator(rescale=(1.0 \/ 255), horizontal_flip=True)\n    test_datagen = ImageDataGenerator(rescale=(1.0 \/ 255))\n    train_generator = train_datagen.flow_from_directory(DATASET_TRAIN_DIR, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n    test_generator = test_datagen.flow_from_directory(DATASET_TEST_DIR, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n    model.fit_generator(train_generator, steps_per_epoch=int((9000 \/ batch_size)), epochs=epochs, validation_data=test_generator, validation_steps=int((2000 \/ batch_size)), callbacks=[checkpoint, lr_scheduler])\n"}
{"label_name":"save","label":1,"method_name":"plot_learner_and_save","method":"\n\ndef plot_learner_and_save(learner, fname):\n    (fig, ax) = plt.subplots()\n    tri = learner.interpolator(scaled=True).tri\n    triang = mtri.Triangulation(*tri.points.T, triangles=tri.vertices)\n    ax.triplot(triang, c='k', lw=0.8)\n    ax.imshow(learner.plot().Image.I.data, extent=((- 0.5), 0.5, (- 0.5), 0.5))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.savefig(fname, bbox_inches='tight', transparent=True, dpi=300, pad_inches=(- 0.1))\n"}
{"label_name":"save","label":1,"method_name":"_save_aggregate_fold_outputs","method":"\n\ndef _save_aggregate_fold_outputs(combined_oof_predictions, combined_test_predictions, mean_test_prediction, pipeline_name):\n    logger.info('Saving out of fold valid predictions')\n    save_submission(combined_oof_predictions, params.experiment_dir, '{}_predictions_train_oof.csv'.format(pipeline_name), logger)\n    logger.info('Saving out of fold test predictions')\n    save_submission(combined_test_predictions, params.experiment_dir, '{}_predictions_test_oof.csv'.format(pipeline_name), logger)\n    logger.info('Saving averaged out of fold test predictions')\n    save_submission(mean_test_prediction, params.experiment_dir, '{}_predictions_test_am.csv'.format(pipeline_name), logger)\n"}
{"label_name":"forward","label":3,"method_name":"forward_char","method":"\n\n@register('forward-char')\ndef forward_char(event):\n    ' Move forward a character. '\n    buff = event.current_buffer\n    buff.cursor_position += buff.document.get_cursor_right_position(count=event.arg)\n"}
{"label_name":"save","label":1,"method_name":"save_video","method":"\n\ndef save_video(video, save_path_template):\n    'Save frames of the videos into files.'\n    try:\n        from PIL import Image\n    except ImportError as e:\n        tf.logging.warning('Showing and saving an image requires PIL library to be installed: %s', e)\n        raise NotImplementedError('Image display and save not implemented.')\n    for (i, frame) in enumerate(video):\n        save_path = save_path_template.format(i)\n        with tf.gfile.Open(save_path, 'wb') as sp:\n            Image.fromarray(np.uint8(frame)).save(sp)\n"}
{"label_name":"train","label":0,"method_name":"_create_tidy_training_set","method":"\n\ndef _create_tidy_training_set():\n    colnames = (['Date', 'Site', 'Item'] + map(str, range(24)))\n    df = pd.read_csv(PATH_GIVEN_TRAIN, names=colnames, skiprows=1)\n    df = df.loc[:, (['Date', 'Item'] + map(str, range(24)))]\n    df = pd.melt(df, id_vars=['Date', 'Item'], value_vars=map(str, range(24)), var_name='Hour', value_name='Value')\n    df['Datetime'] = pd.to_datetime((((df.Date + ' ') + df.Hour) + ':00:00'))\n    df = df.loc[:, ['Datetime', 'Item', 'Value']]\n    df.loc[((df.Value == 'NR'), 'Value')] = 0\n    df['Value'] = df['Value'].astype(float)\n    df = df.pivot_table(values='Value', index='Datetime', columns='Item', aggfunc='sum')\n    return df\n"}
{"label_name":"save","label":1,"method_name":"save_gracefully","method":"\n\ndef save_gracefully(signal, frame):\n    save()\n    sys.exit(0)\n"}
{"label_name":"train","label":0,"method_name":"train_xgboost","method":"\n\ndef train_xgboost(parameters, X, y, num_rounds=50):\n    ddata = xgb.DMatrix(data=X, label=y, nthread=(- 1))\n    with Timer() as t:\n        clf = xgb.train(parameters, ddata, num_boost_round=num_rounds)\n    return (clf, t.interval)\n"}
{"label_name":"process","label":2,"method_name":"_preprocessing","method":"\n\ndef _preprocessing(config, is_train=True):\n    if is_train:\n        xy_train = Step(name='xy_train', transformer=XYSplit(**config.xy_splitter), input_data=['input'], adapter={'meta': [('input', 'meta')], 'train_mode': [('input', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)\n        text_cleaner_train = Step(name='text_cleaner_train', transformer=TextCleaner(**config.text_cleaner), input_steps=[xy_train], adapter={'X': [('xy_train', 'X')]}, cache_dirpath=config.env.cache_dirpath)\n        xy_valid = Step(name='xy_valid', transformer=XYSplit(**config.xy_splitter), input_data=['input'], adapter={'meta': [('input', 'meta_valid')], 'train_mode': [('input', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)\n        text_cleaner_valid = Step(name='text_cleaner_valid', transformer=TextCleaner(**config.text_cleaner), input_steps=[xy_valid], adapter={'X': [('xy_valid', 'X')]}, cache_dirpath=config.env.cache_dirpath)\n        cleaning_output = Step(name='cleaning_output', transformer=Dummy(), input_data=['input'], input_steps=[xy_train, text_cleaner_train, xy_valid, text_cleaner_valid], adapter={'X': [('text_cleaner_train', 'X')], 'y': [('xy_train', 'y')], 'train_mode': [('input', 'train_mode')], 'X_valid': [('text_cleaner_valid', 'X')], 'y_valid': [('xy_valid', 'y')]}, cache_dirpath=config.env.cache_dirpath)\n    else:\n        xy_train = Step(name='xy_train', transformer=XYSplit(**config.xy_splitter), input_data=['input'], adapter={'meta': [('input', 'meta')], 'train_mode': [('input', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)\n        text_cleaner = Step(name='text_cleaner_train', transformer=TextCleaner(**config.text_cleaner), input_steps=[xy_train], adapter={'X': [('xy_train', 'X')]}, cache_dirpath=config.env.cache_dirpath)\n        cleaning_output = Step(name='cleaning_output', transformer=Dummy(), input_data=['input'], input_steps=[xy_train, text_cleaner], adapter={'X': [('text_cleaner_train', 'X')], 'y': [('xy_train', 'y')], 'train_mode': [('input', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)\n    return cleaning_output\n"}
{"label_name":"train","label":0,"method_name":"lms_train","method":"\n\ndef lms_train(p0, Zi, Data):\n\n    def error(p, y, args):\n        l = len(p)\n        f = p[(l - 1)]\n        for i in range(len(args)):\n            f += (p[i] * args[i])\n        return (f - y)\n    Para = leastsq(error, p0, args=(Zi, Data))\n    return Para[0]\n"}
{"label_name":"predict","label":4,"method_name":"predict_bayes","method":"\n\ndef predict_bayes(opt):\n    team_data = get_team_representations(opt.team_data_type)\n    bayes.train(team_data, opt)\n"}
{"label_name":"train","label":0,"method_name":"subsubtrain_directory_pattern","method":"\n\ndef subsubtrain_directory_pattern() -> str:\n    return _get_name_from_file('subsubtrain_directory_pattern')\n"}
{"label_name":"save","label":1,"method_name":"save_parameters","method":"\n\ndef save_parameters(path, *parameters):\n    with open(path, 'wb') as file_handle:\n        for param in parameters:\n            cPickle.dump(param, file_handle, protocol=cPickle.HIGHEST_PROTOCOL)\n"}
{"label_name":"process","label":2,"method_name":"preprocess_image","method":"\n\ndef preprocess_image(image, output_height, output_width, is_training=False, add_image_summaries=True):\n    \"Preprocesses the given image.\\n\\n  Args:\\n    image: A `Tensor` representing an image of arbitrary size.\\n    output_height: The height of the image after preprocessing.\\n    output_width: The width of the image after preprocessing.\\n    is_training: `True` if we're preprocessing the image for training and\\n      `False` otherwise.\\n    add_image_summaries: Enable image summaries.\\n\\n  Returns:\\n    A preprocessed image.\\n  \"\n    if is_training:\n        return preprocess_for_train(image, output_height, output_width, add_image_summaries=add_image_summaries)\n    else:\n        return preprocess_for_eval(image, output_height, output_width, add_image_summaries=add_image_summaries)\n"}
{"label_name":"train","label":0,"method_name":"default_train_step_kwargs","method":"\n\ndef default_train_step_kwargs(m, obj, logdir, rng_seed, is_chief, num_steps, iters, train_display_interval, dagger_sample_bn_false):\n    train_step_kwargs = {}\n    train_step_kwargs['obj'] = obj\n    train_step_kwargs['m'] = m\n    train_step_kwargs['rng_data'] = [np.random.RandomState(rng_seed), np.random.RandomState(rng_seed)]\n    train_step_kwargs['rng_action'] = np.random.RandomState(rng_seed)\n    if is_chief:\n        train_step_kwargs['writer'] = tf.summary.FileWriter(logdir)\n    else:\n        train_step_kwargs['writer'] = None\n    train_step_kwargs['iters'] = iters\n    train_step_kwargs['train_display_interval'] = train_display_interval\n    train_step_kwargs['num_steps'] = num_steps\n    train_step_kwargs['logdir'] = logdir\n    train_step_kwargs['dagger_sample_bn_false'] = dagger_sample_bn_false\n    return train_step_kwargs\n"}
{"label_name":"predict","label":4,"method_name":"get_expected_predictions","method":"\n\ndef get_expected_predictions(X, y, cv, classes, est, method):\n    expected_predictions = np.zeros([len(y), classes])\n    func = getattr(est, method)\n    for (train, test) in cv.split(X, y):\n        est.fit(X[train], y[train])\n        expected_predictions_ = func(X[test])\n        if (method is 'predict_proba'):\n            exp_pred_test = np.zeros((len(test), classes))\n        else:\n            exp_pred_test = np.full((len(test), classes), np.finfo(expected_predictions.dtype).min)\n        exp_pred_test[:, est.classes_] = expected_predictions_\n        expected_predictions[test] = exp_pred_test\n    return expected_predictions\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train():\n    with tf.name_scope('data'):\n        x = tfc.placeholder(tf.float32, [None, 28, 28, 1])\n        tfc.summary.image('data', x)\n    with tfc.variable_scope('variational'):\n        (q_mu, q_sigma) = inference_network(x=x, latent_dim=FLAGS.latent_dim, hidden_size=FLAGS.hidden_size)\n        q_z = tfp.distributions.Normal(loc=q_mu, scale=q_sigma)\n        assert (q_z.reparameterization_type == tfp.distributions.FULLY_REPARAMETERIZED)\n    with tfc.variable_scope('model'):\n        p_x_given_z_logits = generative_network(z=q_z.sample(), hidden_size=FLAGS.hidden_size)\n        p_x_given_z = tfp.distributions.Bernoulli(logits=p_x_given_z_logits)\n        posterior_predictive_samples = p_x_given_z.sample()\n        tfc.summary.image('posterior_predictive', tf.cast(posterior_predictive_samples, tf.float32))\n    with tfc.variable_scope('model', reuse=True):\n        p_z = tfp.distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32), scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n        p_z_sample = p_z.sample(FLAGS.n_samples)\n        p_x_given_z_logits = generative_network(z=p_z_sample, hidden_size=FLAGS.hidden_size)\n        prior_predictive = tfp.distributions.Bernoulli(logits=p_x_given_z_logits)\n        prior_predictive_samples = prior_predictive.sample()\n        tfc.summary.image('prior_predictive', tf.cast(prior_predictive_samples, tf.float32))\n    with tfc.variable_scope('model', reuse=True):\n        z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n        p_x_given_z_logits = generative_network(z=z_input, hidden_size=FLAGS.hidden_size)\n        prior_predictive_inp = tfp.distributions.Bernoulli(logits=p_x_given_z_logits)\n        prior_predictive_inp_sample = prior_predictive_inp.sample()\n    kl = tf.reduce_sum(tfp.distributions.kl_divergence(q_z, p_z), 1)\n    expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x), [1, 2, 3])\n    elbo = tf.reduce_sum((expected_log_likelihood - kl), 0)\n    optimizer = tfc.train.RMSPropOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize((- elbo))\n    summary_op = tfc.summary.merge_all()\n    init_op = tfc.global_variables_initializer()\n    sess = tfc.InteractiveSession()\n    sess.run(init_op)\n    mnist_data = tfds.load(name='binary_mnist', split='train', shuffle_files=False)\n    dataset = mnist_data.repeat().shuffle(buffer_size=1024).batch(FLAGS.batch_size)\n    print(('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir))\n    train_writer = tfc.summary.FileWriter(FLAGS.logdir, sess.graph)\n    t0 = time.time()\n    for (i, batch) in enumerate(tfds.as_numpy(dataset)):\n        np_x = batch['image']\n        sess.run(train_op, {x: np_x})\n        if ((i % FLAGS.print_every) == 0):\n            (np_elbo, summary_str) = sess.run([elbo, summary_op], {x: np_x})\n            train_writer.add_summary(summary_str, i)\n            print('Iteration: {0:d} ELBO: {1:.3f} s\/iter: {2:.3e}'.format(i, (np_elbo \/ FLAGS.batch_size), ((time.time() - t0) \/ FLAGS.print_every)))\n            (np_posterior_samples, np_prior_samples) = sess.run([posterior_predictive_samples, prior_predictive_samples], {x: np_x})\n            for k in range(FLAGS.n_samples):\n                f_name = os.path.join(FLAGS.logdir, ('iter_%d_posterior_predictive_%d_data.jpg' % (i, k)))\n                imwrite(f_name, np_x[k, :, :, 0].astype(np.uint8))\n                f_name = os.path.join(FLAGS.logdir, ('iter_%d_posterior_predictive_%d_sample.jpg' % (i, k)))\n                imwrite(f_name, np_posterior_samples[k, :, :, 0].astype(np.uint8))\n                f_name = os.path.join(FLAGS.logdir, ('iter_%d_prior_predictive_%d.jpg' % (i, k)))\n                imwrite(f_name, np_prior_samples[k, :, :, 0].astype(np.uint8))\n            t0 = time.time()\n"}
{"label_name":"train","label":0,"method_name":"train_step","method":"\n\n@tf.function\ndef train_step(model, x, y):\n    with tf.GradientTape() as tape:\n        (y_pred, logits) = model(x)\n        loss = cross_entropy_loss(logits, y)\n    with train_summary_writer.as_default():\n        tf.summary.scalar('loss', loss, step=optimizer.iterations)\n        x_image = tf.reshape(x, [(- 1), 28, 28, 1])\n        tf.summary.image('training image', x_image, max_outputs=10, step=optimizer.iterations)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"}
{"label_name":"process","label":2,"method_name":"process_word","method":"\n\ndef process_word(word, training=True):\n    X = []\n    y = []\n    word_int = [char_to_int[c] for c in word if (c != HYPHENATION_INDICATOR)]\n    word_int = np.array((([0, 0] + word_int) + [0, 0]))\n    hyphenations = 0\n    padded = False\n    if (len(word_int) < WINDOW_SIZE):\n        zeros = np.zeros((WINDOW_SIZE - len(word_int)), dtype=word_int.dtype)\n        word_int = np.concatenate((word_int, zeros))\n        padded = True\n    num_windows = ((len(word_int) - WINDOW_SIZE) + 1)\n    indexer = (np.arange(WINDOW_SIZE)[None, :] + np.arange(num_windows)[:, None])\n    windows = word_int[indexer]\n    for (offset, window) in enumerate(windows):\n        o = ((offset + 2) + hyphenations)\n        if training:\n            hyphenation = (word[o] == HYPHENATION_INDICATOR)\n            if hyphenation:\n                hyphenations += 1\n        one_hot = np.zeros((WINDOW_SIZE, n_vocab), dtype=np.bool)\n        one_hot[(np.arange(WINDOW_SIZE), window)] = True\n        X.append(one_hot)\n        if training:\n            y.append(hyphenation)\n    if training:\n        return (X, y)\n    else:\n        return X\n"}
{"label_name":"process","label":2,"method_name":"preprocess_data","method":"\n\ndef preprocess_data(data, process_method='default'):\n    \"Preprocesses dataset.\\n\\n    Args:\\n        data(dict): Python dict loaded using io_tools.\\n        process_method(str): processing methods needs to support\\n          ['raw', 'default'].\\n\\n        if process_method is 'raw'\\n          1. Convert the images to range of [0, 1] by dividing by 255.\\n          2. Remove dataset mean. Average the images across the batch dimension.\\n             This will result in a mean image of dimension (8,8,3).\\n          3. Flatten images, data['image'] is converted to dimension (N, 8*8*3)\\n\\n        if process_method is 'default':\\n          1. Convert images to range [0,1]\\n          2. Convert from rgb to gray then back to rgb. Use skimage\\n          3. Take the absolute value of the difference with the original image.\\n          4. Remove dataset mean. Average the absolute value differences across\\n             the batch dimension. This will result in a mean of dimension (8,8,3).\\n          5. Flatten images, data['image'] is converted to dimension (N, 8*8*3)\\n\\n    Returns:\\n        data(dict): Apply the described processing based on the process_method\\n        str to data['image'], then return data.\\n    \"\n    if (process_method == 'raw'):\n        scaled_image = (data['image'] \/ 255)\n        N = len(scaled_image)\n        image_mean = (np.sum(scaled_image, axis=0) \/ N)\n        for image in scaled_image:\n            image -= image_mean\n        data['image'] = scaled_image.flatten().reshape(N, ((8 * 8) * 3))\n    elif (process_method == 'default'):\n        scaled_image = (data['image'] \/ 255)\n        N = len(scaled_image)\n        grayscale = color.rgb2gray(scaled_image)\n        recRgb = color.gray2rgb(grayscale)\n        absval = abs((recRgb - scaled_image))\n        image_mean = (np.sum(absval, axis=0) \/ N)\n        for image in absval:\n            image -= image_mean\n        data['image'] = absval.flatten().reshape(N, ((8 * 8) * 3))\n    elif (process_method == 'custom'):\n        pass\n    return data\n"}
{"label_name":"predict","label":4,"method_name":"predict","method":"\n\n@app.route('\/api\/ml\/predict\/<name>', methods=['POST'])\ndef predict(name):\n    message = 'Success'\n    code = 200\n    result = []\n    try:\n        start = datetime.now()\n        data = json.loads(request.data)\n        service = projectmgr.GetService(name, constants.ServiceTypes.MachineLearning)\n        servicejson = json.loads(service.servicedata)\n        savePrediction = False\n        if ('save_prediction' in data):\n            savePrediction = data['save_prediction']\n        result = {}\n        if (servicejson['data_format'] == 'image'):\n            testfile = data['imagepath']\n        elif (servicejson['data_format'] == 'csv'):\n            testfile = data['testfile']\n        pipeline.init(pipeline, name, servicejson['model_type'])\n        predictions = pipeline.Predict(testfile, savePrediction)\n        predictions = json.loads(predictions)\n        if (servicejson['data_format'] == 'csv'):\n            result = predictions['0']\n        else:\n            result = predictions\n        logmgr.LogPredSuccess(name, constants.ServiceTypes.MachineLearning, start)\n    except Exception as e:\n        code = 500\n        message = str(e)\n        logmgr.LogPredError(name, constants.ServiceTypes.MachineLearning, start, message)\n    return jsonify({'statuscode': code, 'message': message, 'result': result})\n"}
{"label_name":"process","label":2,"method_name":"process_transactions","method":"\n\n@web.route('\/transactions\/process', methods=['GET', 'POST'])\n@login_required\ndef process_transactions():\n    '\\n    Process uploaded transactions.\\n\\n    Return a form for processing uploaded transactions or process submitted\\n    form and redirect to Transactions HTML page.\\n    '\n    form = ProcessUploadedTransactionsForm()\n    form.date_format.choices = [('DMY', 'DD\/MM\/YY'), ('MDY', 'MM\/DD\/YY'), ('YMD', 'YY\/MM\/DD'), ('YDM', 'YY\/DD\/MM')]\n    transactions = session['uploaded_transactions']\n    predicted_categories = predict_categories()\n    (predicted_columns, header_row) = predict_columns()\n    classify_cols_form = ClassifyTransactionColumnsForm()\n    if (request.method != 'POST'):\n        for _ in range(0, len(transactions[0])):\n            form.col_classifications.append_entry(classify_cols_form)\n    for (num, subform) in enumerate(form.col_classifications):\n        subform.form.column_label.choices = [('date', 'Date'), ('description', 'Description'), ('dr', 'Debit'), ('cr', 'Credit'), ('drcr', 'Debit\/Credit'), ('ignore', 'Ignore')]\n        subform.form.column_label.default = predicted_columns[num]\n    classify_rows_form = ClassifyTransactionRowsForm()\n    if (request.method != 'POST'):\n        for _ in range(0, len(transactions)):\n            form.row_classifications.append_entry(classify_rows_form)\n    categories = current_user.group().categories\n    category_names = [(category.catname, category.catname) for category in categories]\n    actions = [('Keep', 'Keep'), ('Ignore', 'Ignore')]\n    for (num, subform) in enumerate(form.row_classifications):\n        subform.form.category_name.choices = category_names\n        subform.form.category_name.default = predicted_categories[num]\n        subform.form.action.choices = actions\n        if ((num == 0) and header_row):\n            subform.form.action.default = 'Ignore'\n            subform.form.category_name.default = 'Unspecified Expense'\n        else:\n            subform.form.action.default = 'Keep'\n    if form.validate_on_submit():\n        if form.add.data:\n            if (not classifications_valid(form.col_classifications.data)):\n                flash('Invalid classifications, please try again.')\n                return redirect(url_for('.process_transactions'))\n            for (transno, transaction) in enumerate(transactions):\n                action = form.row_classifications.data[transno]['action']\n                if (action == 'Ignore'):\n                    continue\n                amount = 0\n                date = ''\n                description = ''\n                for (fieldno, field) in enumerate(transaction):\n                    classification = form.col_classifications.data[fieldno]['column_label']\n                    if (transaction[fieldno].isspace() or (transaction[fieldno] is None) or (not transaction[fieldno])):\n                        continue\n                    if (classification == 'date'):\n                        date = transaction[fieldno]\n                    elif (classification == 'description'):\n                        description = transaction[fieldno]\n                    elif (classification == 'dr'):\n                        amount = abs((float(transaction[fieldno]) * 100))\n                    elif (classification == 'cr'):\n                        amount = abs((float(transaction[fieldno]) * 100))\n                    elif (classification == 'drcr'):\n                        amount = abs((float(transaction[fieldno]) * 100))\n                catname = form.row_classifications.data[transno]['category_name']\n                accname = session['upload_account']\n                if (form.date_format.data == 'DMY'):\n                    current_user.group().add_transaction(amount=amount, date=date, catname=catname, accname=accname, description=description)\n                elif (form.date_format.data == 'MDY'):\n                    current_user.group().add_transaction(amount=amount, date=date, catname=catname, accname=accname, description=description, dayfirst=False)\n                elif (form.date_format.data == 'YMD'):\n                    current_user.group().add_transaction(amount=amount, date=date, catname=catname, accname=accname, description=description, dayfirst=False, yearfirst=True)\n                elif (form.date_format.data == 'YDM'):\n                    current_user.group().add_transaction(amount=amount, date=date, catname=catname, accname=accname, description=description, yearfirst=True)\n        if form.cancel.data:\n            pass\n        db.session.commit()\n        session['transactions'] = [transaction.transno for transaction in current_user.group().transactions]\n        return redirect(url_for('.transactions_page'))\n    for subform in form.row_classifications:\n        subform.form.process()\n    for subform in form.col_classifications:\n        subform.form.process()\n    return render_template('process_transactions.html', form=form, transactions=transactions, num_transactions=len(transactions), menu='transactions')\n"}
{"label_name":"forward","label":3,"method_name":"forward_prop","method":"\n\ndef forward_prop(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    Z1 = (np.dot(W1, X) + b1)\n    A1 = np.tanh(Z1)\n    Z2 = (np.dot(W2, A1) + b2)\n    A2 = softmax(Z2.T)\n    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n    return (A2, cache)\n"}
{"label_name":"train","label":0,"method_name":"train_input_fn","method":"\n\ndef train_input_fn():\n    data = tfds.load(name='mnist', as_supervised=True)\n    train_ds = data['train']\n    train_ds = train_ds.map(preprocess).shuffle(60000).batch(BATCH_SIZE).take(N_TRAIN_BATCHES)\n    return train_ds\n"}
{"label_name":"train","label":0,"method_name":"_expected_tf_training_uri","method":"\n\ndef _expected_tf_training_uri(tf_training_version, py_version, processor='cpu', region=REGION):\n    version = Version(tf_training_version)\n    if (version < Version('1.11')):\n        repo = 'sagemaker-tensorflow'\n    elif (version < Version('1.13')):\n        repo = 'sagemaker-tensorflow-scriptmode'\n    elif (version >= Version('1.14')):\n        repo = 'tensorflow-training'\n    else:\n        repo = ('sagemaker-tensorflow-scriptmode' if (py_version == 'py2') else 'tensorflow-training')\n    return expected_uris.framework_uri(repo, tf_training_version, _sagemaker_or_dlc_account(repo, region), py_version=py_version, processor=processor, region=region)\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(images, labels, ckpt_path, dropout=False):\n    '\\n  This function contains the loop that actually trains the model.\\n  :param images: a numpy array with the input data\\n  :param labels: a numpy array with the output labels\\n  :param ckpt_path: a path (including name) where model checkpoints are saved\\n  :param dropout: Boolean, whether to use dropout or not\\n  :return: True if everything went well\\n  '\n    assert (len(images) == len(labels))\n    assert (images.dtype == np.float32)\n    assert (labels.dtype == np.int32)\n    with tf.Graph().as_default():\n        global_step = tf.Variable(0, trainable=False)\n        train_data_node = _input_placeholder()\n        train_labels_shape = (FLAGS.batch_size,)\n        train_labels_node = tf.placeholder(tf.int32, shape=train_labels_shape)\n        print('Done Initializing Training Placeholders')\n        if FLAGS.deeper:\n            logits = inference_deeper(train_data_node, dropout=dropout)\n        else:\n            logits = inference(train_data_node, dropout=dropout)\n        loss = loss_fun(logits, train_labels_node)\n        train_op = train_op_fun(loss, global_step)\n        saver = tf.train.Saver(tf.global_variables())\n        print('Graph constructed and saver created')\n        init = tf.global_variables_initializer()\n        sess = tf.Session(config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement))\n        sess.run(init)\n        print('Session ready, beginning training loop')\n        data_length = len(images)\n        nb_batches = math.ceil((data_length \/ FLAGS.batch_size))\n        for step in xrange(FLAGS.max_steps):\n            start_time = time.time()\n            batch_nb = (step % nb_batches)\n            (start, end) = utils.batch_indices(batch_nb, data_length, FLAGS.batch_size)\n            feed_dict = {train_data_node: images[start:end], train_labels_node: labels[start:end]}\n            (_, loss_value) = sess.run([train_op, loss], feed_dict=feed_dict)\n            duration = (time.time() - start_time)\n            assert (not np.isnan(loss_value)), 'Model diverged with loss = NaN'\n            if ((step % 100) == 0):\n                num_examples_per_step = FLAGS.batch_size\n                examples_per_sec = (num_examples_per_step \/ duration)\n                sec_per_batch = float(duration)\n                format_str = '%s: step %d, loss = %.2f (%.1f examples\/sec; %.3f sec\/batch)'\n                print((format_str % (datetime.now(), step, loss_value, examples_per_sec, sec_per_batch)))\n            if (((step % 1000) == 0) or ((step + 1) == FLAGS.max_steps)):\n                saver.save(sess, ckpt_path, global_step=step)\n    return True\n"}
{"label_name":"process","label":2,"method_name":"preprocess","method":"\n\ndef preprocess(df_train, df_val, df_test, n_durations=50):\n    cols = list(df_train.columns.drop(['duration', 'event', 'duration_true', 'event_true', 'censoring_true']))\n    x_mapper = DataFrameMapper([([col], StandardScaler()) for col in cols])\n    x_train = x_mapper.fit_transform(df_train).astype('float32')\n    x_val = x_mapper.transform(df_val).astype('float32')\n    x_test = x_mapper.transform(df_test).astype('float32')\n    labtrans = LogisticHazard.label_transform(n_durations)\n    get_dur_ev = (lambda df: (df['duration'].values.astype('float32'), df['event'].values.astype('float32')))\n    y_train = labtrans.fit_transform(*get_dur_ev(df_train))\n    y_val = labtrans.transform(*get_dur_ev(df_val))\n    y_test = labtrans.transform(*get_dur_ev(df_test))\n    train = tt.tuplefy(x_train, y_train)\n    val = tt.tuplefy(x_val, y_val)\n    test = tt.tuplefy(x_test, y_test)\n    return (train, val, test, labtrans)\n"}
{"label_name":"predict","label":4,"method_name":"bql_predict_confidence","method":"\n\ndef bql_predict_confidence(bdb, population_id, generator_id, modelnos, rowid, colno, numsamples):\n    if (generator_id is None):\n        generator_ids = core.bayesdb_population_generators(bdb, population_id)\n        index = bdb.np_prng.randint(0, high=len(generator_ids))\n        generator_id = generator_ids[index]\n    modelnos = _retrieve_modelnos(modelnos)\n    backend = core.bayesdb_generator_backend(bdb, generator_id)\n    (value, confidence) = backend.predict_confidence(bdb, generator_id, modelnos, rowid, colno, numsamples=numsamples)\n    return json.dumps({'value': value, 'confidence': confidence})\n"}
{"label_name":"predict","label":4,"method_name":"predict_with_optimized_hyps","method":"\n\n@utils_common.validate_types\ndef predict_with_optimized_hyps(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, str_cov: str=constants.STR_GP_COV, str_optimizer_method: str=constants.STR_OPTIMIZER_METHOD_GP, prior_mu: constants.TYPING_UNION_CALLABLE_NONE=None, fix_noise: float=constants.FIX_GP_NOISE, debug: bool=False) -> constants.TYPING_TUPLE_THREE_ARRAYS:\n    '\\n    This function returns posterior mean and posterior standard deviation\\n    functions over `X_test`, computed by the Gaussian process regression\\n    optimized with `X_train` and `Y_train`.\\n\\n    :param X_train: inputs. Shape: (n, d) or (n, m, d).\\n    :type X_train: numpy.ndarray\\n    :param Y_train: outputs. Shape: (n, 1).\\n    :type Y_train: numpy.ndarray\\n    :param X_test: inputs. Shape: (l, d) or (l, m, d).\\n    :type X_test: numpy.ndarray\\n    :param str_cov: the name of covariance function.\\n    :type str_cov: str., optional\\n    :param str_optimizer_method: the name of optimization method.\\n    :type str_optimizer_method: str., optional\\n    :param prior_mu: None, or prior mean function.\\n    :type prior_mu: NoneType, or function, optional\\n    :param fix_noise: flag for fixing a noise.\\n    :type fix_noise: bool., optional\\n    :param debug: flag for printing log messages.\\n    :type debug: bool., optional\\n\\n    :returns: a tuple of posterior mean function over `X_test`, posterior\\n        standard deviation function over `X_test`, and posterior covariance\\n        matrix over `X_test`. Shape: ((l, 1), (l, 1), (l, l)).\\n    :rtype: tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray)\\n\\n    :raises: AssertionError\\n\\n    '\n    assert isinstance(X_train, np.ndarray)\n    assert isinstance(Y_train, np.ndarray)\n    assert isinstance(X_test, np.ndarray)\n    assert isinstance(str_cov, str)\n    assert isinstance(str_optimizer_method, str)\n    assert isinstance(fix_noise, bool)\n    assert isinstance(debug, bool)\n    assert (callable(prior_mu) or (prior_mu is None))\n    assert (len(Y_train.shape) == 2)\n    utils_gp.check_str_cov('predict_with_optimized_kernel', str_cov, X_train.shape, shape_X2=X_test.shape)\n    assert (X_train.shape[0] == Y_train.shape[0])\n    assert (X_train.shape[1] == X_test.shape[1])\n    assert (str_optimizer_method in constants.ALLOWED_OPTIMIZER_METHOD_GP)\n    time_start = time.time()\n    (cov_X_X, inv_cov_X_X, hyps) = get_optimized_kernel(X_train, Y_train, prior_mu, str_cov, str_optimizer_method=str_optimizer_method, fix_noise=fix_noise, debug=debug)\n    (mu_Xs, sigma_Xs, Sigma_Xs) = predict_with_cov(X_train, Y_train, X_test, cov_X_X, inv_cov_X_X, hyps, str_cov=str_cov, prior_mu=prior_mu, debug=debug)\n    time_end = time.time()\n    if debug:\n        logger.debug('time consumed to construct gpr: %.4f sec.', (time_end - time_start))\n    return (mu_Xs, sigma_Xs, Sigma_Xs)\n"}
{"label_name":"train","label":0,"method_name":"download_trained_weights","method":"\n\ndef download_trained_weights(coco_model_path, verbose=1):\n    'Download COCO trained weights from Releases.\\n\\n    coco_model_path: local path of COCO trained weights\\n    '\n    if (verbose > 0):\n        print((('Downloading pretrained model to ' + coco_model_path) + ' ...'))\n    with urllib.request.urlopen(COCO_MODEL_URL) as resp, open(coco_model_path, 'wb') as out:\n        shutil.copyfileobj(resp, out)\n    if (verbose > 0):\n        print('... done downloading pretrained model!')\n"}
{"label_name":"train","label":0,"method_name":"train","method":"\n\ndef train(train_log_dir, checkpoint, eval_every_n_steps=10, num_steps=3000):\n    dataset_fn = datasets.mnist.TinyMnist\n    w_learner_fn = architectures.more_local_weight_update.MoreLocalWeightUpdateWLearner\n    theta_process_fn = architectures.more_local_weight_update.MoreLocalWeightUpdateProcess\n    meta_objectives = []\n    meta_objectives.append(meta_objective.linear_regression.LinearRegressionMetaObjective)\n    meta_objectives.append(meta_objective.sklearn.LogisticRegression)\n    (checkpoint_vars, train_one_step_op, (base_model, dataset)) = evaluation.construct_evaluation_graph(theta_process_fn=theta_process_fn, w_learner_fn=w_learner_fn, dataset_fn=dataset_fn, meta_objectives=meta_objectives)\n    batch = dataset()\n    (pre_logit, outputs) = base_model(batch)\n    global_step = tf.train.get_or_create_global_step()\n    var_list = list(snt.get_variables_in_module(base_model, tf.GraphKeys.TRAINABLE_VARIABLES))\n    tf.logging.info('all vars')\n    for v in tf.all_variables():\n        tf.logging.info(('   %s' % str(v)))\n    global_step = tf.train.get_global_step()\n    accumulate_global_step = global_step.assign_add(1)\n    reset_global_step = global_step.assign(0)\n    train_op = tf.group(train_one_step_op, accumulate_global_step, name='train_op')\n    summary_op = tf.summary.merge_all()\n    file_writer = summary_utils.LoggingFileWriter(train_log_dir, regexes=['.*'])\n    if checkpoint:\n        str_var_list = checkpoint_utils.list_variables(checkpoint)\n        name_to_v_map = {v.op.name: v for v in tf.all_variables()}\n        var_list = [name_to_v_map[vn] for (vn, _) in str_var_list if (vn in name_to_v_map)]\n        saver = tf.train.Saver(var_list)\n        missed_variables = [v.op.name for v in (set(snt.get_variables_in_scope('LocalWeightUpdateProcess', tf.GraphKeys.GLOBAL_VARIABLES)) - set(var_list))]\n        assert (len(missed_variables) == 0), 'Missed a theta variable.'\n    hooks = []\n    with tf.train.SingularMonitoredSession(master='', hooks=hooks) as sess:\n        step = sess.run(global_step)\n        if ((step == 0) and checkpoint):\n            tf.logging.info('force restore')\n            saver.restore(sess, checkpoint)\n            tf.logging.info('force restore done')\n            sess.run(reset_global_step)\n            step = sess.run(global_step)\n        while (step < num_steps):\n            if ((step % eval_every_n_steps) == 0):\n                (s, _, step) = sess.run([summary_op, train_op, global_step])\n                file_writer.add_summary(s, step)\n            else:\n                (_, step) = sess.run([train_op, global_step])\n"}
{"label_name":"train","label":0,"method_name":"train_lightgbm","method":"\n\ndef train_lightgbm(parameters, X, y, num_rounds=50):\n    ddata = lgb.Dataset(X.values, y.values, free_raw_data=False)\n    with Timer() as t:\n        clf = lgb.train(parameters, ddata, num_boost_round=num_rounds)\n    return (clf, t.interval)\n"}
{"label_name":"train","label":0,"method_name":"train_loss_score","method":"\n\ndef train_loss_score(net, X=None, y=None):\n    return net.history[((- 1), 'batches', (- 1), 'train_loss')]\n"}
{"label_name":"train","label":0,"method_name":"backwardBxConstraint","method":"\n\ndef backwardBxConstraint(left, right):\n    if (not crossedDirs(left, right)):\n        return False\n    if ((not left.dir().can_cross()) and right.dir().can_cross()):\n        return False\n    return left.arg().is_primitive()\n"}
{"label_name":"train","label":0,"method_name":"lms_train","method":"\n\ndef lms_train(p0, Zi, Data):\n\n    def error(p, y, args):\n        l = len(p)\n        f = p[(l - 1)]\n        for i in range(len(args)):\n            f += (p[i] * args[i])\n        return (f - y)\n    Para = leastsq(error, p0, args=(Zi, Data))\n    return Para[0]\n"}
{"label_name":"predict","label":4,"method_name":"plot_prediction","method":"\n\ndef plot_prediction(x, f, pred, x_obs=None, y_obs=None):\n    plt.plot(x, f, label='True', style='test')\n    if (x_obs is not None):\n        plt.scatter(x_obs, y_obs, label='Observations', style='train', s=20)\n    (mean, lower, upper) = pred\n    plt.plot(x, mean, label='Prediction', style='pred')\n    plt.fill_between(x, lower, upper, style='pred')\n    tweak()\n"}
{"label_name":"save","label":1,"method_name":"save_weights","method":"\n\ndef save_weights(fname, params, metadata=None):\n    ' assumes all params have unique names.\\n    '\n    names = [par.name for par in params]\n    if (len(names) != len(set(names))):\n        raise ValueError('need unique param names')\n    param_dict = {param.name: param.get_value(borrow=False) for param in params}\n    if (metadata is not None):\n        param_dict['metadata'] = pickle.dumps(metadata)\n    logging.info('saving {} parameters to {}'.format(len(params), fname))\n    fname = Path(fname)\n    if fname.exists():\n        tmp_fname = Path((fname.stripext() + '.tmp.npz'))\n        np.savez_compressed(str(tmp_fname), **param_dict)\n        tmp_fname.rename(fname)\n    else:\n        np.savez_compressed(str(fname), **param_dict)\n"}
{"label_name":"process","label":2,"method_name":"preprocessing_CIFAR10_data","method":"\n\ndef preprocessing_CIFAR10_data(X_train, y_train, X_val, y_val, X_test, y_test):\n    X_train = np.reshape(X_train, (X_train.shape[0], (- 1)))\n    X_val = np.reshape(X_val, (X_val.shape[0], (- 1)))\n    X_test = np.reshape(X_test, (X_test.shape[0], (- 1)))\n    mean_image = np.mean(X_train, axis=0)\n    X_train -= mean_image\n    X_val -= mean_image\n    X_test -= mean_image\n    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n    return (X_train, y_train, X_val, y_val, X_test, y_test)\n"}
{"label_name":"train","label":0,"method_name":"toggle_trainable","method":"\n\ndef toggle_trainable(network, state):\n    network.trainable = state\n    for layer in network.layers:\n        layer.trainable = state\n"}
{"label_name":"process","label":2,"method_name":"preprocess","method":"\n\ndef preprocess(text):\n    raw = text.lower()\n    tokens = tokenizer.tokenize(raw)\n    pospeech = []\n    tag = nltk.pos_tag(tokens)\n    for j in tag:\n        if ((j[1] == 'NN') or (j[1] == 'JJ')):\n            pospeech.append(j[0])\n    stopped_tokens = [i for i in pospeech if (not (i in en_stop))]\n    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n    return stemmed_tokens\n"}
